**Github仓库：https://github.com/ShaoQ2/032002204.git

#  一、PSP表格

## (1.1)在开始实现程序之前，在附录提供的PSP表格记录下你估计将在程序的各		个模块的开发上耗费的时间。（3'）

## (1.2)在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间。（3'）

| PSP2.1                                      | Personal Software<br/>Process Stages         | 预估耗时<br/>（分钟） | 实际耗时<br/>（分钟） |
| ------------------------------------------- | -------------------------------------------- | --------------------- | --------------------- |
| Planning                                    | 计划                                         | 60                    | 50                    |
| · Estimate                                  | · 估计这个任务需要多少时间                   | 10                    | 15                    |
| Development                                 | 开发                                         | 900                   | 1200                  |
| · Analysis                                  | · 需求分析 (包括学习新技术)                  | 300                   | 400                   |
| · Design Spec                               | · 生成设计文档                               | 40                    | 50                    |
| · Design Review                             | · 设计复审                                   | 30                    | 25                    |
| · Coding Standard                           | · 代码规范 (为目前的开发制<br/>定合适的规范) | 20                    | 30                    |
| · Design                                    | · 具体设计                                   | 30                    | 40                    |
| · Coding                                    | · 具体编码                                   | 1000                  | 1200                  |
| · Code Review                               | · 代码复审                                   | 200                   | 180                   |
| · Test                                      | · 测试（自我测试，修改代<br/>码，提交修改）  | 200                   | 180                   |
| Reporting                                   | 报告                                         | 60                    | 70                    |
| · Test Repor                                | · 测试报告                                   | 60                    | 60                    |
| · Size Measurement                          | · 计算工作量                                 | 20                    | 15                    |
| · Postmortem & Process<br/>Improvement Plan | · 事后总结, 并提出过程改进<br/>计划          | 20                    | 15                    |
|                                             | · 合计                                       | 2950                  | 3530                  |



# 二、任务要求的实现

## (2.1)项目设计与技术栈

**1.任务环节分析**

- 首先分析要爬取的页面结构，通过不断尝试分析要用到的爬虫技术进行网页爬取
- 分析爬取的内容，根据要求进行数据提取
- 将需要统计的数据写入excel中
- 对提取的数据进行热点分析以反应疫情的每日态势
- 对提取的数据进行可视化表示，用柱状图来实现

**2.完成各个环节的方法或渠道**

- 爬取网页：用到了pyppetter模块、asyncio模块、os模块、etree模块
- 数据提取：re模块、os模块、xlwt模块
- 数据写入到excel中：openpyxl模块
- 每日热点：机器学习算法
- 数据可视化：openpyxl

**3.技术栈**

语言：python

模块：pyppetter模块、asyncio模块、os模块、etree模块、re模块、xlwt模块、openpyxl模块

## (2.2)爬虫与数据处理

## （2.2.1）爬虫

**业务逻辑：**

**1.首先要获取每个页面的url（共42页）**

多观察几个页面的url，可以发现页面的url存在规律,如下图:

![image-20220918172348556.png](https://s2.loli.net/2022/09/20/5VHh4kSKmvQJTuF.png)

![image-20220918172406154.png](https://s2.loli.net/2022/09/20/gCsmyFtzJHWLlaP.png)

![image-20220918172434328.png](https://s2.loli.net/2022/09/20/Kmae7RiNBj5tVJk.png)

![image-20220918172457713.png](https://s2.loli.net/2022/09/20/4rVgsjODlnvxmKz.png)

```
def get_PageUrl():
    url_list = []# 用于存储每一页的url
    # 获取41页的url
    for page in range(1, 42):
        if page == 1:
            page_url = "http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml"# 第一页作特殊处理
            url_list.append(page_url)# 加入url列表
        else:
            page_url = "http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_" + str(page) + ".shtml"
            url_list.append(page_url)
    return url_list
```

目的：通过获取每个页面的url之后，再调用获取源代码的函数，获取每个页面的源代码



**2.获取每个页面的源代码**

```
# 关键函数
async def pyppeteer_url(url):
    # 创建一个浏览器实例对象
    browser = await launch({'headless': False, 'dumpio': True})
    # 用浏览器打开一个网页，并创建page对象
    page = await browser.newPage()
    # 绕过浏览器的检测
    await page.evaluateOnNewDocument('() =>{ Object.defineProperties(navigator,'
                                     '{ webdriver:{ get: () => false } }) }')
    width, height = screen_size()
    # 设置页面视口的大小
    await page.setViewport({'width': width, 'height': height})
    # 设置UA
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36 Edg/105.0.1343.33')
    # 输入网址等待加载
    await page.goto(url)
    # 等待页面加载出来
    await asyncio.wait([page.waitForNavigation()])
    content = await page.content()
    await browser.close()# 关闭浏览器
    return content
```

```
# 获取网页的源代码
def get_PageSource(url):
    # 用get_event_loop()方法创建一个事件循环
    loop = asyncio.get_event_loop()
    # 再调用事件循环对象loop的run_until_complete方法将协程注册到事件循环中
    return loop.run_until_complete(pyppeteer_url(url))
```

注：在尝试爬取页面的过程中，会发现页面具有**反爬机制**。服务器在处理网络请求时是要验证cookies的，cookie的值是动态变化的。网站会检测识别并限制 Selenium 的访问，用 Selenium 访问得到的只是一个空界面。显然，此网站的cookie是通过js加密的。因此我们使用pyppeteer，可以说就是进阶版的 selenium，功能更完善，效率更高，更容易绕过反爬检测。

Pyppeteer：Pyppeteer 是 Puppeteer 的 Python 实现。Puppeteer 是谷歌开发的一个 Node 库，它提供了一个高级API 来通过 DevTools 协议控制 Chromium 或 Chrome。Pyppeteer 默认以 headless模式运行，但是可以通过修改配置文件运行“有头”模式。

**3.从页面源代码中获取每日的url**

在页面中右键点击检查，点击元素可以观察网页的源代码，可以观察到ul下多个li标签，通过获取li标签下的a标签中的href属性，可以获取每一天疫情内容的url，进而获取每日疫情的源代码，进而提取数据

![image-20220918171207695.png](https://s2.loli.net/2022/09/20/Yg8RSsPh4pUflDX.png)

```
# 获取疫情数据url
def get_Link(s):
    link_list = []# 用列表存储页面每一天url
    link_tree = etree.HTML(s) # 解析HTML
    li_list = link_tree.xpath("/html/body/div[3]/div[2]/ul/li")# 利用xpath提前所有li
    for li in li_list:
        day_link = li.xpath("./a/@href")# 每一个li中都由a标签，提取其中的href属性的值
        day_link = "http://www.nhc.gov.cn" + "".join(day_link)# 完整的url
        link_list.append(day_link)
    return link_list
```

**4.获取每一天的源代码，从中提取文本**

```
# 获取每一天疫情内容
def get_Content(s):
    content_tree = etree.HTML(s)# 解析HTML
    # 获取链接页面中的疫情数据
    all_content = content_tree.xpath("/html/body/div[3]/div[2]/div[3]//text()")# 利用xpath找到文本并用text()提取
    day_content = "".join(all_content)# 将列表转换为字符串
    return day_content
```

**5.保存文本为txt文件**

```
# 保存
def savefile(path, filename, content):
    # 判断path文件是否存在
    if not os.path.exists(path):
        # 创建目录
        os.makedirs(path)
    # 保存文件,打开文件用于写入‘w'
    with open(path + filename + '.txt', 'w', encoding='utf-8')as f:
        f.write(content)
```

**6.主函数和需要导入的模块**

```
import asyncio
import tkinter as tk
from pyppeteer import launch
from lxml import etree
import time
import os
```

```
if __name__ == '__main__':
    for url in get_PageUrl():
        # source为网页的源代码
        source = get_PageSource(url)
        time.sleep(3)
        dates = get_Date(source)
        print("日期列表:", dates)
        links = get_Link(source)
        print("页面链接", links)
        idx = 0
        for link in links:
            s = get_PageSource(link)# 获取链接页面的源代码
            content = get_Content(s)# 获取疫情数据
            print(dates[idx], "疫情数据", content)
            savefile("E:/疫情数据", dates[idx], content)
            idx = idx + 1
    print("over!")
```

**7.运行结果**

![image-20220918185423150.png](https://s2.loli.net/2022/09/20/hD5yJ8UfBFINSi9.png)

![image-20220918185648912.png](https://s2.loli.net/2022/09/20/XANlufHx1tMS8zp.png)

## （2.2.2）数据处理

1. 统计中国大陆每日**本土新增确诊人数**及**新增无症状感染人数**，境外输入类型和疑似病例等无需统计。

  **（1）遍历提取到的txt文件，并用正则表达式提取我们所需要的文本内容，用列表存储**

![image-20220920162039992.png](https://s2.loli.net/2022/09/20/ihlc3oyA1zSdnvj.png)

  **代码实现：**

  ```
  for file in files: # 遍历文件夹
      position = path+'\\' + file # 构造绝对路径，"\\"，其中一个'\'为转义符
      print(position)# 打印路径
      with open(position, "r", encoding='utf-8')as f:    # 打开文件
          data = f.read()   # 读取文件内容
          # 匹配新增确诊人数
          res_add_Confirm = re.search(r'本土[\u4e00-\u9fa5]*病例[\u4e00-\u9fa5]*[0-9]+例', data)
          # 匹配新增无症状感染人数
          res_add_Infected = re.search(r'本土[1-9]+(.*?)。', data)
          if res_add_Confirm != None:
              str_Confirm = res_add_Confirm.group()
              print(str_Confirm)# group用于获取字符串
              # 提取数据,删除中文字符
              data_Confirm = re.sub("[\u4e00-\u9fa5]", "", str_Confirm)
              print(data_Confirm)
              datas_Confirm.append(data_Confirm)
          else:
              datas_Confirm.append("0")
  
          if res_add_Infected != None:
              str_Infected = res_add_Infected.group()
              print(str_Infected)
              data_Infected = re.search(r'\d+',str_Infected)
              if data_Infected != None:
                  data_Infected = data_Infected.group()
                  print(data_Infected)
                  datas_Infected.append(data_Infected)
          else:
              datas_Infected.append("0")
  ```

  **初始化定义**

  ```
  path = "E:/疫情数据" #文件夹目录
  files = os.listdir(path) #得到文件夹下的所有文件名称,存储在列表中
  dates_list = []# 存储日期
  datas_Confirm = [] # 存每日新增确诊
  datas_Infected = [] # 存每日新增感染
  
  for i in range(len(files)):
      dates = re.sub(".txt", "", files[i])# 去除后缀表示日期
      dates_list.append(dates)# 存放到日期列表中
  ```

  **（2）将提取到的数据存储到excel中**

  ```
  workbook = xlwt.Workbook(encoding='utf-8')  # 设置一个workbook，其编码是utf-8
  worksheet = workbook.add_sheet("中国大陆每日本土新增")  # 新增一个sheet
  worksheet.write(0, 0, label = '日期')
  worksheet.write(0, 1, label = '新增确诊人数')  
  worksheet.write(0, 2, label = '新增无症状感染者') 
  for i in range(len(datas_Confirm)):  # 循环将2个列表的数据插入至excel
      worksheet.write(i + 1, 0, label = dates_list[i])
      worksheet.write(i + 1, 1, label = datas_Confirm[i])# 从第1行第1列开始存
      worksheet.write(i + 1, 2, label = datas_Infected[i])# 从第1行第2列开始存
  workbook.save(r"C:\Users\ASUS\Desktop\python学\本土每日新增.xls")  # 这里save需要特别注意，文件格式只能是xls，不能是xlsx，不然会报错
  ```

  **（3）需要导入的库**

  ```
  import os
  import re
  import xlwt
  ```

  **（4）运行结果**

  ![image-20220918191532093.png](https://s2.loli.net/2022/09/20/SaTw2L31bRYemJX.png)

  ![image-20220918191446950.png](https://s2.loli.net/2022/09/20/Wmsnkf4q3ahjdbX.png)

2. 统计**所有省份包括港澳台每日本土新增确诊人数**及**新增无症状感染人数**，境外输入类型和疑似病例等无需统计。

  **（1）统计每个省份的新增病例**

  ```
  def get_Local_data(content):
      area_num = {} # 字典存储
      if re.search(r"[^\u4e00-\u9fa5]本土病例.*。", content) is None:
          area_num['本土病例'] = '0'
          return area_num
      else:
          information = (re.search(r"[^\u4e00-\u9fa5]本土病例.*。", content)).group()# 如果匹配用group方法将search对象转换成字符串
  
          all_increase = (re.search(r'\d+', information)).group()  # all_increase# 本土新增的病例数
          area_num[(re.search(r'本土病例', information)).group()] = all_increase# 存入字典
  
          # 提取每个省份新增人数的信息
          province_increase = (re.search(r'（.*?）', information)).group()
  
          # 解决文本格式不一致的问题
          if '；' in province_increase:
              increase_list = province_increase.split('；')
          else:
              increase_list = province_increase.split('，')
  		# 遍历各个省份的数据列表，用正则表达式提取
          for area_increase in increase_list:
              area_name = re.search(r'[\u4e00-\u9fa5]+', area_increase).group()
              if re.search(r'[0-9]+', area_increase) is None:
                  area_name = verify_name(area_name)
                  area_num[area_name] = all_increase
              else:
                  area_name = verify_name(area_name)
                  increase_num = re.search(r'[0-9]+', area_increase).group()
                  area_num[area_name] = increase_num
          # 测试数据
          for data in area_num:
              print(data, area_num[data])
          return area_num
  ```

  **函数实现：**

  ```
  def verify_name(name_str):
      area_name = ''
      for name in province_name:# 遍历省份
          if name in name_str:# 如果省份出现在内容中
              area_name = name# 存储省份
              break
      return area_name# 返回与文本匹配的省份
  ```

  ```
  def refine_dict(dictionary):
      for province in province_name:
          if province not in dictionary:
              dictionary[province] = 0# 没有新增病例或者没有新增无症状感染者的省份用0填充
      return dictionary
  ```

  **（2）提取新增无症状感染者的数据**

  ```
  def get_Asymptomatic_infected_data(content):
      asymptomatic_infected_dict = {}# 用于存储新增无症状感染者的字典
      if re.search(r'新增无症状感染者[0-9]+例.*。', content) is None:
          asymptomatic_infected_dict['新增无症状感染者'] = '0'
          return asymptomatic_infected_dict
      else:
          asymptomatic_infected_info = (re.search(r"新增无症状感染者[0-9]+例.*。", content)).group()
          if re.search(r'本土[0-9]+例（.*?）', asymptomatic_infected_info) is None:# 匹配不到
              asymptomatic_infected_dict['新增无症状感染者'] = 0
              return asymptomatic_infected_dict
          else:
          	# 本土新增无症状
              mainland_info = re.search(r'本土[0-9]+例（.*?）', asymptomatic_infected_info).group()
              mainland_asymptomatic_infected_increase = re.search(r'[0-9]+', mainland_info).group()
              asymptomatic_infected_dict['新增无症状感染者'] = int(mainland_asymptomatic_infected_increase)
              # 正则提取与省份有关的信息
              province_asymptomatic_infected_increase = re.search(r'（.*?）', mainland_info).group()
              if len(province_asymptomatic_infected_increase.split('；')) == 1:# 切割
                  province_list = province_asymptomatic_infected_increase.split('，')
              else:
                  province_list = province_asymptomatic_infected_increase.split('；')
              # 遍历列表
              for province in province_list:
                  area_name = re.search(r'[\u4e00-\u9fa5]+', province).group()
                  if re.search(r'[0-9]+', province) is None:
                      area_name = verify_name(area_name)
                      asymptomatic_infected_dict[area_name] = mainland_asymptomatic_infected_increase
                  else:
                      area_name = verify_name(area_name)
                      increase_num = re.search(r'[0-9]+', province).group()
                      asymptomatic_infected_dict[area_name] = increase_num
              return asymptomatic_infected_dict
  ```

  **（3）将数据保存到excel**

  ```
  def save_to_excel(local_key, local_value, asymptomatic_key, asymptomatic_value, filename):
      # 创建一个新的Excel
      wb = Workbook()  
      ws = wb.active 
      ws.title = '每日本土新增'
      # 写入数据
      local_low = len(local_key)
      for row in range(1, local_low+1):
          ws.cell(row=row, column=1, value=local_key[row-1])
          ws.cell(row=row, column=2, value=int(local_value[row-1]))
      # 保存
      ws2 = wb.create_sheet("本土新增无症状感染者")# 新建sheet保存新增无症状感染者
      for row in range(1, len(asymptomatic_key)+1):
          ws2.cell(row=row, column=1, value=asymptomatic_key[row-1])
          ws2.cell(row=row, column=2, value=int(asymptomatic_value[row-1]))
      wb.save(f'E:/疫情数据excel/{filename}.xlsx')# 保存
      mychart(f'E:/疫情数据excel/{filename}.xlsx')
    
  ```

​		**（4）主函数**

```
if __name__ == '__main__':
    path = "E:/疫情数据"
    dirs = os.listdir(path)# 得到文件夹下所有文件的名称
    dirs.reverse()# reverse方法将列表中的数据进行反转，从最新的一天提取数据
    print(dirs)
    for file in dirs:
        if file == '2021-05-15.txt':
           break
        #f = open(file=f'C:/Users/ASUS/Desktop/疫情防控数据/{file}', mode='r', encoding='utf-8')
        f = open(file=f'E:/疫情数据/{file}', mode='r', encoding='utf-8')
        text = f.read()# 读取数据
        # 提取新增本土数据
        local_data = refine_dict(get_Local_data(text))
        # 提取新增无症状感染者的数据
        asymptomatic_data = refine_dict(get_Asymptomatic_infected_data(text))	
        local_data_key = list(local_data.keys())
        local_data_value = list(local_data.values())
        asymptomatic_data_key = list(asymptomatic_data.keys())
        asymptomatic_data_value = list(asymptomatic_data.values())
		# 保存到excel
        save_to_excel(local_data_key, local_data_value,asymptomatic_data_key, asymptomatic_data_value, file)
        f.close()
        print(f"{file}提取完成")
    print("全部提取完毕")
```

**（5）运行结果**

![image-20220918194512436.png](https://s2.loli.net/2022/09/20/N1RzvsgjTK467ud.png)

![image-20220918194601446.png](https://s2.loli.net/2022/09/20/X49UVcRAhJsDHqN.png)

## (2.3)数据统计接口部分的性能改进

![image-20220918230619664.png](https://s2.loli.net/2022/09/20/3cMNqO9dDUxApV7.png)

## (2.4)每日热点的实现思路

先进行数据预处理，用LSTM模型实现，最后分析出每日热点。

## (2.5)数据可视化界面的展示

**Excel数据可视化的实现：**

本程序采用**柱形图**，横坐标为**省份**，纵坐标为**各省份新增的数据**

利用提取到excel中的数据生成图表，每一天各个省份**新增确诊**和**新增无症状感染者**各对应一个柱状图

**1.代码实现：**

```
def mychart(excel_name):
    wb = openpyxl.load_workbook(excel_name)
    sheet = wb.active
    # 创建一个Reference
    for sheetname in wb.sheetnames:
        sheet = wb[f'{sheetname}']# 遍历每一个sheet
        values = openpyxl.chart.Reference(sheet, min_row=1, min_col=2, max_row=32, max_col=2)# 设置数据区域
        # 创建图表对象
        chart = openpyxl.chart.BarChart()
        chart.legend=None
        chart.showGridLines = False # 不显示网格线
        chart.dLbls = label.DataLabelList()
        chart.dLbls.showVal =True
        chart.varyColors = True
        chart.width = 30
        chart.height =15
        chart.title='各省份每日新增'
        chart.x_axis.title='省份'
        chart.y_axis.title='新增人数'
        categs =openpyxl.chart.Reference(sheet, min_col=1, min_row=1, max_row=32, max_col=1)
        #往图表对象中添加数据
        chart.add_data(values)
        chart.set_categories(categs)
        #将图表添加到指定的sheet中
        sheet.add_chart(chart,'G2')
        wb.save(excel_name)
```

**导入的模块：** 

```
import openpyxl
from openpyxl.chart import BarChart, Reference, label
```

**2.可视化界面展示**

![image-20220918212039801.png](https://s2.loli.net/2022/09/20/B6QZMW2cNSDbuKX.png)

![image-20220918212021136.png](https://s2.loli.net/2022/09/20/2lUMQYoEjmg1ze4.png)

# 三、心得体会

## （3.1）爬虫心得体会

​		以前只有了解过python的语法，还没有熟练使用，只能爬取简单页面的内容。在做本次爬虫之前，没有对网页结构进行充分的分析，而只是套用以前学过的一点方法，因此失败了很多次。在一开始，我用以往的方法urllib、request尝试进行怕爬取页面，发现都不可行。在网上搜集资料后，学习了selenium，发现还是无法爬取。一度以为这个网站爬不了，再后来和同学交流才知道存在反爬，在通过大量的资料搜索后，学习了有关Pyppeteer的知识，才爬取成功。虽然最后爬取成功，但其中还有许多的问题不会解决，例如爬取的时间过长。本次作业中我体会到在以后进行爬虫，应该充分分析网页结构选择好爬虫的方法再进行爬虫，否则容易很迷茫。在爬虫的过程中，新学习到的知识有pyppeteer、asyncio、tkinter等模块的使用。觉得学到的最宝贵的知识是Pyppeteer，它是一个非常强大的自动化测试工具。

## （3.2）数据统计心得体会

​		以前了解过正则表达式，但缺乏实践的运用，无法准确运用。本次作业的数据提取许多地方都用到了正则表达式，我从中学习到了更多关于正则表达式的知识，也感受到了正则表达式对于准确提取数据的强大作用。还新学习了openpyxl和xlwt模块，将数据正确导入Excel。这是我以前没有掌握的知识。还有就是，在数据统计之前，应该充分分析要提取数据在文本中的特点，才能更快的写出相对应的正则表达式。当然，我觉得想正确掌握正则表达式更重要的是要多加练习实践。

## （3.3）数据可视化心得体会

​		我的数据可视化做的比较简单，只是做了柱状图。以前没有接触过数据可视化，通过b站学习了如何利用excel中的数据制作可视化图表。一开始我照着模板制作的柱状图很丑，后来通过网上搜索资料才添加了一些参数设置柱状图的一些细节，勉强才看得过去。至于可视化大屏，还没有学习实践，只是在pyecharts中了解了一点相关内容。本次作业让我更深的意识到自己各方面的薄弱，还需要多加学习。

