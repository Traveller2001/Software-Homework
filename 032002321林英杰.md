# 2022软工K班个人编程任务

## 

​	个人任务Github链接[2021/032002321 at main · 9531lyj/2021 (github.com)](https://github.com/9531lyj/2022Software-Homework/tree/mywork)

#   一、PSP表格

软件工程第一次个人作业

## (2.1)PSP****表格**记录下你估计将在程序的各个模块的开发上耗费的时间。（3）

| ***\*PSP2.1\****                      | ***\*Personal Software Process Stages\**** | ***\*预估耗时（分钟）\**** | ***\*实际耗时（分钟）\**** |
| ------------------------------------- | ------------------------------------------ | -------------------------- | -------------------------- |
| Planning                              | 计划                                       | 10                         |                            |
| Estimate                              | 估计这个任务需要多少时间                   | 1440*3                     |                            |
| Development                           | 开发                                       | 1440                       |                            |
| Analysis                              | 需求分析 (包括学习新技术)                  | 1060                       |                            |
| Design Spec                           | 生成设计文档                               | 20                         |                            |
| Design Review                         | 设计复审                                   | 30                         |                            |
| Coding Standard                       | 代码规范 (为目前的开发制定合适的规范)      | 60                         |                            |
| Design                                | 具体设计                                   | 20                         |                            |
| Coding                                | 具体编码                                   | 1440                       |                            |
| Code Review                           | 代码复审                                   | 60                         |                            |
| Test                                  | 测试（自我测试，修改代码，提交修改）       | 120                        |                            |
| Reporting                             | 报告                                       | 60                         |                            |
| Test Repor                            | 测试报告                                   | 60                         |                            |
| Size Measurement                      | 计算工作量                                 | 30                         |                            |
| Postmortem & Process Improvement Plan | 事后总结, 并提出过程改进计划               | 20                         |                            |
| 合计                                  |                                            |                            |                            |

## (2.2)各个模块上实际花费的时间（3）

| ***\*PSP2.1\****                      | ***\*Personal Software Process Stages\**** | ***\*预估耗时（分钟）\**** | ***\*实际耗时（分钟）\**** |
| ------------------------------------- | ------------------------------------------ | -------------------------- | -------------------------- |
| Planning                              | 计划                                       | 10                         | 10                         |
| Estimate                              | 估计这个任务需要多少时间                   | 1440*3                     | 1440*2                     |
| Development                           | 开发                                       | 1440                       | 1200                       |
| Analysis                              | 需求分析 (包括学习新技术)                  | 60                         | 30                         |
| Design Spec                           | 生成设计文档                               | 20                         | 60                         |
| Design Review                         | 设计复审                                   | 30                         | 60                         |
| Coding Standard                       | 代码规范 (为目前的开发制定合适的规范)      | 60                         | 30                         |
| Design                                | 具体设计                                   | 20                         | 30                         |
| Coding                                | 具体编码                                   | 1440                       | 1200                       |
| Code Review                           | 代码复审                                   | 60                         | 120                        |
| Test                                  | 测试（自我测试，修改代码，提交修改）       | 120                        | 60                         |
| Reporting                             | 报告                                       | 60                         | 10                         |
| Test Repor                            | 测试报告                                   | 60                         | 60                         |
| Size Measurement                      | 计算工作量                                 | 30                         | 10                         |
| Postmortem & Process Improvement Plan | 事后总结, 并提出过程改进计划               | 20                         | 40                         |
| 合计                                  |                                            | 7730                       | 5800                       |

## 

 # 二、任务要求的实现

## (3.1)**项目设计与技术栈**

项目设计

通过requests模块访问网页，通过selenium模拟请求处理反爬，通过xpath、正则表达式解析文本；通过pandas的DataFrame处理数据生成excel，通过pyecharts实现可视化（大屏）。主要通过观看视频，实践爬了几个网站，和同学交流互助不断学习进步。

技术栈：python的基本使用，爬虫入门，数据分析入门，可视化入门

## (3.2)**爬虫与数据处理**（20）

业务逻辑：通过requests模块访问网页，通过selenium模拟请求处理反爬，通过xpath、正则表达式解析文本；通过pandas的DataFrame处理数据生成excel，通过pyecharts实现可视化（大屏）

经过专业pycharm 专业版功能profile 后生成的函数调用流程图如下

![爬虫函数流程图](https://s1.328888.xyz/2022/09/18/o8OtF.png)

由于诸多模块调用许多内置的模块等，故还需详细讲述各个函数的大致功能。

![main.py（依次调用如下函数）](F:\Pictures\main.py（依次调用如下函数）.png)

### (1)获取子网页的所有链接

通过看入门视频,实际操作,查看网络csdn/博客园/StackOverflow的教程实践学习requests模块,lxml中的xpath解析模块

```python
import requests
from lxml import etree

test_url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_4.shtml'
first_url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml'
part_url = 'http://www.nhc.gov.cn'
first_kid_url = 'http://www.nhc.gov.cn/xcs/yqtb/202209/78ea88c5c23e41c391376ee9b103cfec.shtml'
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0"}
filename = 'child.txt'
def get_child_page():
    for i in range(1, 42):
        if (i==1):
            re_url = first_url;
        else :
            # 合成每一页的母网页，为了获取子连接的后部分
            next_url = '/xcs/yqtb' + f'/list_gzbd_{i}' #:
            re_url = part_url + next_url + '.shtml'
        res = requests.get(url=re_url, headers=headers)
        res_text = res.text
        tree = etree.HTML(res_text)
        child_part_url = tree.xpath('/html/body/div[3]/div[2]/ul//li/a/@href')
        while ( len(child_part_url) == 0 ):  # 没有得到正常网页,重复请求
            res = requests.get(url=re_url, headers=headers)
            res_text = res.text
            tree = etree.HTML(res_text)
            child_part_url = tree.xpath('/html/body/div[3]/div[2]/ul//li/a/@href')
        with open(filename, 'a') as ob: # 将子网页的链接批量保存下来
            for j in child_part_url:
                print(j+'was Done!')
                ob.write(part_url+j+'\n')
        print(f'Page {i} is finished!!! \n')
```

### (2)获取子网页所对应的文本信息 :

通过看入门视频,实际操作,查看网络博客园/StackOverflow的教程实践学习selenium模块模拟浏览器发送请求,通过multiprocessing模块优化爬取速率, python文件读取保存

```python
from selenium import webdriver
from lxml import etree
front_name = './raw/'
bro = webdriver.Firefox()
bro.get(first_kid_url)   # line -> url
page_text = bro.page_source
tree2 = etree.HTML(page_text)
def get_data_by_selenium():
    with open('child.txt', 'r') as file_obj:
        for line in file_obj:
            # print(line);  # eg  http://www.nhc.gov.cn/xcs/yqtb/202209/78ea88c5c23e41c391376ee9b103cfec.shtml
            bro.get(line)   # line -> url,通过浏览器访问网页
            page_text = bro.page_source
            while ('疫情通报' not in page_text):  # 处理异常的网页响应
                bro.get(line)
                page_text = bro.page_source
                tree2 = etree.HTML(page_text)
            tits = tree2.xpath('//div[@class="tit"]/text()')
            paras = tree2.xpath('//div[@class="con"]//text()')
            # 将文本保存至本地: 文件路径+发布时间+ 标题 +.txt
            for tit in tits:
                fname = front_name + tit + '.txt'
                with open(fname, 'a', encoding='utf-8') as f:
                    # 'gbk' codec can't encode character '\u2002' in position 0: illegal multibyte sequence
                    f.write(tit)
                    for para in paras:
                        f.write(para)

```



### (3)依次访问 本地文件并解析文本并且生成excel表格:(以获取本土新增为例)

本模块主要与同学一起交流分析文本结构,通过正则表达式处理各种异常状况,清理数据,并且通过看视频/精读delftstack相关文档，初步了解认识pandas中DataFrame数据结构的使用,对excel的增删改查.

```python
def get_asymptomatic(path):  # 获取无症状
    path_list = os.listdir(path)
    date_number = 0
    time_city_dic = {}
    for file_list in path_list:
        date_number += 1
        province_list = {'河北': 0, '山西': 0, '辽宁': 0, '吉林': 0, '黑龙江': 0, '江苏': 0, '浙江': 0, '安徽': 0,
                         '福建': 0,
                         '江西': 0, '山东': 0, '河南': 0, '湖北': 0, '湖南': 0, '广东': 0, '海南': 0, '四川': 0, '贵州': 0,
                         '云南': 0,
                         '陕西': 0, '甘肃': 0, '青海': 0, '内蒙古': 0, '广西': 0, '西藏': 0, '宁夏': 0,
                         '新疆': 0, '北京': 0,
                         '天津': 0, '上海': 0, '重庆': 0, '台湾': 0, '香港': 0, '澳门': 0, '兵团': 0, '中国大陆（无港澳台）': 0}
        ex = r'(\d{4}-\d{2}-\d{2})'
        date = re.findall(ex, file_list)
        path = r'D:\Working_Dir\Python.works\MadeinPycharm\untitled\Yiqing\contents' + '\\' + file_list
        with open(path, "r", encoding='utf-8') as f:
            file = f.readlines()
        filecontent = ''.join(file)
        if ("无症状" in filecontent):
            ex = "(新增无症状感染者.*)"
            str_test = re.findall(ex, filecontent)
            if (len(str_test) != 0):  # 匹配目标段落
                ex = "新增无症状感染者(\d+)例"  # 匹配新增无症状感染者后的总人数
                all_infected_num_list = re.findall(ex, str_test[0])
                ex = "新增无症状感染者\d+例[，]?(（.*?）)?"
                bracket_content = re.findall(ex, str_test[0])
                if (len(bracket_content) != 0 and bracket_content[0] != ''):  # 匹配括号
                    ex = ".*?(\d+).+"
                    input_num = re.findall(ex, bracket_content[0])  # 匹配括号里是否有数字
                    if (len(input_num) != 0):
                        province_list["中国大陆（无港澳台）"] = int(all_infected_num_list[0]) - int(input_num[0])
                        print(str(date[0]) + '日无具体具体各省份信息！！！')
                else:  # 无括号
                    ex = "本土(\d+)例"
                    all_infected_num_list = re.findall(ex, str_test[0])
                    if (len(all_infected_num_list) != 0):
                        province_list["中国大陆（无港澳台）"] = int(all_infected_num_list[0])
                        ex = "本土\d+例[，]?(（.*?）)?"  # 匹配本土100例后的括号里的内容
                        every_province_list = re.findall(ex, str_test[0])
                        if (len(every_province_list) != 0 and every_province_list[0] != ''):
                            jishu = 0
                            for city in province_list.keys():
                                # ----------------------------------
                                if (len(str_test) != 0):
                                    if (city in str_test[0]):
                                        ex = city + '(\d*)例'
                                        num = re.findall(ex, str_test[0])
                                        if (len(num) != 0):  # 正常找到多少例：如括号内的北京2例
                                            province_list[city] += int(num[0])
                                            jishu += int(num[0])
                                        else:  # 本土病例x例（在山西）
                                            ex = '本土(\d.*?)例.*?'
                                            num = re.findall(ex, str_test[0])
                                            if (len(num) != 0):
                                                # -----------------------------------
                                                if (city == '河北' and "河北区" in str_test[0]):  # 2022-02-03 天津:河北区三例
                                                    continue
                                                # --------------------------------------
                                                province_list[city] += int(num[0])
                                                jishu += int(num[0])

                                            else:
                                                province_list[city] += 1  # 2020-05-03 1例为本土病例（在山西）
                                                province_list["中国大陆（无港澳台）"] += 1
        time_specific = str(date_number) + '.' + date[0]
        time_city_dic[time_specific] = province_list
        print(str(date[0]) + '日信息录入完毕！！！')
    df = pd.DataFrame.from_dict(time_city_dic, orient='index')
    df.to_excel('中国每日本土新增无症状人数（转置版）.xlsx')
    df = df.T
    df.to_excel('中国每日本土新增无症状人数.xlsx')
    print("中国每日本土新增无症状人数已完成！！！")
    pass
    # PS: 以上解析文本详细数据的函数（get_asymptomatic()）参考自福州大学2020级计算机03班瞿林杰同学,特此感谢

```

### (4)excel可视化生成对应年份数据图

```python
import pyecharts.options as opts
from pyecharts.globals import ThemeType
from pyecharts.commons.utils import JsCode
from pyecharts.charts import Timeline, Grid, Bar, Map, Pie, Line
from typing import List

# 学习Map gallery中各种类型的图的参数含义,通过借鉴模仿添补修改,使用pyecharts生成可视化大屏
# 函数的样例
def get_year_chart(year: str):
    map_data = [
        [[x["name"], x["value"]] for x in d["data"]] for d in data if d["time"] == year
    ][0]
    min_data, max_data = (minNum, maxNum)
    data_mark: List = []
    i = 0
    # for x in time_list[600:950]:
    for x in time_list:
        if x == year:
            data_mark.append(total_num[i])
            # print(total_num[i],' ',str(i))
        else:
            data_mark.append("")
        i = i + 1
    # print(data_mark)
    map_chart = (
        Map()
        .add(
            series_name="",
            data_pair=map_data,
            zoom=1,
            center=[119.5, 34.5],
            is_map_symbol_show=False,
            itemstyle_opts={
                "normal": {"areaColor": "#323c48", "borderColor": "#404a59"},
                "emphasis": {
                    "label": {"show": Timeline},
                    "areaColor": "rgba(255,255,255, 0.5)",
                },
            },
        )
        .set_global_opts(
            title_opts=opts.TitleOpts(
                title="" + str(year) + "全国分地区GPD情况（单位：亿） 数据来源：国家统计局",
                subtitle="",
                pos_left="center",
                pos_top="top",
                title_textstyle_opts=opts.TextStyleOpts(
                    font_size=25, color="rgba(255,255,255, 0.9)"
                ),
            ),
            tooltip_opts=opts.TooltipOpts(
                is_show=True,
                formatter=JsCode(
                    """function(params) {
                    if ('value' in params.data) {
                        return params.data.value[2] + ': ' + params.data.value[0];
                    }
                }"""
                ),
            ),
            visualmap_opts=opts.VisualMapOpts(
                is_calculable=True,
                dimension=0,
                pos_left="30",
                pos_top="center",
                range_text=["High", "Low"],
                range_color=["lightskyblue", "yellow", "orangered"],
                textstyle_opts=opts.TextStyleOpts(color="#ddd"),
                min_=min_data,
                max_=max_data,
            ),
        )
    )
    line_chart = (
        Line()
        .add_xaxis(time_list)
        # .add_xaxis(time_list[600:950])
        # .add_yaxis("", total_num1[600:950])
        .add_yaxis("", total_num1)
        .add_yaxis(
            "",
            data_mark,
            markpoint_opts=opts.MarkPointOpts(data=[opts.MarkPointItem(type_="max")]),
        )
        .set_series_opts(label_opts=opts.LabelOpts(is_show=False))
        .set_global_opts(
            title_opts=opts.TitleOpts(
                title="中国每日本土新增无症状(单位:人）", pos_left="72%", pos_top="5%"
            )
        )
    )
    bar_x_data = [x[0] for x in map_data]
    bar_y_data = [{"name": x[0], "value": x[1][0]} for x in map_data]
    bar = (
        Bar()
        .add_xaxis(xaxis_data=bar_x_data)
        .add_yaxis(
            series_name="",
            y_axis=bar_y_data,
            label_opts=opts.LabelOpts(
                is_show=True, position="right", formatter="{b} : {c}"
            ),
        )
        .reversal_axis()
        .set_global_opts(
            xaxis_opts=opts.AxisOpts(
                max_=maxNum, axislabel_opts=opts.LabelOpts(is_show=False)
            ),
            yaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(is_show=False)),
            tooltip_opts=opts.TooltipOpts(is_show=False),
            visualmap_opts=opts.VisualMapOpts(
                is_calculable=True,
                dimension=0,
                pos_left="10",
                pos_top="top",
                range_text=["High", "Low"],
                range_color=["lightskyblue", "yellow", "orangered"],
                textstyle_opts=opts.TextStyleOpts(color="#ddd"),
                min_=min_data,
                max_=max_data,
            ),
        )
    )
    pie_data = [[x[0], x[1][0]] for x in map_data]
    pie = (
        Pie()
        .add(
            series_name="",
            data_pair=pie_data,
            radius=["15%", "35%"],
            center=["80%", "82%"],
            itemstyle_opts=opts.ItemStyleOpts(
                border_width=1, border_color="rgba(0,0,0,0.3)"
            ),
        )
        .set_global_opts(
            tooltip_opts=opts.TooltipOpts(is_show=True, formatter="{b} {d}%"),
            legend_opts=opts.LegendOpts(is_show=False),
        )
    )
    grid_chart = (
        Grid()
        .add(
            bar,
            grid_opts=opts.GridOpts(
                pos_left="10", pos_right="45%", pos_top="50%", pos_bottom="5"
            ),
        )
        .add(
            line_chart,
            grid_opts=opts.GridOpts(
                pos_left="65%", pos_right="80", pos_top="10%", pos_bottom="50%"
            ),
        )
        .add(pie, grid_opts=opts.GridOpts(pos_left="45%", pos_top="60%"))
        .add(map_chart, grid_opts=opts.GridOpts())
    )
    # print(map_data[0])
    return grid_chart

```



## (3.3)**数据统计接口部分的性能改进**（6）

性能分析截图：（使用pycharm 专业版 内置工具 profile) 最耗时（wait函数）

![call graph](https://s1.328888.xyz/2022/09/18/o8OtF.png)

![函数性能分析图](https://s1.328888.xyz/2022/09/19/2mPgB.png)

![图2](https://s1.328888.xyz/2022/09/19/2FgSX.png)

![image-20220919192047514](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220919192047514.png)

![image-20220919192119131](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220919192119131.png)第一次为了处理爬取到的网页受到加密而乱码，学习了selenium模块，大概爬了一个晚上（晚上11点之次日8点），profile 性能分析工具没来得及使用。后来使用了multiprocessing模块，使用了Pool完成多进程，改造为get_child_text_multiPool（）将程序运行速度提高到24分钟左右~~（虽然目前用profile分析仍然占用主函数90%以上运行时间）~~

## (3.4)**每日热点的实现思路**：(6)（未完成。。。）

热点事件发现是话题发现与跟踪技术（TDT）在实际领域中的应用。每天的批处理过程主要是对该天的语料做第一层聚类，即凝聚聚类，得到每天的微类。实时处理过程则是对某个时间段内所有天的微类，按照时间的先后顺序，做第二层聚类，即signle-pass聚类，得到一个事件列表。接着利用事件热度计算公式，对候选事件进行过滤和排序，得到最终的热点事件。

内容来源：卫健委疫情通报

#### single pass 单程事件识别算法

根据某一单独事件与某一类事件进行相应数据计算和分析。

其需要的数据有：（1）事件的关键字及其出现频率；（2）事件相关的话题出现和查询的频率。

#### KNN最近邻事件分类识别算法

KNN最近邻事件是指某一热点事件范围内所出现的不同角度的各种话题，能够全面准确地对当前热点事件的状态。

**分析方法：**

1)汉语分词与词性标注 2)利用交叉信息熵计算有代表性的关键词w，权重

![image-20220919193519540](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220919193519540.png)

**步骤:**

**第一步：数据读取**

```python
def get_new_confirmed(path):
    path_list = os.listdir(path)
    path_list = sorted(path_list, reverse=True)  #
	for file_list in path_list:
        #组合成一篇篇文本
        with open(path, "r", encoding='utf-8') as f:
            file = f.readlines()
            filecontent = ''.join(file)
```



**预处理函数**

去除数字字母 和停用词
 停用词链接：[nlp 中文停用词数据集](https://blog.csdn.net/kobeyu652453/article/details/106939559)

```python
# 定义删除除字母,数字，汉字以外的所有符号的函数
def remove_punctuation(line):
    line = str(line)
    if line.strip() == '':
        return ''
    rule = re.compile(u"[^a-zA-Z0-9\u4E00-\u9FA5]")
    line = rule.sub('', line)
    return line

#停用词
def stopwordslist(filepath):
    stopwords = [line.strip() for line in open(filepath, 'r', encoding='gbk').readlines()]
    return stopwords
```

### 用于信息检索（information retrieval）与文本挖掘（text mining）的常用**加权技术**tf-id算法

### **Jieba实现TF-IDF算法**



```python
import jieba.analyse
	# to be continued
```

### TF-IDF优缺点：

**实现简单快速**

TF-IDF算法的不足

TF-IDF 采用文本逆频率 IDF 对 TF 值加权取权值大的作为关键词，但 IDF 的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以 TF-IDF 算法的精度并不是很高，尤其是当文本集已经分类的情况下。

在本质上 IDF 是一种试图抑制噪音的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用。这对于大部分文本信息，并不是完全正确的。IDF 的简单结构并不能使提取的关键词， 十分有效地反映单词的重要程度和特征词的分布情 况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被盖。

### 可能的改进方法:

**[改进的 TF-IDF 关键词提取方法](https://image.hanspub.org/pdf/CSA20130100000_81882762.pdf)**

## (3.5)**数据可视化界面的展示**（15）

![爬虫可视化大屏展示](https://s1.328888.xyz/2022/09/18/oUEw6.gif)

![爬虫无症状可视化动图](https://s1.328888.xyz/2022/09/18/oUFHK.gif)

组件：包含每年（包含每一天）所对应的每日本土新增确诊、新增无症状的中国地图，饼图，柱状图以及从疫情开始统计至今为止的折线图。

设计思路·：以可以缩放、并且随着各省人数不同而带来的颜色深浅、层次不同的中国地图（map）为中心，开篇名义。左上角附有人数对应的颜色属性，左下与右下又有着随着右侧时间轴（timeline）自然而然地缓缓滑动而不断律动的柱状图（bar）和饼图（pie），让人不禁感到计算机可视化的方便、优雅。最后一张折线图（line）便可以清晰明了了解到中国整体疫情情况。

# 三、心得体会

1. 本次个人作业，我收获颇丰，清醒的意识到在短时间内要想完成一项“工程”，个人的力量实在有限。在时间限制下，在面对毫无概念的项目时，如果直接埋头苦干，有时说不定会南辕北辙，虽然努力了却又没有完成相应任务~~（比如机器学习获取今日热点部分）~~

2. 学习伙伴在学习生涯的重要性实在太大了。面对同一个目标，有时一个人钻研无果，但是与另一个，另外一堆伙伴，在突击目标后的激烈讨论，吐槽，有时候也会带来1+1大于2的意料之外的效果。虽然是个人作业，但是互相学习也无可厚非。有前辈指点，势必事半功倍。

3. 第一次实现了勉强能看的可视化，完成了大一未能完成的心愿。也清楚地意识到，网络上已经有许多造好的轮子了，在我们懵懂无知的对于许多工具的实现原理一头雾水的时候，已经能够通过大犇的成果制作一些从前看起来很遥远的小工具了~~。（暂不考虑有多少bug，毕竟也几乎算是初尝试）~~

4. Learning by doing. 

5. 工欲善其事必先利其器。

6. 有志与力，而又不随之以怠，至于幽暗昏惑而无物以相之，亦不能至也。然力足以至焉，于人为可讥，而在己为有悔；尽吾志也而不能至者，可以无悔矣。

7. 附加题额外功能实现：

   ​				

   ​	

   ​	

   