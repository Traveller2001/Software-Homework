[GITHUB作业链接](https://github.com/dreammak3r/032002122/tree/main/032002122)
# 一、PSP表格
|  PSP2.1 | Personal Software<br> Process Stages | 预估耗时（分钟）  | 实际耗时（分钟）  |
|:---|:---|:---|:---|
| Planning  | 计划  |  30| 30  |
| · Estimate  | · 估计这个任务需要多少时间  | 30  |  30 |
|Development   | 开发  |  6450 | 10080  |
| · Analysis  | · 需求分析 (包括学习新技术)  |  6000 | 8815  |
| · Design Spec  | · 生成设计文档  | 60  | 60  |
| · Design Review  | · 设计复审  | 20  | 30  |
|· Coding Standard   | · 代码规范 (为目前的开发制定合适的规范)  | 10  | 20  |
| · Design  | · 具体设计  | 60  |  75 |
|· Coding   |  · 具体编码 |  240 | 900  |
| · Code Review  | · 代码复审  | 30  | 60  |
| · Test  | · 测试（自我测试，修改代码，提交修改）  |  30 |  120 |
| Reporting  | 报告  |  120 | 150  |
| · Test Repor  | · 测试报告  |  60 | 60  |
|· Size Measurement   | · 计算工作量  | 30  |  20 |
| · Postmortem & Process<br>Improvement Plan  |  · 事后总结, 并提出过程改进计划 |  30 | 70  |
|   | · 合计  |  6600 | 10260  |

# 二、任务要求的实现
## A.项目设计与技术栈
 1.环节：
☞ 模块设计
☞ 具体功能实现
☞ 程序性能优化

 2.技术栈：
☞ 爬虫技术
☞ 正则表达式
☞ html
☞ xpath解析
☞ python操作xlsx文件
## B.爬虫与数据处理
☞ 爬取网页并解析HTML获取纯文本的处理代码，使用requests库，每日疫情的详细信息在疫情通报里面，所以要先爬疫情通报将每日的url获取到再爬文本，且疫情通报有40多页，每页的url都有规律所有使用for循环进行爬取
```python
	for i in range(1, range_up_limit):
        if i == 1:
            url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml'
        else:
            url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_{}.shtml'.format(i)

        req = requests.get(url, headers=headers)
        e = etree.HTML(req.text)
        html = e.xpath('//div[@class="w1024 mb50"]/div[@class="list"]/ul[@class="zxxx_list"]/li/a/@href')
        title = e.xpath('//div[@class="w1024 mb50"]/div[@class="list"]/ul[@class="zxxx_list"]/li/a/@title')



        for html_ite, title_c, count_line in zip(html, title, range(len(html))):
            row = 1 + (i - 1) * 24 + count_line + 1

            #省份转换成字典
            adata = dict.fromkeys(app_issue, 0)
            ndata = dict.fromkeys(not_issue, 0)

            # 访问网站并解析html
            surl = "http://www.nhc.gov.cn" + html_ite
            resq = requests.get(surl, headers=headers)
            se = etree.HTML(resq.text)

            # 获取纯文本信息
            words_num = se.xpath('string(.)')
            text = ''.join(words_num)
```
☞ 文本解析利用正则表达式， 具体思路是先将网页的纯文字信息提取出来然后拼接为完整的文章，再利用正则表达式将文章分为三段，第一段为新增确诊，第二段为新增无症状，第三段为港澳台累计数据，第三段用于计算港澳台每日新增，一、二段用于获取大陆每个省份的疫情信息。然后再用正则将每段“省份XX例”提取出来，但是经过分析有可能会提取到市或区的疫情信息，为了解决该问题我事先导入省份的列表，录入数据时先将前面的汉字提取出来判断是否在省份列表里，在的话录入，不在丢弃。

```python
            # 单独获取新增确诊和新增无症状和港澳台三段文字且进行拼接
            # 正则表达式模式
            yiqing_pattern = '[\u4e00-\u9fa5]{2,}[0-9]+(?=例|人)'
            first_pattern = re.compile('新增确诊[\u4e00-\u9fa5]+[0-9]+例[。|，][^。]+|新增无症状[\u4e00-\u9fa5]+[0-9]+例[。|，][^。]+|累计收到[\u4e00-\u9fa5]+[0-9]+[^ ]+')
            digit_pattern = '[0-9]+'
            hanzi_pattern = '[\u4e00-\u9fa5]+'
            date_pattern = '[0-9]+月[0-9]+日'
            in_in_pattern = '本土病例[0-9]+例[^。）]+|本土[0-9]+例[^。）]+|累计收到[\u4e00-\u9fa5]+[0-9]+[^ ]+'

            #获取日期信息
            time_data = re.findall(date_pattern, title_c)
            time_data_final = ''.join(time_data)

            # 文本处理
            says_1 = re.findall(first_pattern, text)
            temp_1 = ''.join(says_1)
            says_2 = re.findall(in_in_pattern, temp_1)
            temp_2 = '\n'.join(says_2)
            final_says = re.findall(yiqing_pattern, temp_2)
```
☞ 初始化excel表格，创建新增确诊和新增无症状确诊两个工作表并初始化省份和日期信息
```python
def init_xls():
    wb = openpyxl.Workbook()
    del wb["Sheet"]
    sheet_a = wb.create_sheet("新增普通")
    sheet_n = wb.create_sheet("新增无症状")

    sheet_a.cell(1, 1, "日期")
    for x1, x2 in zip(range(2, 40), app_issue):
        sheet_a.cell(1, x1, x2)
    # 初始化本土无症状新增表格头
    sheet_n.cell(1, 1, "日期")
    for x1, x2 in zip(range(2, 34), not_issue):
        sheet_n.cell(1, x1, x2)

    return wb
```

☞ 使用openpxyl库，写入excel表格，通过表头的信息查找字典中有没有该省份的信息，有的话填入表格

```python
def write_into_xls(workbook, time_data_final, i, count_line, len_html, adata, ndata, range_up_limit):
    #获取表格工作表
    sheet_a = workbook["新增普通"]
    sheet_n = workbook["新增无症状"]

    #计算该写入的行数
    row = 1 + (i - 1) * 24 + count_line + 1

    #普通新增数据写入
    sheet_a.cell(row, 1).value = time_data_final#写入普通新增的日期
    for x1 in range(2, 40):
        if (sheet_a.cell(1, x1).value) in adata:
            sheet_a.cell(row, x1).value = adata[sheet_a.cell(1, x1).value]


    #无症状新增数据写入
    sheet_n.cell(row, 1).value = time_data_final
    for x1 in range(2, 34):
        sheet_n.cell(row, x1).value = ndata[sheet_n.cell(1, x1).value]

    #保存excel文件

    workbook.save("covid-19.xlsx")
```
☞ 使用pycharts的组件生成可视化大屏，并自动打开

```python
def page_draggable_layout(today_covid_adata, today_not_app_covid_adata, line_time_data, line_covid_data, line_not_app_covid_data, bar_time_data, bar_covid_data, bar_not_app_covid_data, tl, time_data_final):
    page = Page(layout=Page.DraggablePageLayout)
    page.add(
        map_3d_creat(today_covid_adata,today_not_app_covid_adata),
        bar_create(bar_time_data, bar_covid_data, bar_not_app_covid_data),
        line_create(line_time_data, line_covid_data, line_not_app_covid_data),
        word_cloud_creat(today_covid_adata, today_not_app_covid_adata, True),
        word_cloud_creat(today_covid_adata, today_not_app_covid_adata, False),
    )
    page.render("page_draggable.html")
    page.save_resize_html("page_draggable.html",cfg_file= "firstall.json", dest="my_page_charts.html")
    os.system("my_page_charts.html")
```

## C.数据接口部分的性能改进
![使用pycharm自带的性能分析工具的结果](https://img-blog.csdnimg.cn/ad11bb20c4c64d0683b9ed6bd40bf5c3.png#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/50abd5550e5f4d0b90db6d6315e7fd13.png#pic_center)
☞ 经过查找耗时最多的两个模块均为为requests库get方法里的某个接口，也就是请求网页的耗时最多，经过分析目前没有什么好的方法减少时间，然后我将crwal函数的一个多余的循环省掉了，crwal函数的时间下降了1秒
## D.每日热点的实现思路
```python
 #每日热点
            flag = 1
            for key, value in adata.items():
                if value >= 100:
                    print(key+'今日新增确诊超过百例\n')
                    flag = 0
            if flag:
                print('今日新增无太大增长')
            flag = 1
            for key, value in ndata.items():
                if value >= 100:
                    print(key + '今日新增无症状超过百例\n')
                    flag = 0
            if flag:
                print('今日新增无症状无太大增长')
```
☞ 1. 通过简单的判断找出今日疫情波动较大的省份

```python
    if a == True:
        c = (
            WordCloud(init_opts=opts.InitOpts(width="1600px", height="900px", theme=ThemeType.WESTEROS, chart_id="今日新增"))
            .add(series_name="今日新增确诊", data_pair=data_a, word_size_range=[30, 90])
            .set_global_opts(
                title_opts=opts.TitleOpts(
                    title="今日新增确诊", title_textstyle_opts=opts.TextStyleOpts(font_size=20)
                ),
                tooltip_opts=opts.TooltipOpts(is_show=True), legend_opts=opts.LegendOpts(selected_mode='single')
            )
        )
        return c
    else:
        c = (
            WordCloud(init_opts=opts.InitOpts(width="1600px", height="900px", theme=ThemeType.WESTEROS, chart_id="今日新增无症状"))
            .add(series_name="今日新增无症状", data_pair=data_n, word_size_range=[30, 100])
            .set_global_opts(
                title_opts=opts.TitleOpts(
                    title="今日新增无症状", title_textstyle_opts=opts.TextStyleOpts(font_size=20), pos_bottom=0, pos_right=0
                ),
                tooltip_opts=opts.TooltipOpts(is_show=True), legend_opts=opts.LegendOpts(selected_mode='single')
            )
        )
        return c
```
☞ 2. 通过pyecharts提供的词云图将疫情热点做成词云图
![在这里插入图片描述](https://img-blog.csdnimg.cn/6550bac06bdd454485fc32720d94b135.png)

## E.数据可视化界面的展示
![在这里插入图片描述](https://img-blog.csdnimg.cn/8559b3b2141c4fd19fb96685968ec837.png#pic_center)

☞ 通过pyecharts库生成各种图表并拼接在一起生成数据大屏，地图为3D形式可以旋转，词云图数据为今日新增确诊和今日新增无症状，柱状图和折线图展示了所爬取的全部疫情信息。地图、折线图、柱状图均可动态切换新增确诊和新增无症状的数据。上面所有图表均为pyecharts生成，然后再利用pyecharts的page组件将其拼接并保存位置信息为json文件，再利用page组件自带的函数导入位置信息，使得每一次生成图表都能获得正确的位置信息

# 三、心得体会
☞ 通过这次个人编程作业，本人感觉被很很的PUSH了一把，学到了很多东西很开心。一开始在听到这次作业的题目时，有点错愕，因为之前没有接触过爬虫不太了解相关的技术栈，没有什么经验。一开始是打算使用JAVA语言来进行爬虫的编写，大约学习了三天的时间发现JAVA爬虫对于没有JAVA基础的人来说不好写，网上关于JAVA爬虫的教程比较少，遂萌生了放弃的念头。后面看到室友们使用的都是python，遂也转用python来做，现在只想说一句真香。python关于爬虫的教程网上非常多完全不愁，在浪费了三天时间后遂马上开始爬虫的学习。

☞ 在这次作业中收获最大的就是了解了html、网页结构、爬虫编写、正则表达式等技术栈，同时掌握了爬虫的编写。通过这次作业让我感受到了付出的快乐，将近两个星期的学习和编程让我收获了许多，同时真正感受到了计算机技术的实际应用，让我燃起了对计算机技术的激情，同时也更加期待结对作业。
