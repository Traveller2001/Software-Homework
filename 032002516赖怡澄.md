# 1.1Github链接
https://github.com/slor37/s/tree/slor37-032002516
---
# 一、PSP表格
| Personal Software Process Stages         | 预估耗时 （分钟） | 实际耗时 （分钟） |
| ---------------------------------------- | ----------------- | ----------------- |
| 计划                                     | 180               | 200               |
| · 估计这个任务需要多少时间               | 180               | 200               |
| 开发                                     | 10天              | 14天              |
| · 需求分析 (包括学习新技术)              | 5天               | 9天               |
| · 生成设计文档                           | 60                | 70                |
| · 设计复审                               | 10                | 5                 |
| · 代码规范 (为目前的开发制 定合适的规范) | 10                | 5                 |
| · 具体设计                               | 60                | 30                |
| · 具体编码                               | 2天               | 3天               |
| · 代码复审                               | 30                | 20                |
| · 测试（自我测试，修改代 码，提交修改）  | 1天               | 2天               |
| 报告                                     | 120               | 240               |
| · 测试报告                               | 60                | 120               |
| ·计算工作量                              | 30                | 60                |
| ·事后总结, 并提出过程改进 计划           | 30                | 60                |
| ·合计                                    | 12天              | 15天              |

# 二、任务要求的实现
## 项目设计与技术栈
### （2.1.1） 项目设计

> 这次任务主要分成三个大环节，第一步阅读理解题意，第二步实现代码，第三步完成报告。 其中第二步又分为两个环节，爬虫获取数据，数据可视化。
> 第二步爬虫获取数据，首先需要学习python语言与爬虫，学习主要是通过B站课程，当对python与爬虫有大体地了解后，开始尝试写代码。代码学习主要是参考csdn或者博客园或者简书或者知乎，参考研究大佬们的代码（因为爬虫基础为0），并模仿写代码。数据可视化学习与爬虫类似。
### （2.1.2） 技术栈
爬虫部分：

 - requests

 可视化部分：

 - pandas
 - numpy
 - matplotlib

## 爬虫与数据处理


```python
 主要用requests技术栈爬取数据
> def __init__(self) 
  self.area_url  
  self.home_url
 //默认构造函数,要爬的网站的地址
 
> def get_json_from_url(self, url)
 headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36"}
        # 发起请求, get方法会返回一个响应对象
        response = requests.get( url= url,headers = headers)
        # 解析requests返回的response(json格式)
        return response.json()
//根据url,获取响应内容的字符串数据，设置header来隐藏自己，通过get方法向目标url发送get请求，返回响应的结果，是一个response对象

> def data_analyz(self,area_name,data)
  date = data['date']
        add_confirm = data['today']['confirm'] # 新增确诊
        total_confirm = data['total']['confirm'] #累计确诊
        heal = data['total']['heal'] #累计治愈
        dead = data['total']['dead']  #累计死亡
        suspect = data['total']['suspect'] #疑似
        now_confirm = total_confirm - heal -  dead #现有确诊
        return {'area_name':area_name,'date':date,'now_confirm':now_confirm,'add_confirm':add_confirm,'total_confirm':total_confirm,'heal':heal,'dead':dead,'suspect':suspect}
//解析json内容，获取数据（新增确诊，累计确诊，累计治愈等）

> def get_countryAndcity_id(self)
> area_code_list = []
  data_json = self.get_json_from_url(self.area_url)
  data = data_json['data']['areaTree']
         for country_list in data  # 找到国家
            if country_list['name'] == '中国': # 提取国家各省
                for province_list in country_list['children'] :
                    for city_list in province_list['children']:
                    # append函数添加国家名、省名等
                  area_code_list.append({'country_name':country_list['name'],'country_id':country_list['id'],'province_name':province_list['name'],'province_id':province_list['id'],'city_name':city_list['name'],'city_id':city_list['id']})
                  # 省略部分代码
  area_code_df = pd.DataFrame(area_code_list) # 二维标签数据结构
  return area_code_df
//获取国家代码及城市代码，pd.DataFrame二维标签数据结构

> def get_area_yq_data_his(self,area_code,area_name)
  yq_datalist = []
        data_json = self.get_json_from_url(self.home_url+area_code)
        yq_json = data_json['data']['list']
        for data in yq_json:
            # 在原有列表上增加对象
            yq_datalist.append(self.data_analyz(area_name,data))
        return pd.DataFrame(yq_datalist)
//获取不同代码的历史数据，并提取数据

> def list_with(self,Series)
  return ''.join(list(set(Series)))
//去重

> def save_to_excel(self,df,sheetName)
//生成Excel表格

> def get_res_code(self)
  res_area_code_list = []
        area_df = self.get_countryAndcity_id()
        # value_counts()统计次数，index查找指定值的首次出现，将数组作为列表返回
        temp = area_df[area_df.country_name == '中国']['province_name'].value_counts().index.tolist()
        res_area_code_list.append({'area_code':self.list_with(area_df[area_df.country_name == '中国']['country_id']), 'area_name':'中国'})
        # 遍历增添省名
        for te in temp:
            res_area_code_list.append({'area_code':self.list_with(area_df[area_df.province_name == te]['province_id']), 'area_name':te})
        return res_area_code_list
//获取目标区域的id

> def run(self) 
  df = self.get_area_yq_data_his(area['area_code'],area['area_name'])
//运行
```

## 数据统计接口部分的性能改进
使用cProfile分析
[![xingn.jpg](https://i.postimg.cc/prjMM7fq/xingn.jpg)](https://postimg.cc/kR7pxThb)

[![x7.jpg](https://i.postimg.cc/GptN1VrG/x7.jpg)](https://postimg.cc/2qpTQH1S)

![在这里插入图片描述](https://img-blog.csdnimg.cn/dbbcbe72fe8f4fe0a7d8a65f24ab7b0c.jpeg#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/6a8fffc6aa76452eae83398e54bc6e3a.jpeg#pic_center)

改进：减少函数调用的数量，减少函数的调用层次
改进算法，如字典 (dictionary) 与列表 (list)，Python字典中使用了 hash table，因此，查找操作的复杂度为 O(1)，而 list 实际是个数组，在 list 中查找需要遍历整个 list，其复杂度为 O(n)，因此对成员的查找访问等操作字典要比 list 更快。



## 每日热点的实现思路

> 爬百度疫情话题
> 设headers，用requests技术栈去百度中筛选与疫情相关的新闻热点
> req = session.get(url, headers=headers)，
> 如果爬到，return req.text，将信息返回
> 再设一个函数def get_baidu(html)
> response = etree.HTML(html)
> //etree用来把得到的HTML对象，转变成属于lxml.etree._Element的类对象，调用etree的xpath()函数完成标签定位，此时获得的标签其实是xpath()函数返回的对象
> 创建txt文件保存筛选完的热点。

## 数据可视化界面的展示
[![tu1.jpg](https://img-blog.csdnimg.cn/img_convert/307767f85377e6ad5f31dbaceb88c8c7.jpeg)](https://postimg.cc/2qzLNvJx)
[![tu2.jpg](https://img-blog.csdnimg.cn/img_convert/9c593dd48dfcd94f855d3d72996f83fc.jpeg)](https://postimg.cc/tYw8cNsL)


[![333tu.jpg](https://img-blog.csdnimg.cn/img_convert/9af61a9d31898465983f546b26bda69c.jpeg)](https://postimg.cc/v4WMLN0P)

![在这里插入图片描述](https://img-blog.csdnimg.cn/48141692a39849eab888412daf3353fa.jpeg#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/ff3231db8d7a4c2a838fc51ae7764f92.jpeg#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/809344da7f3f421d85a79f83007108f9.jpeg#pic_center)

```python
可视化部分采用pandas、numpy、matplotlib技术栈。
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用黑体显示中文

df = pd.read_excel("yq_data_all.xlsx")  # 用pandas读取疫情数据
df['date'] = pd.to_datetime(df['date']) # 时间序列解析，获取指定的时间和日期
df.set_index('date',inplace=True) # 索引设置
// 用pandas读取疫情数据，并解析时间序列，索引设置

df[df['area_name']=='中国']['add_confirm'].plot() # plot函数绘制线图
plt.title("中国新增确诊") # 设置标题
plt.xlabel("时间") # 为两条坐标轴设置名称
plt.ylabel("数量")
plt.legend() # 显示图例
plt.show()
//plot函数绘制线图

 names = df['area_name'].value_counts().index.tolist()
 temp = df[df['area_name'] == name].iloc[-1,2:-1] #索引切片
 x = temp.index.tolist()
 y = temp.values
//设置xy坐标

 plt.bar(x=x, height=y, label='数量', color='steelblue', alpha=0.8)
 # 在柱状图上显示具体数值, ha参数控制水平对齐方式, va控制垂直对齐方式
 for x1, yy in zip(x, y):
      plt.text(x1, yy + 1, str(yy), ha='center', va='bottom', fontsize=10, rotation=0)
//绘制柱状图
```


# 三、心得体会
这次作业对于我来说非常非常非常地难，因为完全是0基础，不会python，不会爬虫，不会数据可视化，什么都不会。一切都要从头学习，花了九天的时间在b战速成了爬虫（依然非常不熟练），在csdn和博客园参考阅读理解了大佬们爬虫代码与数据可视化代码，才勉强写出作业，这次的作业难度非常大，学的非常崩溃，但也提高了自学能力，对python，爬虫，可视化也有了初步掌握与理解，有所收获。