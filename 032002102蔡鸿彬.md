https://github.com/Cua666/032002102
# 一、PSP表格
|  PSP2.1   |  Personal Software Process Stages |  预估耗时（分钟）|  实际耗时（分钟）|
| :---- | :----  | :----| :---- |
| Planning | 计划 | 4800| |
|· Estimate |· 估计这个任务需要多少时间| 4800| | 
| Development|开发 |3200 |3890 | 
| · Analysis|· 需求分析 (包括学习新技术) | 1200|1500 | 
|· Design Spec |  · 生成设计文档|20 |30 | 
|· Design Review |· 设计复审 |30 |30 | 
| · Coding Standard|· 代码规范 (为目前的开发制定合适的规范) | 30| 30| 
|· Design |· 具体设计 |30 |30 | 
|· Coding  |· 具体编码 |1800 |2160 | 
|· Code Review |· 代码复审| 30|20 | 
|· Test |· 测试（自我测试，修改代码，提交修改） |60 | 90| 
|Reporting | 报告|90 |45 | 
|· Test Repor  |· 测试报告 |30 |15 | 
|· Size Measurement | · 计算工作量|30 |15 | 
|· Postmortem & ProcessImprovement Plan |· 事后总结, 并提出过程改进计划 |30 |15 | 
| |· 合计 | 3290| 3935| 


# 二、任务要求的实现
## 1.项目设计与技术栈。

这一次任务我分成了两个环节：1.爬虫和数据处理；2.数据可视化


本次任务使用python实现，爬虫部分使用requests库，解析html页面使用bs4库中的BeautifulSoup模块， 解析每日通报文本使用re库，编辑并保存excel文件使用openpyxl库， 数据可视化使用pyecharts库， 最后通过webbrowser查看可视化最终得到的html文件。


## 2.爬虫与数据处理。
这一部分使用了四个函数：yqtbCrawler( )、getUrl( )、getData( )、parseData( ).

yqtbCrawler( )是爬虫调用函数，检查当前目录下是否已经有数据，没有的话就调用getUrl( )并且在最后保存excel文件。

getUrl( )通过使用requests的get函数爬取，每个疫情通报目录页，从中得到每日疫情通报页面的URL，之后用这个URL调用getData( )

getData( )继续通过get函数爬取每日通报目录页，并使用BeautifulSoup模块解析得到的html文本，得到每日通报文本，并将它传递给parseData( )

parseData( )使用re库解析每日通报报文并得到各个省份的数据，并通过openpyxl库插入excel对象中。

## 3.数据统计接口部分的性能改进。
使用cProfile、pstats查看接口性能，整个程序消耗最大的函数是爬虫部分reques的get函数，数据统计接口部分耗时最多的是openpyxl库的append函数

![image](https://github.com/Cua666/032002102/blob/master/pic/1.png)
![image](https://github.com/Cua666/032002102/blob/master/pic/2.png)
## 4.每日热点的实现思路。
没做
## 5.数据可视化界面的展示。
![image](https://github.com/Cua666/032002102/blob/master/pic/3.png)

该部分使用pyecharts库实现，用户通过终端输入要查询的日期，如果包含该日期可视化结果的html文件，就直接打开它，否则打开数据excel文件，查找当天数据，并用它构造pyecharts库中的map对象，并存入html文件，最后打开它。

# 三、心得体会        
本次任务我认为最有意义的是学习新技能，包括python语法，Git的使用，python各种库的使用，预计往后Git工具会使用的比较多，也会使用markdown语法编辑资料。通过这次任务， 体验了一次比较完整的开发的经历，从构思、计划、学习技能、编码、测试、修改代码等，体验不错。

