@[TOC](2022软工K班个人编程任务)
<https://github.com/Luozbno1/Software-Homework>
# 一、PSP表格

(1)
PSP2.1   |Personal Software Process Stages| 预估耗时（分钟)|实际耗时（分钟）
-------- | -----|------------- | -----
Planning| 计划| 30|45+
Estimate|  估计这个任务需要多少时间 | 1500| 900
Analysis   |开发 | 1000|800
Design Review  | 设计复审 | 30 |60+
 Design Spec  | 生成设计文档 | 30|30
Coding Standard  | 代码规范 (为目前的开发制定合适的规范) | 20|5
Design   | 具体设计| 60 |30
 Coding  | 具体编码 | 2000 |1500
 Code Review  | 代码复审 | 100 |60
Test  | 测试（自我测试，修改代码，提交修改）| 45 |30
Reporting   | 报告 | 120 |60
Test Repor   | 测试报告|  30|50
Size Measurement   | 计算工作量 | 200 |100
 Postmortem & ProcessImprovement Plan| 事后总结, 并提出过程改进计划 | 50 |30
 ||合计|5215|3700



# 二、任务要求的实现


*(1) 项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。*
学习爬虫的基本用法
开始运用自己所学的知识开始爬取一天疫情通报的数据
爬取完一天的数据成功后，尝试爬取疫情通报页面的数据，获取每天的url，再用第二部的方法获取数据
将每天获取的数据做成表格
技术栈：
selenium，BeautifulSoup，xlwt
*(2) 爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。*
首先是数据的获取，运用常规的爬虫方法容易出现反爬，因此学习了如何使用selenium来爬取网页，接下来就是如何进行想要数据的筛选，在这里我采用了正则表达式，找到关键词，然后得到想要的数据，接下来就是如何获取每天的数据，在这里如果简单的运用for循环会出现报错，应该是由于反爬的问题，为此参考了网络上的资料，也是运用了selenium的方法还有引用了time来进行解决。最后则是如何将得到的数据导入excel表，我选择了用字典的方式，根据网上参考的方法。
数据获取的关键是：（即模拟浏览器进行访问）
```
s = Service("chromedriver.exe")
driver = webdriver.Chrome(service=s)
driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
    "source": """
    Object.defineProperty(navigator, 'webdriver', {
      get: () => undefined
    })
  """
})
driver = webdriver.Chrome()
driver.get("http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml")
time.sleep(1)  # 留出加载时间
```
正则表达式的步骤：
```
temp = re.search('本土病例.*?例（(.*?)）.*?，', province_list[0].get_text()).group(1)
temp1 = re.search('报告新增无症状感染者.*?本土.*?例（(.*?)）.*?。', province_list[0].get_text()).group(1)
date = re.search('(.*?)0.*?，', province_list[0].get_text()).group(1)
quezhen_sum = re.search('本土病例(.*?)例', province_list[0].get_text()).group(1)
wuzhengzhuang_sum = re.search('报告新增无症状感染者.*?本土(.*?)例', province_list[0].get_text()).group(1)
```
*(3) 数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。*
未进行分析
*(4. )每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。*
时间过于紧凑没有实现
*(5.)数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。*
时间过于紧凑没有实现


# 三、心得体会
本次作业收获还是很大，第一次运用爬虫来进行程序的编写。也算是边学习边运用了。对于我这样的爬虫小白来说难度还是很大，再爬取疫情通报栏目的问题上困惑很久，一直被反爬，通过网络查询资料以及和同学讨论也是找到了比较有效的爬取方法。学习到了许多新的知识，每段代码需要如何实现就得自己去琢磨发现，搜查资料，从来没有想过自己也能够实现这样的功能。但是由于时间过于紧凑，作业的很多要求还是没能达到，提交的成品也有许多令自己遗憾的地方。总的来说还是很有成就感的，意识到了自主学习的重要性，以后就算是没有像这样的作业来推动，也要尽可能多的去学习新知识，不断更新完善自己才能进步。

