**(1.1)**在Github仓库中新建一个学号为名的文件夹，同时在**博客正文首行**给出作业Github链接，**并登录软工在线平台完善信息**。**（2'）**

[Github链接](https://github.com/xujinxinya/softwork)

# **一、**PSP表格

**(2.1)**在开始实现程序**之前**，在附录提供的**PSP表格**记录下你估计将在程序的各个模块的开发上耗费的时间。**（3'）**

**(2.2)**在你实现完程序**之后**，在附录提供的**PSP表格**记录下你在程序的各个模块上实际花费的时间。**（3'）**

| **PSP2.1**                              | **Personal Software Process Stages**    | **预估耗时**（小时） | **实际耗时**（小时） |
| --------------------------------------- | --------------------------------------- | -------------------- | -------------------- |
| Planning                                | 计划                                    | 0.5                  | 0.5                  |
| · Estimate                              | · 估计这个任务需要多少时间              | 0.5                  | 0.5                  |
| Development                             | 开发                                    | 35.5                 | 40.2                 |
| · Analysis                              | · 需求分析 (包括学习新技术)             | 4                    | 5                    |
| · Design Spec                           | · 生成设计文档                          | 0.5                  | 0                    |
| · Design Review                         | · 设计复审                              | 0.5                  | 0.5                  |
| · Coding Standard                       | · 代码规范 (为目前的开发制定合适的规范) | 0.5                  | 0.2                  |
| · Design                                | · 具体设计                              | 1                    | 1.5                  |
| · Coding                                | · 具体编码                              | 24                   | 28                   |
| · Code Review                           | · 代码复审                              | 2                    | 2                    |
| · Test                                  | · 测试（自我测试，修改代码，提交修改）  | 3                    | 3                    |
| Reporting                               | 报告                                    | 6.5                  | 7.5                  |
| · Test Repor                            | · 测试报告                              | 1                    | 1                    |
| · Size Measurement                      | · 计算工作量                            | 0.5                  | 0.5                  |
| · Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划          | 5                    | 6                    |
|                                         | · 合计                                  | 43                   | 48.7                 |



# **二、任务要求的实现**

#### **(3.1)项目设计与技术栈**。

​		从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。**（5'）**

一.爬虫爬取官网信息并统计数据：

- scdn、b站学习基础python爬虫知识
- 参考学习博客https://blog.51cto.com/u_14137942/2736232解决卫建委反爬虫机制，使用pyppeteer爬取数据
- 使用 BeautifulSoup解析网页
- 使用re匹配具体需要的数据
- 把数据循环放入对于列表

二.数据导入Excel表中：

- 使用openpyxl把数据按行存入Excel表

三.数据可视化：

- 使用pandas把数据从Excel表读入
- 使用pyecharts实现静态数据大屏

四.性能测试：

- python profile

五.博客：

- 使用Typora写md博客文件

#### **(3.2)爬虫与数据处理**。

说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。**（20'）**

- 因为有反爬虫机制，通过资料搜索和尝试，最后爬虫主要用 pyppeteer 和 BeautifulSoup 库,处理统计数据用re库

```python
from pyppeteer import launch
from bs4 import BeautifulSoup
import os
import asyncio
import re
import openpyxl
#import time
```

- 分析卫建委网站的 URL 规则，发现除第一页之外，其他页码的 URL 都有显著的规律——URL 中的数字对应了当前的页码，也就是说，是通过 URL 来实现翻页的。所有的文章，都存放在一个 class 为 zxxx_list 的 ul 标签下，其中每一个 li 标签对应着一篇文章。然后分析文章详情界面，正文部分存放在一个 id 为 xw_box 的 div 标签下。
- 将 pyppeteer 的操作封装成 fetchUrl 函数，用于发起网络请求，获取网页源码。

```python
async def pyppteer_fetchUrl(url):
    browser = await launch({'headless': False,'dumpio':True, 'autoClose':True})
    page = await browser.newPage()

    await page.goto(url)
    await asyncio.wait([page.waitForNavigation()])
    str = await page.content()
    await browser.close()
    return str

def fetchUrl(url):
        try:
        return asyncio.get_event_loop().run_until_complete(pyppteer_fetchUrl(url))
    except:
        return asyncio.get_event_loop().run_until_complete(pyppteer_fetchUrl(url))

```

- 然后我们根据 URL 构成规则，通过 getPageUrl 函数构造每一页的 URL 链接。

```python
def getPageUrl():
    for page in range(1,12):
        if page == 1:
            yield 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml'
        else:
            url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_'+ str(page) +'.shtml'
            yield url
```

- 通过 getTitleUrl 函数，获取某一页的文章列表中的每一篇文章的标题，链接，和发布日期。

```python
def getTitleUrl(html):

    bsobj = BeautifulSoup(html,'html.parser')
    titleList = bsobj.find('div', attrs={"class":"list"}).ul.find_all("li")
    for item in titleList:
        link = "http://www.nhc.gov.cn" + item.a["href"];
        title = item.a["title"]
        date = item.span.text
        yield title, link, date
```

- 通过 getContent 函数，获取某一篇文章的正文中的数据。（如果没有获取到正文部分，则返回 “爬取失败”）。其中对于各省的数据，先匹配到括号内的各省数据，再把"例"字替换掉，然后提取文字和数字，就是当天有新增或无症状的省份和数据。

```python
#正则匹配需要的数据
findtime = re.compile("(.*?)0—24时")#日期
findnew = re.compile("新增确诊病例.*?本土病例(.*?)例（")#本土新增
findno = re.compile("新增无症状感染者.*?本土(.*?)例（")#本土无症状
findx = re.compile("香港特别行政区(.*?)例")#香港累计病例
finda = re.compile("澳门特别行政区(.*?)例")#澳门累计病例
findt = re.compile("台湾地区(.*?)例")#台湾累计病例
findprovince = re.compile("新增确诊病例.*?本土病例\d+例（(.*?)）")#各省新增
findprovince2 = re.compile("新增无症状感染者.*?本土\d+例（(.*?)）")#各省无症状
#通过 getContent 函数，获取某一篇文章的正文中的数据
def getContent(html):
    bsobj = BeautifulSoup(html, 'html.parser')
    #正文
    cnt = bsobj.find('div', attrs={"id": "xw_box"}).find_all("p")
    s = ""
    if cnt:
        for item in cnt:
            s += item.text
        time = re.findall(findtime, s)[0]
        new = re.findall(findnew, s)[0]
        no = re.findall(findno, s)[0]
        xiangGang = re.findall(findx, s)[0]
        aoMen = re.findall(finda, s)[0]
        taiWan = re.findall(findt, s)[0]
        province = re.findall(findprovince, s)[0]
        province2 = re.findall(findprovince2, s)[0]
        #各省数据分析统计
        province = province.replace('例', '')
        people = re.findall(r"\d+", province)
        place = re.findall('[\u4e00-\u9fa5]+', province)
        province2 = province2.replace('例', '')
        people2 = re.findall(r"\d+", province2)
        place2 = re.findall('[\u4e00-\u9fa5]+', province2)

        yield new, no, xiangGang, aoMen, taiWan, time, place, people, place2, people2

    return "爬取失败！"

```

- 通过pro_new()和pro_full()函数处理各省列表的数据统计处理。

```python
#把当天新增或者无症状的省份数据添加到对于省份
def pro_new(place,p_list,people):
    k = 0
    for i in place:
        for j in p_list:
            s = j[0]
            if i == s[:-3]:
                j.append(people[k])
                k = k + 1
                break

#当天没有新增或者无症状的省份列表添加'0'
def pro_full(p_list, num):
    for p in p_list:
        if len(p) < num:
            p.append('0')
```

- 通过create_excel()函数把各个数据列表按行导入excel，每个列表的相同下标为excel的同一行，循环就可以完成导入。

```python
#按行导入excel
def create_excel():
    # 创建工作簿
    workbook = openpyxl.Workbook()
    # 创建工作表
    mysheet = workbook.create_sheet("Sheet2")
    # 获取当前工作表（活跃工作表（当前编辑的工作表））
    worksheet = workbook.active
    # 写入数据
    for jj in range(0, len(timeList)):
        worksheet.append([...各个列表])

    for jj in range(0, len(timeList)):
        mysheet.append([...各个列表])
    # 保存数据
    workbook.save("personal.xlsx")
```

- 最后是主函数运用上述的函数完成爬虫与数据处理。



#### **(3.3)数据统计接口部分的性能改进**。

记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。**（6'）**

![](https://images.cnblogs.com/cnblogs_com/blogs/766546/galleries/2219532/o_220920071642_check.png)

#### **(3.4)每日热点的实现思路**。

简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。**（6'）**

- 每日热点的实现与词云可视化相结合，利用pandas的iloc函数提取excle表最前面的两行（为最新日期的两行），循环相减存入列表作为词云组件的数据之一，这样词云中字体最大的省份就是当天新增病例变化最大的省份。

```python
#热点词云
try:
    time15 = df.iloc[0:1, :]
    #print(time15)
    time14 = df.iloc[1:2, :]
    #print(time14)
    day_day = []

    for t in province_land:
        day_day.append(list(time15[t])[0]-list(time14[t])[0])
    print(day_day)
    day_hot_land = []
    for t in day_day:
        x = t
        if x < 0:
            x = 0
        day_hot_land.append(x)
    # print(day_newcreate_sym)
except:
    print()
```

```python
list(z) for z in zip(province2, day_hot_land)
```

- 这个方法属于比较简单的，可视化效果明显，不过热点信息比较单一，改进方案就是用不同的算法发现多种类的热点。

#### **(3.5)数据可视化界面的展示**。

在博客中介绍数据可视化界面的组件和设计的思路。**（15'）**

![](https://images.cnblogs.com/cnblogs_com/blogs/766546/galleries/2219532/o_220920072629_visual.jpg)

一.使用pandas把数据从Excel表读入

```python
import pandas as pd

# 设置列对齐
pd.set_option('display.unicode.ambiguous_as_wide', True)
pd.set_option('display.unicode.east_asian_width', True)

# 打开文件
df = pd.read_excel('personal.xlsx', sheet_name='Sheet')
dn = pd.read_excel('personal.xlsx', sheet_name='Sheet2')

# 对数据进行统计
data4 = dn['本土无症状']
data4_list = list(data4)
......
```

二.使用pyecharts实现静态数据大屏

- 通过学习和理解官方文档https://pyecharts.org/#/ ,使用里面的Map, Timeline，Bar, Line, Pie, WordCloud组件形成时间轴柱状图、时间轴折线图、词云和饼状图，通过阅读文档中全局配置项、系列配置项、和各图表具体的配置项修改图表样式。
- 再参考这篇文章https://www.bilibili.com/read/cv10798037/ 使用Page组件把各个图和背景图组合一起，重新布局渲染在一个html页面中。

三.总结流程图

![](https://images.cnblogs.com/cnblogs_com/blogs/766546/galleries/2219532/o_220920071458_%E4%B8%BB%E6%B5%81%E5%BC%80%E5%8F%91%E5%86%B3%E7%AD%96%E6%B5%81%E7%A8%8B%E8%A7%84%E8%8C%83(2).png)

# **三、心得体会**

**(4.1)**在这儿写下你完成本次作业的**心得体会**，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~**（10'）**

​		做为一个只看过一点点python语法，没有爬虫和python经验的小白，这次的作业的难度可以说五星级了，不过通过上网搜索和与同学朋友们讨论，也是勉勉强强算完成了。但代码也不是白打的，真的收获了很多！

- 代码能力upupup

​		首先就是python的语法熟悉了不少，也算学会了爬虫的基础，感觉基础的爬虫应该都可以完成了。完成作业的过程中，真的是bug百出，不过俗话说的好，人总要越挫越勇，所以感觉错误出着出着解决bug的能力不断提升了，有突破点去思考往哪找。代码的规范有待加强，被朋友批评了好几次好丑，已经在努力改进了，有利于以后的合作项目的交流沟通。

- 学会多看看文档

​		做可视化的时候，刚开始没有认真研究官方文档，做出了的图不满意想改样式时，上网搜相关帖子都跟官方文档的内容差不多，有些还不大齐全，后来读懂文档后，作图顺利了很多，还能帮助朋友找想要的样式。其实很多库和组件都有官方文档，但以前经常不习惯看文档，都是西拼东凑到处找，感觉还是看懂最根本的官方文档最高效。

- 完成一个程序的流程

​		之前没接触过PSP表格，发现预估和实现会不大一样，也发现完成一个程序，开始的计划和最后的测试也很重要，以前都是重心偏向于程序代码的编写，但没有测试和优化，开始前的规划也没有很详细，感觉以后随着技术的不断成熟，规划和测试在时间占比上会越来越大的

这礼拜打代码脖子好酸，继续加油努力，虽然作品不完美，也没有很好，但重在过程，相信下一次的作业会更好！！！