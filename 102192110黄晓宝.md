## 2022软工K班个人编程任务

Github链接：https://github.com/ErinLD/102192110.git

# 一、PSP表格

| PSP2.1                                 | Personal Software Process Stages       | 预计耗时（分钟） | 实际耗时（分钟） |
| -------------------------------------- | -------------------------------------- | ---------------- | ---------------- |
| Planning                               | 计划                                   | 30               | 30               |
| Estimate                               | 估计这个任务需要多少时间               | 30               | 10               |
| Development                            | 开发                                   | 20h 1200min      | 2100             |
| Analysis                               | 需求分析（包括学习新技术）             | 6h 360min        | 1800             |
| Design Spec                            | 生成设计文档                           | 10               | 180              |
| Design Review                          | 设计复审                               | 180              | 60               |
| Coding Standard                        | 代码规范（为目前的开发制定合适的规范） | 30               | 15               |
| Design                                 | 具体设计                               | 2h 120min        | 120              |
| Coding                                 | 具体编码                               | 240              | 600              |
| Code Review                            | 代码复审                               | 60               | 30               |
| Test                                   | 测试（自我测试，修改代码，提交修改）   | 180              | 60               |
| Reporting                              | 报告                                   | 120              | 180              |
| Test Repor                             | 测试报告                               | 120              | 30               |
| Size Measurement                       | 计算工作量                             | 10               | 20               |
| Postmortern & Process Improvement Plan | 事后总结，并提出过程改进计划           | 30               | 40               |
|                                        | 合计                                   | 2720             | 5275             |

实际时间计算都是大概的小时，具体的没法计算，有时间就在编写程序，查阅资料

# 二、任务要求的实现

### 2.1 项目设计与技术栈

- 技术栈：Python+ jupyter notebook + pyecharts

- 这次任务被拆分为了安装环境、学习网课课程、实践设计、编程四个环节，当然这是计划，实际上，学习查阅资料和上机两个过程分不开，都是一边查阅资料一边实现，技术有限没办法脱离资料直接上代码。

- 安装环境：通过CSDN知道如何安装下载配置python、Pycharm、Anaconda。

  学习网课大部分来自与CSDN还有b站，从B站上了解python抓取数据的基础语法包括重要的正则式。

  在编码上其实遇到了大大小小各种问题，主要求助于CSDN，这要求比较高的自主的学习能力以及理解能力才不会吃力。

  实践设计比较简单了，在Pycharm上直接注释，因为刚看完视频，给了一种爬数据很简单的错觉，只要获取网络链接，解析，正则匹配想要的数据，就据可以完成任务

  编程上，一开始自己按照B站爬取数据的方式爬取官网铩羽而归后，开始了一边CSDN一边编程的方式。

### 2.2 爬虫据处理

- 业务逻辑：

  - 1、发送请求,获取网页响应

  - 2、响应中获取数据

  - 3、匹配关键字，并提取加入到数据集中

  - 4、导出数据

  - 5、绘制图表可视化

- 赋予初值

  ```python
  # 赋予初值
  xzqz={'山东':0, '山西':0, '广东':0, '安徽':0, '甘肃':0, '宁夏':0, '贵州':0, '重庆':0, '广西':0, '黑龙江':0, '上海':0, '内蒙古':0, '青海':0, '福建':0, '湖南':0,  '新疆':0, '江苏':0, '河南':0, '云南':0, '江西':0, '陕西':0, '北京':0, '浙江':0, '吉林':0, '四川':0, '西藏':0, '湖北':0, '辽宁':0, '天津':0, '河北':0, '海南':0}
  xzwzz={'山东':0, '山西':0, '广东':0, '安徽':0, '甘肃':0, '宁夏':0, '贵州':0, '重庆':0, '广西':0, '黑龙江':0, '上海':0, '内蒙古':0, '青海':0, '福建':0, '湖南':0, '新疆':0, '江苏':0, '河南':0, '云南':0, '江西':0, '陕西':0, '北京':0, '浙江':0, '吉林':0, '四川':0, '西藏':0, '湖北':0, '辽宁':0, '天津':0, '河北':0, '海南':0}
  gat={"香港特别行政区":0,"澳门特别行政区":0,"台湾地区":0}
  
  ```

- 在查阅资料发现，对爬取数据除了伪装头部，还有使用selenium和Chrome访问网站的方式，在selenium爬取网页时，有时候需要登入，用selenium获取cookie和携带cookie更加方便

  ```python
  def get_cookie():
      url='http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml'
      driver = webdriver.Chrome()
      driver.get(url)
      time.sleep(10)
      #获取cookie的列表
      cookies = driver.get_cookies()
      driver.quit()
      items = []
      #格式化打印cookie
      for i in range(len(cookies)):
       cookie_value = cookies[i]
       item = cookie_value['name'] + '=' + cookie_value['value']
       items.append(item)
      cookiestr = '; '.join(a for a in items)
      return cookiestr
  ```

- 获取官网的最新发布疫情信息的地址

  ```python
  def get_url():
      url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml'
      r = requests.get(url, headers=headers)
      soup = BeautifulSoup(r.text, 'lxml')
      today_format = datetime.datetime.today().strftime('%Y-%m-%d')
      #latest_news_title = soup.find(name='span', text=today_format).find_previous_sibling(name='a').attrs['title']
      latest_news_href = 'http://www.nhc.gov.cn' + soup.find(name='span', text=today_format).find_previous_sibling(name='a').attrs['href']
      return latest_news_href
  
  ```

- 获取到最新的访问网址，用request请求访问内容以及解析并用get_text方法获取段标签为P的文本内容

  ```
  # 解析网站，获取P标签的内容
  url=get_url()
  r = requests.get(url,headers=headers)
  r.encoding=("utf-8")
  soup = BeautifulSoup(r.text, 'lxml')
  ls=[]
  for li in soup.select('p'):
      ls.append(li.get_text())#获取内容
  
  ```

- 匹配港澳台，新增确诊，新增无症状的数据

  ```python
  s=str(re.findall("香港特别行政区.*?。",ls[6]))
  for i in gat.keys():
      if i in s:
          gat[i]=int(re.findall(f"{i}(\d*)例",s)[0])
  
  print("港澳台",gat)
  s=str(re.findall("本土病例.*?）",ls[0]))
  for i in xzqz.keys():
      if i in s:
          xzqz[i]=int(re.findall(f"{i}(\d*)例",s)[0])
  
  print("新增确诊",xzqz)
  
  s=str(re.findall("本土.*?。",ls[4]))
  for i in xzwzz.keys():
      if i in s:
          xzwzz[i]=int(re.findall(f"{i}(\d*)例",s)[0])
  
  print("新增无症状",xzwzz)
  ```

- 导出数据

  ```python
  df = pd.DataFrame(xzqz)
  df.to_csv('新增疫情数据.csv')
  with ExcelWriter('新增疫情数据.xlsx') as ew:
      pd.read_csv("新增疫情数据.csv").to_excel(ew, sheet_name="新增疫情数据", index=False)
  
  df = pd.DataFrame(xzwzz)
  df.replace('台湾', '中国台湾', inplace=True)  # 更改台湾名称
  df.to_csv('新增无症状疫情数据.csv')
  with ExcelWriter('新增无症状疫情数据.xlsx') as ew:
      pd.read_csv("新增无症状疫情数据.csv").to_excel(ew, sheet_name="新增无症状疫情数据", index=False)
  
  df = pd.DataFrame(gat)
  df.replace('台湾', '中国台湾', inplace=True)  # 更改台湾名称
  df.to_csv('港澳台疫情数据.csv')
  with ExcelWriter('港澳台疫情数据.xlsx') as ew:
      pd.read_csv("港澳台疫情数据.csv").to_excel(ew, sheet_name="港澳台疫情数据", index=False)
  ```



### 2.3接口部分的性能改进

![请添加图片描述](https://img-blog.csdnimg.cn/ab46b42a23ac427392c539b374f48d97.png)

![请添加图片描述](https://img-blog.csdnimg.cn/cc25d8e0f7ed4e7fb47fc7a0b8fcf668.png)


### 2.4每日热点的实现思路

- 想法一：主要想体现疫情的峰值点的事件，在数值中，人们总会关注最高值点，所以通过近一年的增长情况，每日的增长趋势，来找出出峰值背后的疫情反弹事件

  ```python
  background_color_js = ("new echarts.graphic.LinearGradient(0, 0, 0,1, "
                         "[{offset: 0, color: '#99cccc'}, {offset: 1, color: '#00bfff'}], false)")
  
  line1 = Line(init_opts=opts.InitOpts(theme=ThemeType.ROMA,bg_color=JsCode(background_color_js))) #设置主题&背景颜色
  
  line1.add_xaxis(list(chinanewadd["日期"].astype('str')))  #添加x轴
  
  line1.add_yaxis(series_name = "新增确诊",
                  y_axis = list(chinanewadd["新增确诊"]), #增加Y轴数据
                  is_smooth=True,#添加Y轴，平滑曲线
                  areastyle_opts=opts.AreaStyleOpts(opacity=0.3), #区域阴影透明度
                  is_symbol_show = True,
                  label_opts=opts.LabelOpts(is_show=False),
                  yaxis_index = 0 #指定y轴顺序
                 ) #不显示标签
  
  line1.add_yaxis(series_name = "新增本土",
                  y_axis = list(chinanewadd["本土新增确诊"]),
                  is_smooth=True,
                  areastyle_opts=opts.AreaStyleOpts(opacity=0.3),
                  is_symbol_show = True,#是否显示标记
  #                 symbol = 'circle' #标记类型 'circle', 'rect', 'roundRect', 'triangle', 'diamond', 'pin', 'arrow', 'none'
                  label_opts=opts.LabelOpts(is_show=False),
                  yaxis_index = 1
                 )
  #增加副轴
  line1.extend_axis(yaxis=opts.AxisOpts(
                          name="新增本土(人)",
                          name_location="end", #轴标题位置
                          type_="value",#轴类型
                          is_inverse=False, #逆序刻度值
                          axistick_opts=opts.AxisTickOpts(is_show=True),
                          splitline_opts=opts.SplitLineOpts(is_show=True)
                          )
                  )
  #设置图表格式
  line1.set_global_opts(title_opts=opts.TitleOpts(title="国内每日新增趋势", #添加主标题
                                                  subtitle="（含港澳台）", #添加副标题
                                                  subtitle_textstyle_opts = opts.TextStyleOpts(color='#000000'),
                                                  pos_left="center", #标题位置
                                                  pos_top="top"),
                      legend_opts=opts.LegendOpts(pos_left="40%",
                                                  pos_top='10%'), #图例位置-左侧
                      xaxis_opts=opts.AxisOpts(type_="category",
                                               axislabel_opts=opts.AxisTickOpts()),
                      yaxis_opts=opts.AxisOpts(name="新增确诊（人）", 
                                               type_="value", 
  #                                              max_=100000
                                              ),
                      datazoom_opts=opts.DataZoomOpts(type_= 'slider',
                                                      range_start=80 ,#横轴开始百分百
                                                      range_end=100) , #横轴结束百分比
                      toolbox_opts=opts.ToolboxOpts(is_show=True,  #显示工具窗口
                                                    orient='vertical', #垂直排列工具窗口
                                                    pos_left='95%',
                                                    pos_top='middle')
                       )
  
  line1.render_notebook()
  
  ```

  

- 想法二：在查阅资料中，发现有基于kneen的拐点检测，可以对疫情拐点判断，找出疫情明显变化的点，用于从`KneeLocator`中`curve`和`direction`参数的全部组合中，搜索合法的拐点输出值及对应拐点的趋势变化类型，若无则返回None。分别在`curve='concave'`+`direction='increasing'`、`curve='concave'`+`direction='decreasing'`、`curve='convex'`+`direction='increasing'`和`curve='convex'`+`direction='decreasing'`的参数组合基于余弦曲线的拐点模型计算，

  ```python
  def knee_point_search(x, y):
      
      # 转为list以支持负号索引
      x, y = x.tolist(), y.tolist()
      output_knees = []
      for curve in ['convex', 'concave']:
          for direction in ['increasing', 'decreasing']:
              model = KneeLocator(x=x, y=y, curve=curve, direction=direction, online=False)
              if model.knee != x[0] and model.knee != x[-1]:
                  output_knees.append((model.knee, model.knee_y, curve, direction))
      
      if output_knees.__len__() != 0:
          print('发现拐点！')
          return output_knees
      else:
          print('未发现拐点！')
  ```

  4月上海疫情复弹
![请添加图片描述](https://img-blog.csdnimg.cn/7c2ceea69a544da8bb8b95fdfdcdf0b2.png)


  海南疫情反弹
![请添加图片描述](https://img-blog.csdnimg.cn/4f1bdf4899da4389b1039d695c562bde.png)




### 2.5数据界面可视化展示

- 使用matplotlib实现数据的可视化，在查阅资料中，有许多种方式实现可视化。Matplotlib 是一个在 python 下实现的类 matlab 的纯 python 的第三方库，用 python实现matlab 的功能,风格跟 matlab 相似，也继承了 python 的简单明了。Matplotlib模块依赖于NumPy和tkinter模块，可以绘制多种形式的图形，包括线图、直方图、饼图、散点图等，是数据可视化的重要工具。Matplotlib中应用最广的是matplotlib.pyplot模块。Pyplot提供了一套和Matlab类似的绘图API，我们只需要调用Pyplot模块所提供的函数就可以实现快速绘图并设置图表的各个细节。

- 使用matplotlib必须导入import matplotlib.pyplot as plt（在jupyter notebook需要执行% matplotlib notebook），主要使用7个方法，显示图像，显示图例，数值标签，轴标签，轴刻度范围，轴刻度标签，标题。还是很简单的实现图形的绘制，主要先设置画布的大小就是图表的大小，然后绘图，将数据绘制为图形，然后设置XY轴的标签，最后根据需要添加标签和时间

  ```python
  # 可视化
  time=datetime.datetime.today().strftime(('%Y-%m-%d'))
  
  # 折线图
  plt.rcParams['font.sans-serif'] = ['SimHei']
  plt.figure(figsize=(14,5))# 设置画布
  plt.plot(xzqz.keys(),xzqz.values(),mec='black',linewidth=2,marker='o',c="blue")# 新增确诊绘图
  plt.plot(xzwzz.keys(),xzwzz.values(),mec='black',linewidth=2,marker='o',c="black")# 新增无症状绘图
  plt.xticks(rotation=15)
  plt.ylabel('感染人数',fontsize=15,color="teal")# 设置Y轴标签
  plt.title(f'新冠疫情{time}日各省份新增确诊人数',fontsize=15,color="black")# 设置标题
  plt.legend(["新增确诊","新增无症状"])# 为当前axes放置标签
  plt.show()	
  ```
  
  ```python
  china_map = (
      Map()
       .add("新增本土确诊", [list(i) for i in zip(df['省份'].values.tolist(),df['新增本土确诊'].values.tolist())], "china")
       .add("新增无症状", [list(i) for i in zip(df['省份'].values.tolist(),df['新增无症状'].values.tolist())], "china")
      .set_global_opts(
           title_opts=opts.TitleOpts(title="各地区确诊人数"),
           visualmap_opts=opts.VisualMapOpts(max_=600, is_piecewise=True),
          
          
       )
  )
  china_map.render_notebook()
  ```
  
  ```python
  pie = (
       Pie()
       .add(
           "",
           [list(i) for i in zip(df2['省份'].values.tolist(),df2['新增本土确诊'].values.tolist())],
           radius = ["10%","30%"]
       )
       .set_global_opts(
               legend_opts=opts.LegendOpts(orient="vertical", pos_top="70%", pos_left="70%"),
       )
       .set_series_opts(label_opts=opts.LabelOpts(formatter="{b}: {c}"))
   )
  pie.render_notebook()
  ```

-   效果图：

  ![请添加图片描述](https://img-blog.csdnimg.cn/af6fa5d586c24fbeb5a437b2a4b69217.png)

  

  jupyter工具可视化

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/9c4341796e114b5a9b8793e787b974c1.png)

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/0043a57311f84d2c9225b2142272ec46.png)



# 三、心得体会

        在安装软件和配置环境的环节就问题百态，无法使用jupyter notebook的问题，通过csdn，知道默认安装目录里有中文，所以无法使用，搜查了一圈，改变path路径使用降低版本可以正常运行jupyter，当然这个问题一般会伴有jupyter不能自动打开的问题，所以解决上一个问题最好要在解决自动打开网页的基础上实现
    
    	对我来说实验的难点是爬取网站的信息的提取，需要自己匹配提取，由于对python的没有基础，就陷入了误区，一开始就以最低标准来做这次实践，能先实现最低的要求。一开始尝试对官网爬取数据，因为有反爬机制，在首次爬不下来就开始放弃从官网爬取数据。在网上查阅资料，大多数是对统计好的数据爬取，所以在一开始的编程学习中还是比较顺利，完成基础要求，在其中先是对python爬取数据的基础方式，爬取网页，解析内容有个大概的熟悉。
    
    	在爬取网页处理好的内容后，发现可以爬官网的数据，再对官网爬取内容，就容易的多，但是还是经常被拒绝，爬不到数据。难点来了，对文本的提取，需要对正则式比较强的理解，在这个上花费了大量的时间。
    
    	数据可视化在网上有很多方式可以实现，主要还是查阅资料查看了一些可视化工具的使用方法，对其方法的调用，这方面还是比较简单，毕竟只要完成最难的数据提取，可视化还是比较快可以完成的
    
       在热点挖掘上时间花的没有前几项之多，毕竟有deadline，其次在一开始先实现了简单要求任务，再实现困难要求的任务，这是很浪费时间的，没有正确的规划，和在过程中没有思考是否某些方法适合爬取官网。也对整体框架把控不够到位，首次的python任务，重心放在了学习基础知识点上，去查阅资料也是比较零散的查阅，不能很好的对整体任务的时间分配，这是下一次需要提高的地方。
    
    	在使用工具，如githyb上，还是花了不少时间，有网速慢的原因，还有首次使用，对github不熟悉，摸索花了不少时间。还有分析工具，百度了一下，发现pycharm有自带的分析工具，勉勉强强使用。
    
    	想了想这次花很多时间在学习上，先是学习怎么爬取数据，爬取了腾讯的数据，然后也试着爬取官网，时间花费很多在各种尝试上，不过也有了收获。


​		
