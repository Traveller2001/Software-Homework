

# 一、PSP表格

## (2.1)在开始实现程序之前，在附录提供的**PSP**表格记录下你估计将在程序的各个模块的开发上耗费的时间。（3'）

|               **PSP2.1**                |  **Personal Software** Process Stages   | **预估耗时**  **（分钟）** | **实际耗时**（分钟） |
| :-------------------------------------: | :-------------------------------------: | :------------------------: | :------------------: |
|              **Planning**               |                **计划**                 |           **30**           |                      |
|               · Estimate                |       · 估计这个任务需要多少时间        |             30             |                      |
|             **Development**             |                **开发**                 |          **2005**          |                      |
|               · Analysis                |       · 需求分析 (包括学习新技术)       |            720             |                      |
|              · Design Spec              |             · 生成设计文档              |             30             |                      |
|             · Design Review             |               · 设计复审                |             20             |                      |
|            · Coding Standard            | · 代码规范 (为目前的开发制定合适的规范) |             15             |                      |
|                · Design                 |               · 具体设计                |            120             |                      |
|                · Coding                 |               · 具体编码                |            560             |                      |
|              · Code Review              |               · 代码复审                |             60             |                      |
|                 · Test                  | · 测试（自我测试，修改代码，提交修改）  |            480             |                      |
|              **Reporting**              |                **报告**                 |          **200**           |                      |
|              · Test Report              |               · 测试报告                |            120             |                      |
|           · Size Measurement            |              · 计算工作量               |             30             |                      |
| · Postmortem & Process Improvement Plan |     · 事后总结, 并提出过程改进计划      |             50             |                      |
|                                         |                 · 合计                  |          **2235**          |                      |



## (2.2)在你实现完程序之后，在附录提供的**PSP**表格记录下你在程序的各个模块上实际花费的时间。（3'）

|               **PSP2.1**                |  **Personal Software** Process Stages   | **预估耗时**  **（分钟）** | **实际耗时**（分钟） |
| :-------------------------------------: | :-------------------------------------: | :------------------------: | :------------------: |
|              **Planning**               |                **计划**                 |           **30**           |        **22**        |
|               · Estimate                |       · 估计这个任务需要多少时间        |             30             |          22          |
|             **Development**             |                **开发**                 |          **2005**          |       **2224**       |
|               · Analysis                |       · 需求分析 (包括学习新技术)       |            720             |         783          |
|              · Design Spec              |             · 生成设计文档              |             30             |          42          |
|             · Design Review             |               · 设计复审                |             20             |          20          |
|            · Coding Standard            | · 代码规范 (为目前的开发制定合适的规范) |             15             |          11          |
|                · Design                 |               · 具体设计                |            120             |          88          |
|                · Coding                 |               · 具体编码                |            560             |         729          |
|              · Code Review              |               · 代码复审                |             60             |          30          |
|                 · Test                  | · 测试（自我测试，修改代码，提交修改）  |            480             |         521          |
|              **Reporting**              |                **报告**                 |          **200**           |       **224**        |
|              · Test Report              |               · 测试报告                |            120             |         153          |
|           · Size Measurement            |              · 计算工作量               |             30             |          19          |
| · Postmortem & Process Improvement Plan |     · 事后总结, 并提出过程改进计划      |             50             |          72          |
|                                         |                 · 合计                  |          **2235**          |       **2470**       |



# 二、任务要求的实现

## (3.1)**项目设计与技术栈**。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。（5'）

### 项目设计

|               步骤划分                |             完成方式             |       完成渠道       |
| :-----------------------------------: | :------------------------------: | :------------------: |
| 网页分析（网页数据入口、获取方式etc） |             抓包工具             |          —           |
|             选择爬虫技术              |         pyppeteer+xpath          | github+bilibili+csdn |
|               数据提取                |            正则表达式            |       re、bs4        |
|               数据解析                |                —                 |  re、json、jsonpath  |
|               数据存储                |          python文件操作          |          —           |
|            数据可视化分析             |           python+excel           |         csdn         |
|           每日热点分析预测            | 使用线性回归预测明日新增确诊人数 | numpy、scikit-learn  |

### 技术栈

- python3：**csv**、datetime、**json**、logging、os、**re**、time、traceback、**urllib**、**bs4**、**playwright**、jieba、**numpy**、**scikit-learn**

- 网络协议：http&https协议、TCP
- excel：数据透视表

## (3.2)**爬虫与数据处理**。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。（20'）

#### 1.爬虫

卫健委网站的反爬机制很强，通过常规的抓包方式非常困难，所以采用**<u>playwright</u>**框架进行数据爬取。Playwright 是微软在 2020 年初开源的新一代自动化测试工具，它的功能类似于 Selenium、Pyppeteer 等，都可以驱动浏览器进行各种自动化操作。 Playwright 是一个类似 Selenium 一样可以支持网页页面渲染的工具，再加上其强大又简洁的 API，Playwright 同时也可以作为网络爬虫的一个爬取利器。

##### （1）获取每天的疫情通报链接  

`get_notice_urls(headless=False)`函数从疫情通报首页http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml开始，获取所有的疫情通报链接，为了减少对网站的压力，将爬取到的疫情通报链接结果保存到urls.json文件中去。

url分析：根据通过对网页目录的url进行观察分析，发现除了第一页以外，每一页的url都是有规律的以“_n”结尾；

可以使用字符串拼接的方式生成每一页的url。

- 获取下一页地址

  ```python
      def get_next_page_url(_node: bs4.element.Tag):
          """获取下一页地址"""
          for tag_a in _node.find("div", id="page_div").find_all("a"):
              if tag_a.get_text().strip() == "下一页":
                  return "http://www.nhc.gov.cn/xcs/yqtb/" + tag_a.attrs["href"]
          return None
  ```

- url采集（获取所有的疫情通报链接）

  ```python
      with sync_playwright() as playwright:
          browser = playwright.firefox.launch(headless=headless)
          page = browser.new_page()
          while start_url:
              page.goto(start_url)
              page.wait_for_load_state("networkidle")
              html = page.content()
              soup = BeautifulSoup(html, "html.parser")
              for node in soup.find("ul", class_="zxxx_list").find_all("li"):
                  url = urljoin(start_url, node.a.attrs["href"])
                  title = node.a.string
                  logger.info("采集: {}".format(title))
                  if url in history_ls:
                      logger.info("该url已采集, 中止...")
                      start_url = None
                      break
                  url_ls.append(url)
              else:
                  start_url = get_next_page_url(soup)
                  time.sleep(1)
          browser.close()
      url_ls.extend(history_ls)
      logger.info("共采集 {} 条url".format(len(url_ls)))
  
  ```

- url保存

  ```python
  with open(history_file_path, "w", encoding="utf-8") as fp:
          json.dump(url_ls, fp, ensure_ascii=False, indent=4)
  ```

##### （2）数据爬取与数据解析

`get_notice_data(headless=False)`函数读取urls.json文件中的疫情通报链接，逐个<u>爬取疫情通报</u>，使用**beautifulsoup**库**解析**出其中的<u>正文</u>和<u>发布时间</u>，保存数据到notices.json文件中去。

- 读取疫情通报链接

  ```python
   url_ls = get_notice_urls(headless=headless)
      history_file_path = "file/notices.json"
      if os.path.exists(history_file_path):
          with open(history_file_path, encoding="utf-8") as fp:
              history_dict = json.load(fp)
      else:
          history_dict = {}
  ```

- 爬取疫情通报

  ```python
   with sync_playwright() as playwright:
          browser = playwright.webkit.launch(headless=headless)
          page = browser.new_page()
          for url in url_ls:
              if url in history_dict.keys():
                  continue
              page.goto(url)
              page.wait_for_load_state("networkidle")
              html = page.content()
              soup = BeautifulSoup(html, "html.parser")
              main_node = soup.find(name="div", class_="list")
              if main_node is None:
                  logger.info("采集失败: {}".format(url))
                  continue
  ```

- 解析正文和发布时间

  ```python
  publish_date = main_node.find(name="div", class_="fr").next_sibling.next_sibling.get_text().split()[-1]
              title = main_node.find(name="div", class_="tit").get_text().strip()
              content = main_node.find(name="div", class_="con").get_text().strip()
              history_dict[url] = {
                  "publish_date": publish_date,
                  "title": title,
                  "content": content,
              }
              with open(history_file_path, "w", encoding="utf-8") as fp:
                  json.dump(history_dict, fp, ensure_ascii=False, indent=4)
  ```

##### （3）数据提取

`parse_data()`函数根据疫情通报中确诊病例和无症状感染者的格式使用**正则表达式**提取本土和各城市的人数

- 本土确诊病例

  ```python
  match = re.search(r"31个省.*?确诊病例.*?本土病例(\d+)例（(.*?)）[，。；]", content)
              if match:
                  local_confirmed_total = int(match.group(1))
                  local_confirmed_str = match.group(2)
              else:
                  local_confirmed_total = 0
                  local_confirmed_str = ""
              local_confirmed_city_and_num = parse_by_city(local_confirmed_total, local_confirmed_str)
              logger.info("新增确诊病例: 本土={}, 各城市={}".format(
                  local_confirmed_total,
                  dict(filter(lambda item: item[1] > 0, local_confirmed_city_and_num.items())),
              ))
  ```

- 本土无症状感染者

  ```python
  match = re.search(r"31个省.*?无症状感染者.*?本土(\d+)例（(.*?)）[，。；]", content)
              if match:
                  local_asymptomatic_total = int(match.group(1))
                  local_asymptomatic_str = match.group(2)
              else:
                  local_asymptomatic_total = 0
                  local_asymptomatic_str = ""
              local_asymptomatic_city_and_num = parse_by_city(local_asymptomatic_total, local_asymptomatic_str)
              logger.info("新增无症状感染者: 本土={}, 各城市={}".format(
                  local_asymptomatic_total,
                  dict(filter(lambda item: item[1] > 0, local_asymptomatic_city_and_num.items())),
              ))
  ```

##### （4）数据可视化效果展示

- 2022年各省份本土确诊病例（除港澳台）折线图

[![2022.png](https://i.postimg.cc/kXgrM0BJ/2022.png)](https://postimg.cc/bdWV3FxW)

- 2022年各省份本土确诊病例（含港澳台）折线图

  [![2022.png](https://i.postimg.cc/qqC32b7z/2022.png)](https://postimg.cc/q64qTGyT)

- 2022年各省份新增无症状感染者（除港澳台）折线图

[![2022.png](https://i.postimg.cc/tRPdZJnD/2022.png)](https://postimg.cc/dDsZgqBT)

- 2022年各省份新增无症状感染者（含港澳台）折线图

  [![2022.png](https://i.postimg.cc/vBvQk5S3/2022.png)](https://postimg.cc/VrdQb0wC)

  

## (3.3)**数据统计接口部分的性能改进**。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（6'）

- 在开始的时候采用的是普通的爬虫方法，逐条爬取，用时三个小时

  动态cookie，用时两个半小时左右

- 使用playwright框架后，用时大大缩短，花费时间如下图

  [![call-graph.png](https://i.postimg.cc/PqhphN2x/call-graph.png)](https://postimg.cc/xXxT3jrD)

  [![image.png](https://i.postimg.cc/k5XSKB8H/image.png)](https://postimg.cc/yDtWz6Lh)

## (3.4)**每日热点的实现思路**。简要介绍实现该功能的算法原/理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（6'）

### 功能一：统计今天的新增确诊病例和无症状感染者人数+每天新增前十的省份

```python
local_confirmed_ls, local_asymptomatic_ls = parse_data()
    today_c: dict = local_confirmed_ls[0].copy()
    today_date = today_c.pop("时间")
    today_total = today_c.pop("本土")
    today_a: dict = local_asymptomatic_ls[0].copy()
    today_date2 = today_a.pop("时间")
    today_total2 = today_a.pop("本土")
    content = "今天是{}。\n本土新增确诊病例{}例，{}个省（市）有新增确诊病例，当日新增top 5为：{}\n".format(
        today_date.strftime("%Y年%m月%d日"),
        today_total,
        len(list(filter(lambda x: x[1] > 0, today_c.items()))),
        sorted(today_c.items(), key=lambda x: x[1], reverse=True)[:5],
    )
    content += "本土新增无症状感染者{}例，{}个省（市）有新增无症状感染者，当日新增top 5为：{}\n".format(
        today_total2,
        len(list(filter(lambda x: x[1] > 0, today_a.items()))),
        sorted(today_a.items(), key=lambda x: x[1], reverse=True)[:5],
    )
```

### 功能二：使用线性回归模型进行模型训练和预测

- 算法流程和公式推导

  1. 假设目标值（因变量）与特征值（自变量）之间线性相关（即满足一个多元一次方程，f(x)=w1x1+…+wnxn+b）；

  2. 构建损失函数；

  3. 最后通过令损失函数最小来确定参数（最关键的一步）

     损失函数J(a,b)是凸函数，那么分别关于a和b对J(a,b)求偏导，并令其为零解出a和b。
     ![在这里插入图片描述](https://img-blog.csdnimg.cn/4b62083c1c3741edbaf120578e69694e.png#pic_center)
     ![在这里插入图片描述](https://img-blog.csdnimg.cn/10be7913a84849db8ded3c117ec88e3e.png#pic_center)
     解得：
     ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201031231002725.png#pic_center)

     ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201031230931899.png#pic_center)

- 代码实现

```python
import numpy as np
    from sklearn.linear_model import LinearRegression

    x = np.array([[i, ] for i in range(14)])
    y = np.array([i["本土"] for i in local_confirmed_ls[:14][::-1]])
    # x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
    model = LinearRegression()
    model.fit(x, y)
    logger.info("模型训练中...")
    logger.info("系数: {}".format(model.coef_))
    logger.info("截距: {}".format(model.intercept_))
    y_predict = model.predict(np.array([[15, ], [16, ], [17, ]]))
    logger.info(y_predict)
    content += "未来三天预测新冠新增确诊人数为: {}".format("、".join(list(map(lambda _: str(int(round(_, 0))), y_predict))))
```

##### 算法优点

- 高效获取数据集中的线性关系；
- 训练速度和预测速度较快；
- 在小数据集上表现很好。

##### 算法缺点

- 对于非线性数据或者数据特征间具有相关性多项式回归难以建模；

- 新冠病例产生的因素有很多，除了已有的基数之外，还要考虑人口流动、防疫措施的执行程度等等，线性回归模型的预测只考虑到了最近一段时间确诊的病例。

##### 可能的改进方向

- 结合马尔可夫预测、神经网络预测



# 三、心得体会

## (4.1)在这儿写下你完成本次作业的心得体会，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~（10'）

​	对于我来说，本次作业的难度是大的，虽然在此之前有接触过爬虫项目，但这次任务网站的反爬机制太强了！！！普通的爬虫代码模板无法直接套用，普通的反反爬手段（UA伪装、添加refer字段等）也无法解决。在爬虫阶段遇到以下问题：

1. 问题：js实现页面跳转，无法在源码中获取下一页url；

   解决方法：多次抓包获取条状url，分析规律

2. 问题：正常浏览器请求网站，速度不会太快，同一个ip/账号在单位时间内大量请求了卫健委官网，服务器拒绝相应

   解决方法：动态cookie，同时设置请求间随机休眠

​	解决了反爬问题……数据提取又不会了……~~（我是废物）~~在参考csdn诸多篇博客之后，使用re模块实现……在仔细查看清洗后的数据之后，发现卫健委文章中，同样意思有好多种不同的表达啊……

​	这次作业虽然痛苦~但是收获还是特别多的！复习了python库函数的同时，对爬虫技术有了更深的理解和认识，也学到了数据分析、可视化的新知识，但由于时间来不及+前端知识有限，数据可视化大屏hasn‘t been finished yet。也体会到，提取检索关键词的能力有多重要，很多时候在搜索引擎中查找资料，找不到高质量的搜索结果，理解了问题本质后，重新定义问题的关键词（功能、库、函数等等），获取到博客和参考代码就更加贴近自己的需求了。

​	这次作业的完成，让我意识到自己的专业知识和编程能力和优秀的同学比起来还有很大的差距，需要花费更多的时间来复习和完善python技能树。
