https://github.com/workhardhhh/Software-Homework/branches
# 一、PSP表格
## 2.1在开始程序实现之前，在附录提供的PSP表格记录下你估计将在程序的各个模块的开发上耗费的时间
PSP2.1| Personal Software Process Stages|    预估耗时（分钟） | 实际耗时（分钟） |
-------- | ----- | ----- | ----- |
Planning| 计划 | 1600| 
Estimate| 估计这个任务需要多少时间 |1600 | 
Development|开发| 400| 
 Analysis| 需求分析| 200 | 
Design Spec| 生成设计文档 | 100 |
Design Review| 设计复审 | 100 |
 Coding Standard| 代码规范| 100 | 
 Design| 具体设计 |200 |
 Coding|具体编码 | 200|
Code Review|代码编码 | 100|
  Test| 测试（自我测试，修改代码，提交修改）| 120 | 
 Reporting| 报告 | 120|
 Test Repor| 测试报告| 60|
  Size Measurement| 计算工作量| 30|
   Postmortem & Process Improvement Plan | 事后总结并提出过程改进计划| 30|
| |  合计 |1660

## 2.2在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间
PSP2.1| Personal Software Process Stages|    预估耗时（分钟） | 实际耗时（分钟） |
-------- | ----- | ----- | ----- |
Planning| 计划 | 1600| 1910
Estimate| 估计这个任务需要多少时间 |1600 | 1910
Development|开发| 400| 600
 Analysis| 需求分析| 200 | 120
Design Spec| 生成设计文档 | 100 |120
Design Review| 设计复审 | 100 |60
 Coding Standard| 代码规范| 100 | 120
 Design| 具体设计 |200 |250
 Coding|具体编码 | 200|150
Code Review|代码编码 | 100|100
  Test| 测试（自我测试，修改代码，提交修改）| 120 | 120
 Reporting| 报告 | 120|120
 Test Repor| 测试报告| 60|60
  Size Measurement| 计算工作量| 30|30
   Postmortem & Process Improvement Plan | 事后总结并提出过程改进计划| 30|60
| |  合计 |1660|1910
# 二、任务要求的实现
## 3.1项目设计与技术栈
- 项目设计流程主要是需求分析-模块划分-用例设计-具体编码-项目测试
- 项目主要分为以下几个模块：爬虫、页面数据处理、分类器、数据可视化以及一个Web界面集成以上功能，主要思路为爬虫模块负责爬取指定网页信息交给页面数据处理模块处理提取出想要的信息并写入当前目录的excel文件，数据可视化模块读取该文件实现数据统计并展示到前端。Web页面包括爬虫、每日热点、数据可视化三个功能，利用按钮监听事件来选择功能，每日热点通过爬取卫健委官网的新闻标题及其链接实现，获取文章标题交给分类器判断分类，是则展示到网页，数据可视化则是通过用户具体输入查询到某月份的疫情数据统计，页面展示查询月份的疫情情况
- 在各个环节中遇到难题都是通过网上查询解决，需求分析跟模块划分关系比较紧密，主要划分要实现的功能和类，用例实际则是通过亿图图示绘制生成用例图，确定软件的总体结构，具体编码采用的是python语言开发软件采用pyCharm，单元测试也是用pyCharm实现。主要涉及的技术栈是python、多线程、机器学习、爬虫等


-----
## 3.2爬虫与数据处理
- 爬虫模块
--爬虫模块主要负责数据的爬取，最终得到的是某个页面文章的具体内容，通过网上查询，原采用pyppeyeer爬虫框架爬取效率还可以但重复开启谷歌浏览器不方便，且存在莫名的内存泄漏问题，后更改采用了urlib3库的爬虫框架增加了数据爬取的效率，文章内容的数据抽取则是采用BeatifulSoup实现，抽取html文件中指定结点内的内容，后将抽取页面的文章内容交给数据处理模块解决
--爬虫模块主要函数有getpagecon卫健委疫情通报页面爬取一页，urllib_geturl爬取指定url页面返回html文件，getUrl获取卫健委疫情通报页面所有文章的链接，geturlContent获取卫健委疫情通报页面链接的内容，get_onepage_hot爬取一个页面主要用于卫健委官网新闻标题链接的爬取用于每日热点功能，getTitleUrl获取新闻标题的链接用于每日热点功能。
--主要的爬取过程在run函数上可一次性爬取卫健委疫情通报页面中的所有文章即从第一页到最后一页皆爬取，run函数主要先调用urllib_geturl爬取页面然后调用geturlContent判断页面内容是否有效若无效则该函数返回fail重新尝试爬取，有效爬取后将urllib_geturl页面作为变量传递给getpagecon函数，getpagecon调用getUrl获取当前页面所有链接然后先后调用urllib_geturl、geturlContent获取内容交给数据处理对象提取数据写入表格。一页爬完后则根据疫情通报界面的url更换规律更换页面直到爬完为止
-  数据处理模块
数据处理模块获取为爬虫某个疫情文章的内容，利用正则表达式提取想要的内容。将文章内容剪切成几段，拆成几个正则表达式进行匹配提取，并通过网上查询利用openpyxl包完成对excel文件的写入
数据处理模块主要有handle_content主函数输入文章内容并填入要写入文件的指定列即可提取31个省份的每日新增确诊和新增无症状感染者以及港澳台地区的累计确诊的数据，该函数调用re包的split将文章分段，利用compile正则表达式提取内容，后调用write函数写入excel表格，write函数实现写入标题，传入主要参数为handle_content传入的三个情况：（大陆新增确诊、大陆新增无症状、港澳台累计确诊）所有省份的list对list调用get_pro_info函数获取省份信息和确诊人数，由于不是每个省份都会有新增，故设置全局变量字典包含所有省份，局部变量中也创建字典，有数据的加入局部字典中，后通过与全局字典的匹配对数据为空的省自动赋0值，由fill_empty函数实现。

---
## 3.3数据统计接口部分的性能改进
![在这里插入图片描述](https://img-blog.csdnimg.cn/1ee5eba1de334ba088c89536a45ec624.png)

可以看到程序在对excel文件操作消耗的时间较大，在地图数据统计中由于需要统计31天的数据，在程序中read_file函数消耗最大，主要由于每次读取一天的数据都会重新开关读取一次xlsx文件比较消耗内存，时间复杂度较差，故考虑将其更改为一次读取所有数据再一天一天地返回给地图来优化接口性能。即减少excel文件的读取次数来实现接口性能的优化
以下是测试程序，直接生成地图地html文件测试数据读取到可视化的效率

```

html = ChineseMap().ltimeline(2022,2,1)
```


## 3.4每日热点的实现思路
- 每日热点主要思路是通过新闻的标题来判断，本次采用MultinomialNB库实现即多项式朴素贝叶斯，它是基于原始的贝叶斯理论，假设概率分布是服从一个简单多项式分布。多项式分布来源于统计学中的多项式实验，这种实验可以具体解释为：实验包括n次重复试验，每项试验都有不同的可能结果。在任何给定的试验中，特定结果发生的概率是不变的。多项式分布擅长的是分类型变量，在其原理假设中，P(xi|Y)的概率是离散的，并且不同xi下的P(xi|Y)相互独立，互不不影响。多项式实验中的实验结果都很具体，它所涉及的特征往往是次数，频率，计数，出现与否这样的概念，这些概念都是离散的正整数。由于这样的特性，多项式朴素贝叶斯的特征矩阵经常是稀疏矩阵（不一定总是稀疏矩阵），并且它经常被用于文本分类。我们可以使用著名的TF-IDF向量技术，也可以使用常见并且简单的单词计数向量手段与贝叶斯配合使用。这两种手段都属于常见的文本特征提取的方法。
- 从数学的角度来看，在一种标签类别Y=c下，有一组分别对应特征的参数向量θ c = ( θ c 1 , θ c 2 , . . . , θ c n , ) ，其中n表示特征的总数

$$θ 
= \frac{特征X 
i在Y=特征在c这个分类下的所有样本的取值总和}{所有特征在Y=特征在c这个分类下的所有样本的取值总和} \quad  $$
记作P(Xi|Y=c)，表示当Y=c这个条件固定的时候，一组样本Xi在这个特征上的取值被取到的概率。
对于一个在标签类别下，结构为(m, n)的特征矩阵其中每个参数xji都是特征Xi发生的次数。基于这些，通过平滑后的最大似然估计来求解参数θ。MultinomialNB将样本分给得出后验概率最大的类别。

- 在函数实现上事先给 MultinomialNB训练样本进行训练，训练样本存于excel文件通过读取获得数据，将文本转换为矩阵并将其拆分为训练集和测试集，验证分类效果进行调试。
本次每日热点功能通过爬取卫健委官网首页的新闻并用该分类器进行判断是否为每日热点来实现，此次训练中主要标记于疫情相关的新闻才为热点其他则为非热点
效果如图
![在这里插入图片描述](https://img-blog.csdnimg.cn/d3e86ccf119a4e11a4b60a9fe8657286.png)

## 3.5数据可视化界面的展示
- 数据的可视化主要有两部分，都是用pychars实现一部分是利用地图展现，如图所示：
- ![在这里插入图片描述](https://img-blog.csdnimg.cn/77c91fef49d3492199d64f672dd4516b.png)
通过时间线的设置绘制出一个月中31天的疫情地图，若本月当天无数据如无31号的月份则31号数据赋值为空，各个省份根据其不同的确诊数量反映出不同的颜色。此组件为pychars包自动绘制，只需在函数中设置地图的参数，以及将本月的各个省份的疫情数据导入便可反映出疫情数据。且可点击播放自动循环播放每一天的疫情数据。另外一部分则是由折线图反映每日大陆新增的疫情人数如下图所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/92962aa8dd5246e99d26eff70dbe92a6.png)
此组件与地图绘制相较比较简单，需要调节的参数较少，反映本月份大陆新增疫情的趋势。
以上查询组件由pywebio集成实现查询功能，如下图
![在这里插入图片描述](https://img-blog.csdnimg.cn/089d4bdf2cf64b38ad306b6a10e16ac4.png)
通过输入查询的年月以及选择来查询相关的数据，需要按照格式查询，若格式错误则返回警告页面。上页面爬虫按钮点击则后端开启爬虫线程启动爬虫程序进行爬虫，每日热点则是搜索卫健委官网首页的新闻通过贝叶斯分类器返回认为是热点的标题及链接如下图所示
![在这里插入图片描述](https://img-blog.csdnimg.cn/72bb3e7058404535a6fc914cda2b1789.png)

- 设计思路上，主要考虑可以直观的反映出每天各个省份的疫情情况以及短期疫情发展的趋势，故在可视化上采用以上两个组件，地图组件需要输入单月每天的各个省份名称以及对应的疫情数据，采用zip函数将相应的数据捆绑返回给地图，而折线图组件则只需对excel表内共计那行数据进行读取返回即可相对比较简单
# 三、心得体会
## 4.1心得体会
经过本次作业，对软件设计的过程梳理有了进一步的认识，深刻理解了软件编程过程中事先规划的重要性，也意识到边编程边思考设计思路很难写出一份比较好维护，可读性较好的质量较高的代码。除此外，由于本次采用python编程对python语言的编程方法以及其各种常用的函数库也更加熟悉了，编程能力得到了很好的锻炼。同时将编程与日常生活相结合也对编程思路边界有了进一步的拓展。且每日热点功能中将机器学习应用于实践，使自身对于人工智能的应用以及技术发展有了更进一步的了解。本次作业中还是遇到了不少的难题，主要还是来自于对编程的不熟练，对代码底层运行原理不够了解，也暴露了自身的许多不足还需自己往后继续努力改进。
