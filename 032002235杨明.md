@[TOC]
1.（1.1）https://github.com/Yummy-y/2022-K-.git
上述是笔者自己在github上建的库，笔者开发期间的每次作业有进展即签入Github。在提交作业的压缩包里，笔者额外增加了仅保留核心代码的taskchanged.py文件。
# 一、PSP表格
## (2.1)在开始实现程序之前，在附录提供的PSP表格记录下你估计将在程序的各个模块的开发上耗费的时间。（3'）
| PSP2.1      | Personal Software Process Stages | 预估耗时（分钟）     | 
| :---            |    :---        |          :---     | 
| Planning    | 计划          |     |        
| Estimate  | ·估计这个任务需要多少时间      |  30    |        
| Development   | 开发        |      |        
|  Analysis   | ·需求分析 (包括学习新技术)     |   420    |        
| Design Spec  | ·生成设计文档      |   60    |        
| Design Review   | ·设计复审     |    15   |        
|  Coding Standard   | ·代码规范 (为目前的开发制定合适的规范)    |   20    |        
| Design   | ·具体设计      |   30   |        
|  Coding   | ·具体编码    |    300   |        
| Code Review   | ·代码复审        |  20  |        
|  Test   | ·测试（自我测试，修改代码，提交修改）        |   30   |        
| Reporting   | 报告        |      |        
| Test Repor   | ·测试报告        |     20  |        
| Size Measurement   | ·计算工作量        |    20 |        
| Postmortem & Process Improvement Plan   | ·事后总结, 并提出过程改进计划   |30   |     
|   | 合计      |   995   |       
## (2.2)在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间。（3'）

| PSP2.1      | Personal Software Process Stages | 实际耗时（分钟）     | 
| :---            |    :---        |          :---     | 
| Planning    | 计划          |     |        
| Estimate  | ·估计这个任务需要多少时间      | 30     |        
| Development   | 开发        |      |        
|  Analysis   | ·需求分析 (包括学习新技术)     |    450   |        
| Design Spec  | ·生成设计文档      |   60    |        
| Design Review   | ·设计复审     |   10    |        
|  Coding Standard   | ·代码规范 (为目前的开发制定合适的规范)    |  25     |        
| Design   | ·具体设计      |   40   |        
|  Coding   | ·具体编码    |   360    |        
| Code Review   | ·代码复审        |  20  |        
|  Test   | ·测试（自我测试，修改代码，提交修改）        |   30   |        
| Reporting   | 报告        |      |        
| Test Repor   | ·测试报告        |    20  |        
| Size Measurement   | ·计算工作量        |   20  |        
| Postmortem & Process Improvement Plan   | ·事后总结, 并提出过程改进计划   |  30 |     
|   | 合计      |    1095  |   
## 二、任务要求的实现
## (3.1)项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。（5'）
- 从阅读题目到完成作业我一共拆分成“任务分析与计划”、“学习新技术栈”、“数据爬取与提取目标数据”、“数据输出在excel后的分析与简单处理”、“数据可视化”、“测试总结”、“撰写博客”这几个环节。“任务分析与计划”环节我在认真阅读题目，询问同学题目中我不理解的点后认真规划。“学习新技术栈”环节我主要通过B站的网课学习以及求助同学。“数据爬取与提取目标数据”、“数据输出在excel后的分析与简单处理”、“数据可视化”这几个环节我在各个网站上学习他人思路以及观察各个网站疫情数据选择自己较好实现的网站进行数据爬取。“测试总结”环节主要是分析自己代码运行结果，对于性能优化方面进行更多考量。“撰写博客”环节重新阅读了任务要求以及询问自己任务有争议的点后开始着手编写。本次编程任务所用技术栈为python、html、css、JavaScript、echarts。
## (3.2)爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。（20'）
- **1.确定数据源**。网易的疫情播报平台的数据内容非常丰富，不仅包括国内的数据还包括国外的数据，且作为大平台，公信度也比较高。因此我们选择网易的疫情实时动态播报平台作为数据源。由于它是一个实时的动态平台，因此数据一般在Network标签下可以找到。
```javascript
# 开始请求，首先导入使用的包，使用request进行网页请求，使用pandas保存数据。
import requests
import pandas as pd
import time 
import json
pd.set_option('display.max_rows',600)
# 设置请求头，伪装为浏览器
headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
}
url = 'https://c.m.163.com/ug/api/wuhan/app/data/list-total'   # 定义要访问的地址 这里选择网易实时疫情播报平台
r = requests.get(url, headers=headers)  # 使用requests发起请求
```
- **2.1初步提取目标数据**输出数据源可以看到结果是个万长度的字符串，由于字符串格式不方便进行分析，并且在网页预览中发现数据在“data”属性下且为类似字典的json格式，所以我们将其转为json格式。
```javascript
data_json = json.loads(r.text) # 将数据转化为便于分析的json格式
data = data_json['data'] # 取出json中的数据/世界,中国各地疫情消息
```
- **2.2进一步获取目标数据**。在网页检查中可以发现在areaTree键值对中，存放着世界各地的实时数据，areaTree是一个列表，每一个元素都是一个国家的数据，每一个元素的children是各国家省份的数据。 通过该思路找到中国各省的实时数据，如下所示。
```javascript
# 开始提取目标数据
data_province = data['areaTree'][2]['children']  # 取出中国各省的实时数据
# print(data_province[0].keys()) # 查看每个省键名称 /需要today里的数据即每日新增确诊 其中extData里的数据即无症状感染者
for i in range(len(data_province)): # 遍历查看各省名称、更新时间（此例为数据中前5个省）同理可以得到全部省份的实时数据
    print(data_province[i]['name'],data_province[i]['lastUpdateTime'])
    if i == 5:
        break
```
- **3.关键算法——获取today即全国每日更新疫情数据并重命名**。该算法可以获取到我们想要的数据，思路如下，同理可获取全国疫情历史数据下方不再额外展示该过程。
```javascript
pd.DataFrame(data_province).head() # 直接生成数据效果并不理想
# 获取id、lastUpdateTime、name
info = pd.DataFrame(data_province)[['id','lastUpdateTime','name']]

# 列表推导式例子
l1 = [1,1,1,2,2,2]
[i+1 for i in l1 ]     

# 获取today中的数据
today_data = pd.DataFrame([province['today'] for province in data_province ]) 
today_data.head()
['today_'+i for i in today_data.columns] # 用上述推导方式命名提取的数据名字
today_data.columns = ['today_'+i for i in today_data.columns] # 由于today中键名和total键名相同，因此需要修改列名称
today_data.head() # 处理为我们希望的数据格式
```
- **4.1关键算法——获取各省疫情数据**。通过一系列数据剖析，找到第二个数据地址https://c.m.163.com/ug/api/wuhan/app/data/list-by-area-code?areaCode=420000 ，在页面对比发现是湖北省的历史数据。通过对页面各个数据进一步确认，可知，为了得到每个省的历史数据，我们只需要将各省的行政代码作为参数传入这个地址即可，即：https://c.m.163.com/ug/api/wuhan/app/data/list-by-area-code?areaCode=各省行政代码
- 但是数据中并没有显示省的名称，因此需要写入每个省的名称。为了便于写入各省的名称，我们需要生成一个各省行政代码和省名称对应的字典。以一个简单的数据为例，展示生成字典的方法。
```javascript
a = ['1','2','3','4']
b = ['q','w','e','r']

for i,j in zip(a, b):
    print(i,j)
{ i:j  for i,j in zip(a, b)}
province_dict = {num:name for num,name in zip(today_province['id'],today_province['name'])}

# 查看前五个内容
count = 0
for i in province_dict:
    print(i,province_dict[i])
    count += 1
    if count == 5:
        break
# 每一个省的列名是相同的，因此多个省的数据合并起来就可以存入一个数据中，数据合并演示的例子如下
df1 = pd.DataFrame([{'a':1,'b':2,'c':3,},{'a':111,'b':222}])
df2 = pd.DataFrame([{'a':9,'b':8,'c':7,},{'a':345,'c':789}])
# 合并
df1 = pd.concat([df1,df2],axis=0)
# df1
```
- **4.2关键算法函数封装**。
```javascript
# 总结上述方法
start = time.time()
for province_id in province_dict: # 遍历各省编号
    
    try:
        # 按照省编号访问每个省的数据地址，并获取json数据
        url = 'https://c.m.163.com/ug/api/wuhan/app/data/list-by-area-code?areaCode='+province_id
        r = requests.get(url, headers=headers)
        data_json = json.loads(r.text)
        
        # 提取各省数据，然后写入各省名称
        province_data = get_data(data_json['data']['list'],['date'])
        province_data['name'] = province_dict[province_id]
        
        # 合并数据
        if province_id == '420000':
            alltime_province = province_data
        else:
            alltime_province = pd.concat([alltime_province,province_data])

        # save_data(alltime_province,'alltime_province')

            
            # print('-'*20,province_dict[province_id],'成功',
            #       province_data.shape,alltime_province.shape,
            #       ',累计耗时:',round(time.time()-start),'-'*20)
        
        # 设置延迟等待
        time.sleep(10)
        
    except:
        print('-'*20,province_dict[province_id],'wrong','-'*20)
```
- **5.函数封装——保存函数和获取数据函数**。保存函数将爬取的数据输出位csv文件便于使用Excel查看以及建立折线图进行可视化。获取数据函数包装了4.1的算法便于我们在该网站上爬取格式合适的数据。过程如下
```javascript
def save_data(data,name): # 定义保存数据方法
    file_name = name+'_'+time.strftime('%Y_%m_%d',time.localtime(time.time()))+'.csv'
    data.to_csv(file_name,index=None,encoding='utf_8_sig')
    print(file_name+' 保存成功！')
    
# 将提取数据的方法封装为函数
def get_data(data,info_list):
    info = pd.DataFrame(data)[info_list] # 主要信息
    
    today_data = pd.DataFrame([i['today'] for i in data ]) # 生成today的数据
    today_data.columns = ['today_'+i for i in today_data.columns] # 修改列名
    
    total_data = pd.DataFrame([i['total'] for i in data ]) # 生成total的数据
    total_data.columns = ['total_'+i for i in total_data.columns] # 修改列名
    
    return pd.concat([info,total_data,today_data],axis=1) # info、today和total横向合并最终得到汇总的数据
```
- **备注**：提交的爬虫代码task.py中有意保留了在网页上剖析数据并分析处理的以及算法逐步设计改进过程。故除核心代码外还有部分与实现功能无关的测试代码与备注，核心代码即博客中所展示，望谅解。笔者在写完这段话之后重新更新了一份去除非核心部分的爬虫代码，命名为taskchanged.py。并附上requirements.txt文件以及将爬取到截止到17日的疫情数据并保存为csv文件，一并提交到作业文件夹中。
## (3.3)数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（6'）
- 数据接口功能尚未开发。
## (3.4)每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（6'）
- 设计思路：用上述所展示的疫情数据获取代码为起点，我们可以知道该数据格式为<class 'pandas.core.frame.DataFrame'>，我们通过对Python pandas中DataFrame逐行读取的方法（pandas.core.frame.DataFrame类型）对疫情数据（总确诊，今日新确诊数据）进行提取。例如我们发现today_province.loc[indexs].values[3]里存的即是目前该省总确诊人数。那么我们就可以借此设计一个简单算法，判断该省疫情是否严重（以总确诊人数是否超过20w为准）。
```javascript
# Python pandas中DataFrame逐行读取的方法（pandas.core.frame.DataFrame类型）
dict=[[1,2,3,4,5,6],[2,3,4,5,6,7],[3,4,5,6,7,8],[4,5,6,7,8,9],[5,6,7,8,9,10]]
datat=pd.DataFrame(dict)
print(type(datat))
for indexs in today_province.index:
    if today_province.loc[indexs].values[3] >= 200000:
        print(today_province.loc[indexs].values[2],today_province.loc[indexs].values[3],'serious');
    if indexs == 4:
        break
```
- 上方代码以台湾、香港、湖北、上海的数据为例，若总确诊人数大于20w则输出该省名称，总确诊人数和“serious” 上述代码输出如下：
```javaScript
台湾 5970438 serious
香港 406835 serious
```
- 以该思路可以实现很多的根据现有数据反应疫情情况的功能（我们可以操作任意一天的任一疫情数据），包括对多天数据的分析，（例如一周以来疫情情况是否平稳，近期确诊治愈人数增长情况等）。不过这种代码思路还是过于简单了，只能实现疫情数据的简单分析并反馈，，而且依赖遍历循环，代码的性能也比较一般。
## (3.5)数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。（15'）
- **可视化所用技术栈**。本次以截至2022年9月17日的疫情数据为例，作为可视化数据来源，所用技术栈为html，css，JavaScript，echarts。将以代码结合图片形式展示。
- **可视化组件展示**。上方有导航栏，展示全国累计确诊和全国今日新增数据，右方可以点击选择查看各省累计确诊或各省今日新增确诊数据。
![在这里插入图片描述](https://img-blog.csdnimg.cn/1d58e3726aa04f3da245c227dfbee09b.png)

![各省今日新增确诊人数](https://img-blog.csdnimg.cn/2add69ebc8084f57a3bb6c3524325841.png)
- **组件设计思路**。导入ECharts——一款基于JavaScript的数据可视化图表库。导入中国地图组件分别绑定不同的div（容器）并配置不同的数据（各省累计确诊与各省今日新增确诊）。在导航栏中“查看各省累计确诊”与“查看各省今日新增”所在的div上绑定点击事件，事件通过修改中国地图组件所在div的display属性来实现隐藏（none）与显示（block）对应div。关于css的样式配置与html标签的布局不多赘述，于下方代码给出。
 ```javascript
 <!-- css -->
      body {
        background-color: rgb(251,251,250);
      }
      #box ,#con {
        margin: 0 auto;
        padding: 15px 80px;
        background-color: rgb(255, 255, 255);
        border-radius: 2%;
        box-shadow: 0px 0px 1px rgba(138, 138, 138,0.5);
      }
      #con {
        display: none;
      }
      .ctrl {
        margin: 20px auto;
        background-color: aqua;
        display: flex;
        justify-content: space-around;
        background-color: rgb(255, 255, 255);
        box-shadow: 0px 0px 1px rgba(138, 138, 138,0.7);
      }
      #one , #two {
        cursor: pointer;
      }
      #one {
        color: gray;
      }
      #one :hover, #two :hover {
        transform:scale(1.1);
        transition: 0.3s;
      }
      
    <!-- html -->
    <div style="width: 80%;" class="ctrl">
        <div><h4>全国累计确诊:<span>6545234</span></h4></div>
        <div><h4>全国今日新增:<span>89446</span></h4></div>
        <div id="one"><h4>查看各省累计确诊</h4></div>
        <div id="two"><h4>查看各省今日新增</h4></div>
        <div><h4>待开发功能</h4></div>          
    </div>

      <div id="box" style="width: 800px; height: 500px;"></div>
      <div id="con" style="width: 800px; height: 500px;"></div>

      <div style="margin:10px 60px; padding: 10px;">
        <span>注：疫情数据来源于9月17日网易实时疫情播报平台</span>
      </div>
      
    <!-- js（给出实现切换视图的业务功能代码，地图组件的导入与配置代码此处不多赘述） -->
    var alltime = document.getElementById("box");
        var today = document.getElementById("con");
        var one = document.getElementById("one");
        var two = document.getElementById("two");
        one.onclick = function() {
          two.style.color="black"
          one.style.color="gray"
          alltime.style.display="block"
          today.style.display="none"
        };
        two.onclick = function() {
          one.style.color="black"
          two.style.color="gray"
          alltime.style.display="none"
          today.style.display="block"
        };
```
- **补充**。笔者实现爬取数据功能时将数据输出并转化为csv格式文件，在Excel中可将其转化为折线图。以湖北2020/1/20~2020/4/6期间累计确诊人数与2020/1/20-2020/6/30每日新增确诊人数取例如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/3b9a9596c1114469ad30e41df8abb24f.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/39e1dc8c61cd4a798092a16d11c1ebf2.png)


# 三、心得体会
## (4.1)在这儿写下你完成本次作业的心得体会，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~（10'）
- 本次作业难度很高，起初给我造成很大困难，不论是python的简单入门还是爬虫的上手都耗费很多精力但仍完成的难以让自己满意。对于数据统计接口部分的性能改进也是一头雾水。即使是完成了本次作业，再下次再有使用场景时对于其他网站上不同形式不同格式的数据还是难以保证可以顺利的爬取数据。但是总算是对python学习以及爬虫入门开了个头，为以后进一步学习做了铺垫。好在这次作业的数据可视化方面还算有所收获，通过导入echarts库手动配置参数以及开发过程中的debug都让我对于echarts的使用有了一定熟练度，下次再用使用场景的时候不会像开发前这样手足无措了。路漫漫其修远兮，学习无止境，我还需做很多的努力。


