## 2022年软工K班个人编程任务

GitHub链接：https://github.com/13062299165/Software-Homework.git

# 一、PSP表格

| PSP2.1                                | Personal Software Process Stages       | 预估耗时（分钟） | 实际耗时（分钟） |
| ------------------------------------- | :------------------------------------- | ---------------- | ---------------- |
| Planning                              | 计划                                   | 60               | 40               |
| Estimate                              | 估计这个任务需要多少时间               | 20               | 20               |
| Development                           | 开发                                   | 940              | 1200             |
| Analysis                              | 需求分析                               | 90               | 90               |
| Design Spec                           | 生成设计文档                           | 60               | 20               |
| Design Review                         | 设计复审                               | 30               | 0                |
| Coding Standard                       | 代码规范（为目前的开发制定合适的规范） | 0                | 0                |
| Design                                | 具体设计                               | 180              | 180              |
| Coding                                | 具体编码                               | 760              | 1200             |
| Code Review                           | 代码复审                               | 20               | 15               |
| Test                                  | 测试（自我测试，修改代码，提交修改）   | 240              | 240              |
| Reporting                             | 报告                                   | 60               | 70               |
| Test Report                           | 测试报告                               | 30               | 30               |
| Size Measurement                      | 计算工作量                             | 30               | 30               |
| Postmortem & Process Improvement Plan | 事后总结，并提出过程改进计划           | 120              | 60               |
|                                       | 合计                                   | 2640             | 3575             |

#  二、任务要求的实现

#### (3.1)项目设计与技术栈。

​			这次任务被拆分为3步，首先是先python、爬虫基础的学习，其次是对编程的设计，拆分编程的步骤，最后是代码的复审与测试，通过B站的教学视频，以及在CSDN上各种详解博客，我初步学习了python与爬虫，而对编程的设计，是先通过对网页内容分析解构完成的，将编程大体分为4步：爬取所有url，获取有效url中的内容，对内容进行解析，将得到的解析数据导入excel表格并绘制成图表，代码测试部分：准确性测试通过随机抽取日期省份，得到数据与官方,代码速度测试通过python的time函数测试

本次任务使用的技术栈：python

**（****5'****）**

#### (3.2)爬虫与数据处理。

1. 对卫健委的疫情通报页面（以下称为父页面）源码进行分析，发现所有的疫情情况通报（一下称为子页面）的url均存储在class="list"的容器下的li标签中，于是第一步得以确任，首先爬取所有页面的li标签并进行提取，并且在爬取子页面url前还需要获取父页面的页面数量，根据规律得出各个父页面的url，才能爬取其下我们需要的子页面url

   <img src="http://m.qpic.cn/psc?/V50VF2sc2aa3PK46fgUL1SmNl33aaUZg/ruAMsa53pVQWN7FLK88i5ovck5U2FTNU5H7FJ4fxUTxdvvXmWsAyMWp3LMqzzbDOQH3rmlLPnbL2LLY.bo*DVOrm.rJqkJjJoae5r*DrBEc!/mnull&bo=AgT4AAAAAAABB94!&rf=photolist&t=5" style="zoom: 67%;" />

   <img src="http://m.qpic.cn/psc?/V50VF2sc2aa3PK46fgUL1SmNl33aaUZg/ruAMsa53pVQWN7FLK88i5jaU1A8nHLEbYLtqD0Myv0Q7fE4cdRil8XSQHUDcV4Hw7jYUeVnHGc0P5GSApJa7b2BdNKqpstEdCodVCakU9Wk!/mnull&bo=9QPvAAAAAAABBzk!&rf=photolist&t=5" style="zoom: 67%;" />

   

2. 其次，由于需要的数据文本内容在每日的通报页面中，因此在数据分析前需要爬取各个子页面，这一步就是简单的爬取过程，但是也是整个的难点，由于卫健委的防爬机制，若直接一个个爬取失败率极高，需要设计，绕开防爬机制，提高爬取效率

3. 对所有的页面解析完成后进行的是数据处理，需要将页面中包含新增确诊病例与无症状感染者的关键文本检索出

4. 当得到关键文本后，进一步进行提取数据，并将所需数据导入excel,所有数据导入完成后利用excel实现可视化

基于上面四步，我设计了6个函数：

- **geturl**用于获取所有疫情最新情况的子页面的**url**，
- **download**用于爬取并且下载页面的文本内容
- **textget**与**getnum**函数用于文本分析与数据提取并将数据填入表格，**init_excel**在**textget**执行前初始化excel表格的单元格属性与数据
- **drawcharts**依托生成的excel绘制图表

​			其中最为关键的函数为download函数，由于总共有980多个页面需要爬取，简单的循环访问爬取极易被防爬，获取不到需要的界面，程序也可能进入死循环，因此这一步需要进行算法的优化，以及一定的防爬处理

*我的代码如下*：

```python
while True:
    end =time.time()
    if end-start>2000:
        print("系统繁忙，请稍后再试")
        exit()
    for i in range(0, length):
        if flg[i] == 1:
            continue
        contents = ""
        # 随机生成user-agent
        headers = {
            "User-Agent": random.choice(ua_list)}
        response = requests.get(url_list[i], headers=headers)
        response.encoding = "utf-8"
        contents = response.text
        response.close()
        # 有情况的表示页面正确
        test = re.findall('情况</div>', contents)
        if test.__len__() == 0:
            continue
        # 特殊情况刨除
        if re.findall("订正情况", contents):
            continue
        # 成功则置位，后续不再爬取
        flg[i] = 1
        cnt += 1
        soup = BeautifulSoup(contents, "lxml")
        # 得到文本内容，以发布日期为文件名存储
        after = soup.get_text()
        Day = re.findall(r"\d+-\d+-\d+", after)[0]
        fp = open(f"疫情文本/{Day}.txt", 'w', encoding="utf-8")
        fp.write(after)
        fp.close()
    end = time.time()
    print("已爬取"+str(cnt)+"个页面内容，并保存，累计耗时"+str(end-start)+"秒")
    if cnt == length - 14:
        break
```

​				该段代码基于一下直接爬取的缺点进行改进：

-  持续对一个同一个页面进行爬取，极易触发防爬机制，并且使得程序停滞
- 对所有页面进行爬取，无法一次成功

​			基于以上两点，我将算法设计为”不吊死在一棵树上“的方式，依旧是使用requests进行爬取，但是不直接对同一个url重复爬取，而是将逐个爬取爬取url,成功便存储爬取到的内容，但是失败也不会“‘死缠烂打”,而是转而去爬取下一个url，这样一次循环下来（代码中的for循环），先爬取一部分页面内容，再重头开始爬取那些没有爬取成功的页面，循环往复，最终爬取下来

以上代码的简化框架：

```
# 全部爬取一遍后未爬取完成便重头继续
while True:
# 逐个爬取
	for i in range(0, length):
		# 如果爬取成功不再爬取
        	if flg[i] == 1:
            	continue
			.....直接爬取
		爬取成功：flg[i]=1
				进行进一步的文本保存
			失败：flg[i]=0
if 爬取总数达标
	break
```

​				但是这样的代码依然绕不开效率与防爬的问题，在初始版本的测试中，这一段代码的运行时间为45分钟左右，效率低，本来计划使用IP代理池来减少防爬的几率，并提高效率，但是网络上的免费IP不稳定，甚至有时无法访问url界面，故放弃了这个方法，在CSDN上，我发现了另一种代理方法，user_agent代理池，导入ua代理池后，我每次爬取都随机生成user-agent，测试后发现效率有所提升，15分钟左右可以完成所有的爬取

![](http://m.qpic.cn/psc?/V50VF2sc2aa3PK46fgUL1SmNl33aaUZg/ruAMsa53pVQWN7FLK88i5lIfh5wOBwL7Dj01bl3M4I4nb0HMWNl26t.vnuIoW.VD6UH5s0j898LvCvZp9zFkyOBxZaa72YF2Z38raOWEANE!/mnull&bo=.gJ4AQAAAAABB6E!&rf=photolist&t=5)



#### **(3.3)**数据统计接口部分的性能改进。

数据统计部分由原来的bs4检索网页源代码，改为用re检索网页文本内容，减少了由于网页源代码格式改变带来的麻烦

####  (3.4)**每日热点的实现思路**。

这一部分没有实现

#### (3.5)**数据可视化界面的展示**。

导入后图像有些不清晰，但是大体如下图：

**excel表格**：

![](http://m.qpic.cn/psc?/V50VF2sc2aa3PK46fgUL1SmNl33aaUZg/ruAMsa53pVQWN7FLK88i5lIfh5wOBwL7Dj01bl3M4I4y6SH.wGNJochxj8vfUyXwPaxZd9CrTxUUshLaq60jwo7HSBV0JM7.pjVH9lEJx*I!/mnull&bo=nwUZAgAAAAABB6E!&rf=photolist&t=5)

**折线图**：(无症状部分)

<img src="http://m.qpic.cn/psc?/V50VF2sc2aa3PK46fgUL1SmNl33aaUZg/ruAMsa53pVQWN7FLK88i5jlK0YnBsF560*Lr1nyHNY9svAn..OiwIkPoZLEBItret.ZPos.tMt9zI.X2BbIhfuE*9Rvf1OshdqDgIQd9vDI!/mnull&bo=pgXpAgAAAAABB2g!&rf=photolist&t=5"  />

![](http://m.qpic.cn/psc?/V50VF2sc2aa3PK46fgUL1SmNl33aaUZg/ruAMsa53pVQWN7FLK88i5lF5D8cwGglKSO6vBTBW2kQFZv2GyXOHd*6hcrW6t01rV.zEjRhyo46HUQjMwb5hV6Kk2UGczZwKDH6O.QdXK6o!/mnull&bo=DAYXAwAAAAABBz4!&rf=photolist&t=5)

​		我将所有省份的数据集合在一个html文件的折线图中，如果想要单个省份的数据，将其它省份去除即可（需要承认，这一部分功能还不够完善），日期范围可以通过下方滑动条进行变化

这一功能使用了pyecharts的包进行绘图，思路是先利用xlrd包对excel内的单元格数据进行抓取，先抓取日期信息作为x轴，再抓取数据，在理由pyecharts处理，新增确诊与无症状分为两个html文件导出

大致过程如下（非程序代码，只是展示大致方法）：

```python
line = Line()
# 将此前获取的日期信息设置为x轴
line.add_xaxis(dates_list)
# 滑动条属性：由于日期时间太长，无法直接显示，于是利用滑动条控制
atrrs=[{"show": "true","realtime": "true","height": "24", "bottom": "8","start":"0" ,"end":"5",},]
line.set_global_opts(title_opts=opts.TitleOpts(subtitle="新增确诊"),datazoom_opts=atrrs)
for i in range(0,len(pro_list)):
# 将表格中的新增确诊数据导入图表
    num_list=sheet.col_values(2+i)[1:]
    ret1_list=[]
    for j in range(0,len(num_list)):
        if j%2==0:
            ret1_list.append(num_list[j])
    # province[i]为属性
    line.add_yaxis(province[i],ret1_list)
# 将图渲染为html文件
line.render("新增确诊.html")
```



# 三、心得体会

​	这次的作业学习了python与爬虫的内容，在完成作业的过程中也学到了很多，利用CSDN与搜索引擎进行信息检索有所提升，并且让我知道，代码优化的提升对程序运行速度的提升是多么明显，这在以往做PTA的时候是感受

不到的，毕竟当时就顶多就是超时，不超时，而现在是实在的时间，如果不进行优化，每一次的调试运行都将浪费大量时间

