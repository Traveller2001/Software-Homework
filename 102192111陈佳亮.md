@[TOC](【看疫，抗疫】2022·K班个人编程任务)


* [GitHub链接](https://github.com/374671984/Software-Homework.git)
# 一、PSP表格
|PSP2.1 | PSP Stages | 预估耗时(分钟) | 实际耗时(分钟)|
|-------- | ----- | ----- | -----|
|Planning  | 计划| 3600 | 4860|
|· Estimate | · 估计这个任务需要多少时间| 3600 |  4860
|Development   | 开发 | 3300 |  4590|
|· Analysis   | · 需求分析 (包括学习新技术)| 1200 | 1200 |
|· Design Spec   | · 生成设计文档 | 60 | 60 |
|· Design Review   | · 设计复审 | 30 | 30 |
|· Coding Standard | · 代码规范 (为目前的开发制定合适的规范)| 60  |  60|
|· Design  |· 具体设计|  60 |  120 
|· Coding  | · 具体编码| 1200  |  2400
|· Code Review  | · 代码复审| 120  |   120|
|· Test  | · 测试（自我测试，修改代码，提交修改）| 600  |   600 |
|Reporting  | 报告 |  270 |  270 |
|· Test Repor  | · 测试报告 | 180  | 180  |
|· Size Measurement  | · 计算工作量 | 30  |   30|
|·Postmortem & Process Improvement Plan|·事后总结, 并提出过程改进计划| 60  |   60|
|     | · 合计| 3570  |   4860|

# 二、任务要求的实现
## (3.1)项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。（5'）
* 拆分为4个环节：
1、爬虫基本使用方法，包括html怎么读，爬虫怎么抓取；百度学习
2、爬虫中html文本的获取；百度学习
3、爬虫中正则表达式的使用，需要的数据的截取；百度学习
4、可视化；百度学习
* 技术栈：就是用python调用各种包，又不做网页什么的，感觉没有什么技术栈可言
## (3.2)爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。（20'）
```
# 创建表
def creat_file(sheet_name, file_name, data):
	定义工作路径
	创建工作簿wookbook（file_name）
	创建活动表sheet（sheet_name）
	激活活动表
	插入表头（data）
	保存表头数据
```
```
# 获取数据并保存到excel中
def get_data(first_page, last_page, file_name1, file_name2):
	# 抓取 first_page 到 last_page 的数据
	for page in range(first_page, last_page):
		#从代理池选择一个User-Agent
		ua = random.choice(ua_info.ua_list) 
		# 选择第page页的url
		url = "http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_%s.shtml" % page 
		# 构建请求头
		header = {"User-Agent": ua, "Connection": "close"}
		#获取页面的html文本
		html = requests.get（）.text
		list1 = etree.HTML(html)
		lis = list1.xpath('//li/a/@href')  # 链接url对应的文本
		
		# 通过for循环得到该页面中每一条链接的url,并按顺序进入其中一条
        for path in lis:
        	url1 = "http://www.nhc.gov.cn%s" % path
        	header1、html1、list2，获取该链接的html文本
        	# 提取该链接下所有正文
        	lis1 = list2.xpath('//div[@class="con"]//text()')
            time.sleep(1)
        	for g in lis1:
	                i += str(g) + " " # 正文文本输出为字符串
	        text1 = re.split(r'。', i) + ['0'] # 拆分
	        text = str(text1)  # 转为字符串
	        
	        # 通过text获取日期
			yue = re.findall（r'截至(.*)月', text） + ['0']
	        ri = re.findall（r'月(.*?)日', text） + ['0']
	        date = yue[0] + '-' + ri[0]
	        
	        # 匹配正文文本text，得到新增确诊病例，关于本土病例的文本quezhen
	        quezhen1 = re.findall(r'本土病例.*?）', text)
	        可能有多个文本为本土病例，获取quezhen2 = quezhen1[0]
	        将获取到的quezhen2通过循环整合为一个字符串quezhen

			# 通过正则表达式匹配各个省份以及本土新增确诊病例的数量
			如china1 = re.findall(r'本土病例(.*?)例', quezhen) + ['0']

			# 匹配正文文本text，得到关于新增无症状感染者的文本wu
			wu1 = re.findall(r'新增无症状感染者.*?）', text)
			获取列表第0项：wu2 = wu1[0]
			转为字符串 wu
			
			# 通过正则表达式匹配各个省份以及本土新增无症状感染者的数量
			如china2 = re.findall(r'本土(.*?)例', wu) + ['0']
			
			# 各个数据项赋值以后，分别整合到列表data3，data4中
			data3 = [日期，新增确诊病例，本土，各省份]
			data4 = [日期，新增无症状感染者，本土，各省份]
			# 判定如果新增确诊和新增无症状感染的数据为0，说明爬取到的页面为空页
			面，即没有爬取到数据，不写入excel，反之写入
			if data3[1] != '0':
				sheet1.append(data3)
				workbook1.save(file_name1)
			data4同理		
```

```
# 程序入口
if __name__ == "__main__":
    # 两张表的表头数据
    data1 = [
        '时间', '新增确诊病例', '境外输入', '本土新增感染', 各省份 ]
    data2 = [
        '时间', '新增无症状', '境外输入', '本土新增无症状', 各省份 ]
    # file_name的工作簿名称，sheet_name为活动表名称
    定义20年、21年、22年的名称
    如：
    sheet_name1 = '22新增确诊'
    file_name1 = '22新增确诊_测试.xlsx'
    sheet_name2 = '22新增无症状感染'
    file_name2 = '22新增无症状感染_测试.xlsx'

    # 通过while循环强制爬取，直到手动停止程序为止
    while 1:
        try:
            # 判断文件是否存在，如果存在，则在空白的行插入获取的新数据，
              如果不存在，就创建一个execl文件，并插入表头
            # 2022年相关的页面循环完成以后，便开始2021年相关页面数据的提取，而后是2020年
            if os.path.isfile(file_name1) and os.path.isfile(file_name2):
                # 获取22年数据并保存到excel中
                first_page1 = 2
                last_page2 = 12
                get_data(first_page1, last_page2, file_name1, file_name2)
            else: # 没有表格就创建表格并插入表头
                creat_file(sheet_name1, file_name1, data1)
                creat_file(sheet_name2, file_name2, data2)
			# 21年与20年同理，设定page页面范围即可
        except:
            # 可能存在网站服务器太差，但爬取请求的速度太快而导致服务器没有反应，
            5秒后再继续爬取
            time.sleep(5)
            continue
```

* 一共定义了三个函数，两个实现功能，一个为程序入口进行调用实现。第一个函数为execl表的创建creat_file，第二个函数为数据的获取和保存get_data，程序入口通过调用前两个函数来实现所有功能。
* 几个关键思路为：
	1、通过判定文件是否存在来调用creat_file，从而实现将不同年份的数据，新增确诊和新增无症状感染者的数据分开来保存，从而使数据的保存与处理更加方便快捷。并且通过文件是否存在的判定，使得写入文件时不会被覆盖。
	2、通过get_data获取数据，关键思路为：
	1）先获取外部页面的html文本，然后得到链接的url，再通过链接的url去获取正文的html文本。这样处理的目的在于每一条链接的url都有一个部分是毫无规律，且不同的，只能通过外部页面来获取。
	2）数据的获取，难点在于正文文本的获取以及数据如何通过正则来摘取，思路主要在于，对于新增确诊，可以直接摘取第一个带有“本土病例”的数据，一直到右括号截至，这样几乎所有的数据都可以被匹配到。对于新增无症状感染者，由于该文本只出现一次，所以从该文本开始摘取到右括号即可，几乎所有数据也可以被匹配到。省份的匹配较为简单，唯一的难点在于出现“均在xx”会被漏掉，暂时未更新判定来处理该问题，此外河北省由于天津有河北区也有极少部分数据出现问题，也暂未处理该问题。
	3、通过死循环，和try来实现强制爬取，此外还有一些方法诸如headers的Connection参数修改为close等。
	4、通过判定关键数据为0时，为空页面，不写入表格，从而避免太多无效数据的写入
	5、增加了新增确诊病例，新增无症状感染者，境外输入的数据，因为获取这些数据以后，方便我更快的判断爬取的数据是否有问题，是否准确。
	6、由于第一页的最新24天的数据url链接不太一样，后面的页面都是“_数字”，第一页不是，所以新建了一个python文件先爬取，至于整合在一起的话，目前还没有优化。方法代码都是一样的，只是稍微改了一下url那个部分。
## (3.3)数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（6'）
* 没来得及搞，还在下载软件
## (3.4)每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（6'）
* 通过贝叶斯来实现每日热点的功能，通过自己定义一些训练集，即自己认为是热点的一些文本，用贝叶斯来训练自己的分类器，而后在一些大的网站，比如百度，腾讯等去获取网页的标题，然后通过训练出来的贝叶斯分类器分类，获取热点的标题与链接，并进行展示。
* 原理：设每个数据样本用一个n维特征向量来描述n个属性的值，即：X={x1，x2，…，xn}，假定有m个类，分别用C1, C2,…，Cm表示。给定一个未知的数据样本X（即没有类标号），若朴素贝叶斯分类法将未知的样本X分配给类Ci，则一定是
P(Ci|X)>P(Cj|X) 1≤j≤m，j≠i
根据贝叶斯定理，由于P(X)对于所有类为常数，最大化后验概率P(Ci|X)可转化为最大化先验概率P(X|Ci)P(Ci)。如果训练数据集有许多属性和元组，计算P(X|Ci)的开销可能非常大，为此，通常假设各属性的取值互相独立，这样，先验概率P(x1|Ci)，P(x2|Ci)，…，P(xn|Ci)可以从训练数据集求得。
根据此方法，对一个未知类别的样本X，可以先分别计算出X属于每一个类别Ci的概率P(X|Ci)P(Ci)，然后选择其中概率最大的类别作为其类别。
* 由于朴素贝叶斯算法成立的前提是各属性之间要互相独立。当数据集满足这种独立性假设时,分类的准确度较高，否则可能较低。另外，该算法没有分类规则输出。由于热点的训练集是自己给定的，所以局限性比较大，抓取的标题可能并不会太准确，而且限定范围太死。优点的话，相对于决策树，算法的时间复杂度更低，只需要判定属于那个属性的概率更大就好了，会更简单一点，而决策树则是会有很多分支，虽然可以分类别，但是分支太多，太复杂，分类结果不一定会准确。
* 改进方案：尽可能选择互相独立的属性，训练集的设定是一个比较大的问题，还未想到怎么解决。
## (3.5)数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。（15'）
1、地图
* 绘制关于各个省份的2022年新增确诊人数，新增无症状感染者人数的地图。人数为0的单独列了出来以作区分。
![https://img-home.csdnimg.cn/images/20220524100510.png](https://img-blog.csdnimg.cn/7e90de5573ec40c2a4dd0739677504f6.png)

![https://img-home.csdnimg.cn/images/20220524100510.png](https://img-blog.csdnimg.cn/1739a46a6a474ef2887b88ab246d694b.png)
* 各省份的数据通过地图来制作，``from pyecharts.charts import Map, Timeline``，使用pyecharts.charts的MAP和Timeline组件来进行设计。设计思路为：先加载xlsx文件的数据，然后获取该文件的活动表对应的数据，之后对数据进行处理。先循环获取每一行的行值，然后得到每一行的数据以及相对应的时间，之后再循环得到对应省份的列在这一行的值，即可得到对应时间下，各个省份的数据。之后再将这个数据和省份名称通过zip关联在一起，然后用map绘制地图即可，同时通过Timeline来进行时间线的绘制，通过上述循环，即可每个时间都绘制一张map，然后通过时间线即可查看每一个时间对应的map，还可以自动播放。
* 本来想放链接，但是会失效，需要url给予权限，所以就随机选择了一天展示，具体请在zip压缩包中用浏览器打开对应的html文件进行查看，或者运行源代码以后，通过浏览器打开对应生成的html文件。

2、柱形图
* 绘制2022年每日本土新增确诊人数，本土新增无症状感染者人数的柱形图。
* 以时间为横坐标，本土新增人数为纵坐标。
![在这里插入图片描述](https://img-blog.csdnimg.cn/d189c511609c46cda5e166c206ce16ea.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/164a8f96e35d4ea49df4a793707b2853.png)
* 本土新增数据通过柱状图来制作，``from pyecharts.charts import Bar``，使用pyecharts.charts的Bar组件来进行设计。设计思路为：先加载xlsx文件的数据，然后获取该文件的活动表对应的数据，之后对数据进行处理。先循环获取每一行的行值，然后得到每一行相对应的时间和本土数据，之后通过append将数据添加到for循环外面的变量，这样就得到了所有的时间和本土数据。然后以时间数据为横轴，本土数据为纵轴，用Bar绘制柱状图即可，可以通过下方进行横向拉动，来查看全部或者局部时间的柱状图数据。
* 本来想放链接，但是会失效，需要url给予权限，所以就随机选择了一天展示，具体请在zip压缩包中用浏览器打开对应的html文件进行查看，或者运行源代码以后，通过浏览器打开对应生成的html文件。

3、一共设计了4个函数，get_data、get_sheet、creat_map、creat_bar，前两个就是获取文件中的数据，后面就分别是创建地图和创建柱形图，在程序入口进行调用以实现功能，并编辑自己想要写入的文件和输出的文件名，这样子就可以只在程序入口处简单修改，然后得到想要的数据可视化。

# 三、心得体会
* 本次编程作业难度还是比较高的，主要在于爬虫还有数据可视化都是第一次接触，需要学习的东西很多，至于python，只要有一些C语言的基础，编写的话基本没什么问题。主要难点还是爬虫部分的编写，而造成难度的原因自然是卫健委的网站不好爬取了，几个比较卡我的点在于：
（1）链接的url有一段是没有规律的，爬虫只是刚开始接触，这种情况一下子就把我整蒙了，后面才发现这一段无规律的url在页面的html文本里面，这就造成了必须两个循环嵌套才能得到想要的文本，时间复杂度被迫n^2。
（2）正文文本不好获取，我用的是xpath的包，所以一开始是通过子节点来获取文本，而子节点的子节点还有文本，并且两个是一个在前一个在后，每一段顺序还都不一样，所以只能提取到一半的链接的数据。后面查看数据的时候发现总是会漏掉很多，才发现这个问题，便尝试着干脆把父节点所有文本都获取看看，也不知道子节点这种情况获取的文本顺序是否会变化，后面实践证明这个思路是对的，经验值加1。
（3）正则表达式的问题，文本提取一开始有些歧义，导致提取出来的数据可能是错误的数据，比如摘取的句子太多了，除了本土的省份也给摘进来，那么这些不需要的省份数据也会被写进来，后面我进行split重新切割划分以后，算是成功的把需要的句子择出来了，句子直接就是对应的我需要的那个句子，不会有其他省份乱入。但是还有一些细节，比如均在xx省，比如河北省和天津的河北区，比如均为境外输入，这三种非常特殊的情况我还没来得及优化。但是由于情况特殊，所以很少出现，自己去对应日期简单改一改就可以了，毕竟就两种错误，一种是河北，一种是均为。
（4）还有日期的问题，由于有空格，回车，而且有两处日期，正则不太好择，特别是日，好多次都是把后面的文本全择进来了，我也搞不懂，明明最后一个字限定了日，也是比较特殊的，但一直会有问题，后面也不知道怎么的就搞好了，但还是有看到极少部分有该问题，而且导出到execl以后日期格式有空格（毕竟不匹配空格择不出来），所以没办法改为日期格式来排序，导致排序没办法完美排好，后面干脆自己简单输入日期，然后转成日期格式了，excel处理还是算快的。
* ​爬取数据的话，目前22年的数据是基本没问题的，除了上述的极特殊情况，带来的十几天数据有略微问题以外，99%数据都很完美，并且22年所有日期数据都爬取到了。21年可能是爬取时间不够，有少数天数没爬取到，并且由于文本更多情况出现上述问题，所以数据存在比较大的问题，所以对于“均在xxx”的这种情况是必须要优化的，但是我没时间优化了，而如果这个问题也优化了，那么该爬虫对于卫健委的数据基本可以实现几乎完全没错的爬取。20年问题会更大一些，因为文本会更不一样。
* 我的爬虫是通过死循环强制爬取的套路来的，因为有出现未响应的问题，后面根据前人的经验，采取该策略，感觉还是比较可行的，至少我最久的一次是爬了十五六个小时吧，没出现问题。
* ​sleep定了1秒，不敢爬太快，爬取速度算比较慢，而且大部分还是重复和空页面，所以要获取到绝大部分数据需要时间会久一点。
* 增加了新增确诊病例，新增无症状感染者，境外输入的数据，因为获取这些数据以后，方便我更快的判断爬取的数据是否有问题，是否准确。
* 绘制地图算比较简单。只需要百度一下，看别人是怎么使用包，在哪里填入需要的数据即可，主要难点在于数据的导出与提取，一个是excel文件的数据要怎么导出，``xlrd``包新版本不支持``xlsx``文件，所以我特地下载了``xlrd_compdoc_commented``包；另一个是数据的提取，首先是日期，需要将浮点型数据转为字符串格式，不然会输出一串数字，还专门百度了一下，另外就是我省份保存在行而不是列，虽然这样子爬取数据的时候append比较简单，但是导出时需要通过for循环将需要的数据给选择出来，稍微比较麻烦一点点，但是对于编程经验比较成熟的人来说，应该也不算难。
* 一些其他的思路：还可以通过判定获取的数据是否和文件里面的数据相同来决定是否要写入文件（通过日期），虽然感觉没必要，毕竟execl表格可以删除重复数据，但是如果不是写入execl呢，如果是要实时爬取更新数据呢？那么如此多无用重复的数据就会有影响了，而且太多重复数据还需要额外去处理，所以感觉是有必要的。
