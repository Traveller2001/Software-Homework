新冠疫情本土病例分析挖掘
======================================


# 仓库创建
>在Github仓库中新建一个学号为名的文件夹,同时在博客正文首行给出作业Github链接，并登录软工在线平台完善信息。（2'）

>请用以下列一级标题分割你的博客（冒号后的文字设置为一级标题）——博客评分为半自动，如果没有按要求分割博客，造成评分出现问题，将不予处理。

# 一、PSP表格
开始前后PSP表格
![PSP表格](https://img-blog.csdnimg.cn/97a80f27801446beab92c5a57d47c877.png)

# 二、任务要求的实现
**(3.1)项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。（5'）**
>这次的任务从设计的角度来讲，分为了项目需求和设计、代码编程实现、以及测试报告三大环节。其中最主要的是代码编程部分，这里面经过资料查阅学习等，将代码实现部分也分为了三大环节：数据的爬虫获取、数据的处理、以及数据可视化。
>本项目是一个基于python+PyCharm (Pyhon IDE)+Jupyter notebook (Pyhon IDE)进行开发实现，涉及到的技术有：
>- **Python 网络爬虫**
>- **requests请求库和beautifulsoup4、lxml解析库**
>- **正则表达式**
>- **json模块与python间的转换**
>- **基于Jupyter数据可视化展示**

**(3.2)爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。（20'）**
>通过爬虫技术（模拟客户端发送网络请求，获取相应数据）来获取想要获得的疫情数据，并对获得的疫情数据按照一定规则进行解析处理，最后将数据进行保存。

>**① 发起请求，获取响应**
>通过http库，对目标站点进行请求。等同于自己打开浏览器，输入网址。
>常用库：**urllib、urllib3、requests**
>服务器会返回请求的内容，一般为：html、二进制文件（视频，音频）、文档，json字符串等。在此次项目中，我们采用requests库来发送请求，获取相应数据 。
>- **安装: pip install requests**
>- **requests.get(url)**

安装requests库命令
![安装requests库命令](https://img-blog.csdnimg.cn/4edbceaf4cd246a1946451735d7c853a.png)
requests核心代码1
![requests核心代码1](https://img-blog.csdnimg.cn/038126412a7044a5bafd9ad313aae6e7.png)
requests核心代码2
![requests核心代码2](https://img-blog.csdnimg.cn/05fd3d0971fa47c5be2060cd7c9584ee.png)

>**② 解析内容**
>寻找自己需要的信息，就是利用正则表达式或者其他库提取目标信息
>常用库：**re、beautifulsoup4**
>
**使用beautifulsoup4解析内容：**
beautifulsoup4将复杂的HTML文档转换成一个树形结构，每个节点都是Python对象
- 安装：pip install beautifulsoup4
- BeautifulSoup(html)
- 获取节点：find()、find_all()/select()、
- 获取属性：attrs
- 获取文本：text'

bs4安装命令，注：还需要安装lxml（cmd命令：pip install lxml）
![bs4安装命令，注：还需要安装lxml（cmd命令：pip install lxml）](https://img-blog.csdnimg.cn/ab85f75d7c584293a3d88a516339116a.png)


BeautifulSoup对象的find方法核心代码
![BeautifulSoup对象的find方法核心代码](https://img-blog.csdnimg.cn/6154c8439d9843d489e99fb17e68c20e.png)
**使用re（正则表达式）解析内容：**
正则表达式是一种字符串匹配的模式（pattern）。
- re.findall() 方法
- re.search( regex ,str)

>③ 保存数据
>将解析得到的数据持久化到文件或者数据库中，这里面使用了jason模块（jason与python之间的转换）
- jason字符串转换为python类型数据
- json.loads(s)
- json.load(fp)
- python类型数据转换为jason
- json.dumps(obj)	#转为json字符串
- json.dump(obj, fp)		#转为json格式文件

JSON与Python数据类型的对应关系
![JSON与Python数据类型的对应关系](https://img-blog.csdnimg.cn/d30062164c574d539de7151f0e450dfc.png)
![核心代码](https://img-blog.csdnimg.cn/642e64310393483ca6719dbda5b01ef9.png)

**(3.3)数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（6'）**
![注：太长没截图完](https://img-blog.csdnimg.cn/e9a50320f7c34dbd8a1df0628444ebf1.png)

![Profile](https://img-blog.csdnimg.cn/4955958433ee459281747ff44189d68f.png)
**(3.4)每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（6'）**
>每日热点方面，最简单可以想到的是通过爬取百度疫情热搜数据，来提取热搜关键词，形成词云，并链接相应关键词的系列链接来实现。要想进一步实现的话，则需要对新闻内容进行爬取，并设置相应的判断训练集来训练是否为疫情相关热点的内容。

**(3.5)数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。（15'）**
>数据可视化方面，实现比较简单，仅通过Jupyter来实现地图疫情的分级显示，以及疫情确诊数量按省份排名的条形图。
![在这里插入图片描述](https://img-blog.csdnimg.cn/03081c5b6f034165ab00b765d1e8b0d0.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/6fd79504b79642b682f9362691d06207.png)
>通过查询资料，还有其他方法可以实现。通过应用flask来实现参数、数据获取，以及可视化大屏的设置。此外，还可应用Echarts来实现更多样的商业化数据图表制作。但对这一部分仅做了简单的了解，没有进一步去实现。

#三、心得体会
**(4.1)在这儿写下你完成本次作业的心得体会，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~（10'）**
>通过本次作业，较为完整了体验了一遍爬虫获取数据、管理数据和展示数据的基本逻辑和流程。特别是配合PSP模板，去学习练习个人开发的整个流程，对个人开发方面有更具体的认识。
>但从实现效果上来讲，仅勉强达到了作业要求，还没有达到一个良好的自我预期。在开发过程中，要学习不少的知识点，和做一些前置的准备操作。这些需要通过网上查询资料，一点点操作，碰到具体问题就具体查询解决，但有时候一个问题也很容易卡住很长时间。特别是前面安装软件、配置环境等方面，常常报错，就需要查询原因和可能解决方法，并进行尝试。学着用cmd相关的shell命令，配置环境路径等。
>特别是在编程开发的功能实现上，学习得还不够深入，仅做了一些初步了解，没有花更多时间去优化实现功能效果。
>数据获取，我没有从卫健委的官网上进行爬取数据，最后仅从丁香园官网这类第三方网站获取数据。一方面是时间紧凑没有预留出时间，尤其是又碰到充电器坏了临时买耽误两天时间。在再一个是直接从源头官网获取需要处理一个爬虫漏数据的情况，以及对获取的文本的处理需要更多的开发。
>在数据处理和应用方面，也仅简单的实现了对数据整体确诊情况的管理，没有进一步去对数据进行新增确诊等更进一步的分析。尤其是在可视化方面，到后面时间紧凑，预想通过Echarts来实现大屏数据可视化，到突发意外，就以完成作业为先来处理。
跟着开发项目，查询资料学习新知识对个人还是有一定提升。但是做好风险管理以及预留开发时间方面，这个要吸取经验。
