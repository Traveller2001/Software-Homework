
# 一、PSP表格
### 在开始实现程序之前，在附录提供的PSP表格记录下你估计将在程序的各个模块的开发上耗费的时间。（3'）
### 在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间。（3'）
| Personal Software Process Stages         | 预估耗时 （分钟） | 实际耗时 （分钟） |
| ---------------------------------------- | ----------------- | ----------------- |
| ~计划                                     |               |               |
| ·估计这个任务需要多少时间                 | 15                 | 20                 |
| ~开发                                     |                 |               |
| ·需求分析 (包括学习新技术)                | 2500              | 1800              |
| · 生成设计文档                           | 30                | 30                |
| · 设计复审                               | 30                 | 30                 |
| · 代码规范 (为目前的开发制定合适的规范) |         30         |       20          |
| ·具体设计                                 | 120                | 100                |
| · 具体编码                               | 720              | 840              |
| · 代码复审                               | 60                | 50                |
| ·测试（自我测试，修改代码，提交修改）    | 480              | 420              |
| ~报告                                     |                 |                |
| · 测试报告                               | 30                | 30                |
| ·计算工作量                               | 15                | 20                 |
| ·事后总结, 并提出过程改进 计划            | 180                |      240           |
| ~合计                                     |        4210      |    3660           |

# 二、任务要求的实现
### [1.]项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。（5'）
•	项目设计
	使用python爬取数据再通过os库将得到的数据保存本地文档
	读取本地文档中的数据利用正则表达式处理筛选数据并导入mysql
	通过navicat连接数据库将处理好的数据写入Excel表格
	通过flask编写后端接口，将数据库中的数据处理统计后返回前端
	前端利用vue+echarts+datav实现数据可视化
•	技术栈
	Pymysql
	xpath正则表达式
	Scrapy
	Flask
	Vue
	Datav
	Echarts

### [2.]爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。（20'）


爬虫技术：python的scrapy库进行数据的爬取。Scrapy是适用于Python的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据，只需要实现少量的代码，就能够快速的抓取。
爬取工具：PyCharm
爬虫环境：python3.6
爬取网站：卫健委疫情通报http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml


业务逻辑：首先通过分析卫健委疫情通报和具体每日疫情详细的网站结构，以及分析网站翻页后的url的变化，得到网站格式是普通的html格式。其次，利用scrapy框架开始编写爬虫脚本，利用正则表达式从爬虫获取到的响应数据中进一步提取出我们需要的某些特定数据，最后将数据保存在数据库中。

#### Scrapy的业务逻辑：
1.引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。
2.引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。3.引擎向调度器请求下一个要爬取的URL。
4.调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。
5.—旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。
6.引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。7.Spider处理Response并返回爬取到的ltem及(跟进的)新的Request给引擎。
8.引擎将(Spider返回的)爬取到的ltem给ltem Pipeline，将(Spider返回的)Request给调度器。
9.(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。


#### 爬虫数据库设计：
DataMainland：存放中国大陆的每日确诊人数和无症状人数以及其他详细信息


| 字段             | 类型    | 长度 | 主键 | 注释           |
| ---------------- | ------- | ---- | ---- | -------------- |
| id               | int     | 11   | 1    | Id             |
| title            | varchar | 255  |      | 标题           |
| confirmed_detail | varchar | 1000 |      | 确诊人数详情   |
| carrier_detail   | varchar | 1000 |      | 无症状人数详情 |
| confirmed        | varchar | 255  |      | 确诊人数       |
| carrier          | varchar | 255  |      | 无症状人数     |
| url              | varchar | 255  |      | url            |
| pub_date         | varchar | 255  |      | 日期           |
| region           | varchar | 255  |      | 地区           |

DataProvince：每日省份确诊及无症状人数情况

| 字段      | 类型    | 长度 | 主键 | 注释       |
| --------- | ------- | ---- | ---- | ---------- |
| Id        | int     | 11   | 1    | id         |
| province  | varchar | 255  |      | 省份       |
| confirmed | varchar | 255  |      | 确诊人数   |
| carrier   | varchar | 255  |      | 无症状人数 |
| pub_time  | varchar | 255  |      | 日期       |


#### 代码设计：
1、	在安装好了scrapy框架后，使用scrapy startproject wwProject创建一个名为wwProject的爬虫项目
2、	scrapy genspider gzbdspider “http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml”命令生成一个爬虫gzbdspider
3、	进入spider文件夹中的gzbdspider.py文件中进行爬虫脚本编写
 
（1）	发出url请求
由于卫健委的反爬机制，在经过多次的412错误之后，得出服务器在处理网络请求时是要验证 Cookie 的。Cookie 的值是动态变化的，一个 Cookie 的有效时长大概只有几十秒。
访问不同的网页时会更换 Cookie 。
基于上面的结果，在请求是需要附上请求头，请求头中的Cookie需要中的security_session_verify字段随着刷新都会变化，由于不是长期定时爬取卫健委疫情网站，所以一段时间响应为412的情况下，通过更换Cookie中的security_session_verify字段可以获取请求。
（2）	数据处理
 由于网站结构是以html的形式，所以通过xpath的方法来进一步获取需要的数据。

同样，通过构造请求访问详情页，同样通过xpath和正则表达式进行数据的提取。
 

最后，在pipeline中利用pymysql库将数据存储在数据库中
【通过pymysql.connect（）函数进行数据库的连接，创建一个游标对象cursor，模块会通过游标对象来执行sql语句以及获取查询结果，在本文中执行的是insert语句】
 




### [3.]数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（6'）
还在学习
### [4.]每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（6'）
还在学习
### [5.]数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。（15'）
使用vue，datav以及echarts设计数据可视化平台前端界面，后端使用flask来进行统计数据的展示。
![在这里插入图片描述](https://img-blog.csdnimg.cn/e9e484fb7ce1426c88633e7893236f3b.jpeg)

#### 1.今日疫情总览模块
首先页面左上角为今日疫情统计组件，为了统计今日确诊人数，无症状感染人数以及总确诊人数。
前端首先想到在vue中使用datav组件来构建基本图形显示框架来以用来进行数据可视化，再使用axios与后端进行数据交互，得到后端数据并显示。
后端接口使用轻量级 Web 应用框架flask来进行编写。
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/9c0684fc7f014807990d6ec01889e170.png)

#### 2.2022累计疫情总览模块
页面左中部分为2022累计疫情总览组件，为了统计2022年至今的确诊人数，无症状感染人数以及总人数。
想法与上诉组件类似，不过这次使用前端是在vue中使用echarts组件来构建基本图形显示框架来以用来进行数据可视化，因为在图标模板中对于该组件echarts提供的更符合我们的需要（echarts与datav各有千秋只哪个组件的模板符合现在使用需求我们就使用哪个），再使用axios与后端进行数据交互，得到后端数据并显示。
后端接口其实相差不大，接口依然使用flask来进行编写。 
![在这里插入图片描述](https://img-blog.csdnimg.cn/914f8f109f9d42c7870e686691d02f0d.png)

 
#### 3.今日各省疫情模块
页面左下部分为今日各省疫情组件，为了显示各省的感染人数（数据为：时间+感染人数+省份）
想法与上诉组件类似，不过这次使用直接在vue中使用基本标签编写即可，因为是显示信息为文本所以直接使用div标签加span划分即可，因数据量可能会偏大所以采用轮询循环的方式使得数据可以滚动显示，再使用axios与后端进行数据交互，得到后端数据并显示。
后端接口其实相差不大，接口依然使用flask来进行编写。 
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/c341208a60de403da2be2892f874e637.png)

#### 4.疫情分布图模块
页面中部分为疫情分布图组件，为了更加直观的显示各省信息以及各省的感染人数。
前端是在vue中使用echarts组件来构建基本图形显示框架来以用来进行数据可视化，再使用axios与后端进行数据交互，得到后端数据并显示，地图组件比较麻烦要先再网上搜索相应的地图json文件，再通过访问url的形式动态展示出来，再在地图上嵌入中点省会城市以及相应感染人数，且不同等级的感染人数用不同颜色显示，具体操作为动态绑定颜色字段，后根据后端返回的感染人数数量进行判断，来显示不同的颜色，且添加轮询函数来判断鼠标是否划过地图，若在地图特定位置则显示感染数量信息，所有数据使用axios与后端进行数据交互，得到后端数据并显示。
后端接口也相差不大，接口依然使用flask来进行编写。 
![在这里插入图片描述](https://img-blog.csdnimg.cn/f050b505c7b6467e9b5a65a3259b1299.png)


#### 5.全国疫情曲线模块
首先页面右上角为全国疫情曲线组件，为了以坐标轴的形式更直观的显示，半年来每个月的今日确诊人数以及无症状感染人数。
前端想到在vue中使用echarts组件来构建基本图形显示框架来以用来进行数据可视化，再使用axios与后端进行数据交互，通过后端数据建立x与y轴，来画出图表，在定义轮询函数来判断鼠标是否划过该组件的特定位置，如果滑到该位置则显示该月的具体感染信息。
后端接口使用flask来进行编写。 
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/ab6123f22e4d4fdf993e01d94eebb7e5.png)

#### 6.当日疫情排名（TOP8）模块
首先页面右中部分为当日疫情排名（TOP8）组件，为了当日确诊人数+无症状感染人数最多的8个省份排序并把信息展示出来。
前端想到在vue中使用datav组件来构建基本图形显示框架来以用来进行数据可视化，再使用axios与后端进行数据交互，得到后端数据并显示。
后端接口使用flask来进行编写。
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/fc265d1acd77416c8024dd1e53e1b293.png)

#### 7.疫情统计图模块
页面右下部分为今日各省疫情览组件，为了显示各省的感染人数以及警告信息（数据为：时间+确诊人数+无症状感染者人数+省份+警告信息）
想法与第三个模块今日各省疫情组件类似，直接在vue中使用基本标签编写即可，因为是显示信息为文本所以直接使用div标签加span划分即可，因数据量可能会偏大所以采用轮询循环的方式使得数据可以滚动显示，再使用axios与后端进行数据交互，得到后端数据并显示。
后端接口其实也相差不大，接口依然使用flask来进行编写。
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/b90c889c7bf84dc6b03de9e71e663d33.png)


# 三、心得体会
### 在这儿写下你完成本次作业的心得体会，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~（10'）
我深刻体会到这门课程并不是轻而易举就可以学好的，**学习这门课程不仅需要细心严谨的态度，还需要充分发挥我们的想象力**，让**理论和实践充分的结合**在一起，才能达到事半功倍的效果。细节决定成败这句话在程序设计中再合适不过了。
本次编程作业主要是针对我们对项目流程不熟悉和对整体项目的把握不清楚，学习数据库的设计和表的建设以及表与表之间的联系，还有一些代码的编写而布置的，这些都是我们所不熟悉的也是我们最薄弱的部分。通过这段时间的学习和编程，虽然这次作业总的时间不长，但是**总体上收获挺大**的。当然开始学习后也是和想象中一样**非常不顺利**，开始的学习让我感到**学习任务异常的艰巨**，因为学习中我遇到了很多以前未曾遇到的难点，有时后也难免会失去耐心，但是，**通过老师同学朋友的指导以及自己的学习练习，我顺利的化解了一道道的障碍，克服了一道道难关**。另外一点,也是在实训中必不可少的部分，就是**同学之间的互相帮助**。所谓”当局者迷，旁观者清”，有些东西感觉自己做的是时候明明没什么错误，偏偏程序运行时就是有错误，让其他同学帮忙看了一下，发现其实是个很小的错误。所以说，相互帮助是很重要的一点，这在以后的工作或生活中也是很关键的。
**"世上没有完美的工具，只有针对特定目标的优质工具！ ”数据可视化是一种广泛运用于不同子领域（例如科学、商业和新闻业）的沟通...**
“理论与实践相结合的桥梁”。通过这周的实训和学习，我知道了此次编程练习的目的，也清**楚目前自己的不足，那就是缺乏相应的知识与经验，对所学的专业知识不能够很好地运用于实践操作**。正所谓“百闻不如一见”，经过这次自身的切身实践，我才深切地理会到了“走出课堂，投身实践”的必要性。平时，我们只能在课堂上与老师一起纸上谈兵，思维的认识基本上只是局限于课本的范围之内，也许就是这个原因就导致我们对专业知识认识的片面性，使得我们只知所以然，而不知其之所以然!限制了我们网络知识水平的提高。但是课本上所学到的理论知识是为我们的实际操作指明了方向、提供相应的方法，**真正的职业技巧是要我们从以后的实际工作中慢慢汲取的**。而针对实际操作中遇到的一些特殊的问题，我们不能拘泥于课本，不可纯粹地“以本为本”，**要提高自己的动手能力**。

