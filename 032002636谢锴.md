(1.1)在Github仓库中新建一个学号为名的文件夹，同时在博客正文首行给出作业Github链接，并登录 软工在线平台完善信息。（2'）

[GitHub作业链接](https://github.com/codesheepXK/032002636)：https://github.com/codesheepXK/032002636

# 一、PSP表格

(2.1)在开始实现程序之前，在附录提供的PSP表格记录下你估计将在程序的各个模块的开发上耗费 的时间。（3'）

(2.2)在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间。 （3'）

| PSP2.1                                | Personal Software Process Stages       | 预估耗时 （分钟） | 实际耗时 （分钟） |
| :------------------------------------ | -------------------------------------- | ----------------- | ----------------- |
| Planning                              | 计划                                   | 30                | 48                |
| Estimate                              | 估计这个任务需要多少时间               | 15                | 22                |
| Development                           | 开发                                   | 1200              | 1365              |
| Analysis                              | 需求分析 (包括学习新技术)              | 30                | 42                |
| Design Spec                           | 生成设计文档                           | 30                | 45                |
| Design Review                         | 设计复审                               | 30                | 48                |
| Coding Standard                       | 代码规范 (为目前的开发制 定合适的规范) | 20                | 30                |
| Design                                | 具体设计                               | 120               | 180               |
| Coding                                | 具体编码                               | 720               | 1260              |
| Code Review                           | 代码复审                               | 180               | 240               |
| Test                                  | 测试（自我测试，修改代码，提交修改）   | 180               | 240               |
| Reporting                             | 报告                                   | 60                | 75                |
| Test Report                           | 测试报告                               | 25                | 35                |
| Size Measurement                      | 计算工作量                             | 20                | 30                |
| Postmortem & Process Improvement Plan | 事后总结, 并提出过程改进计划           | 90                | 105               |
|                                       | 合计                                   | 2750              | 3765              |



# 二、任务要求的实现

## (3.1)项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。 （5'） 

**这次任务总共被我分成技术学习、编写爬虫、处理数据、数据可视化、编写博客五个环节**。

- 技术学习：通过B站教程学习python中基础requests模块Selenium爬虫方法，根据博客园和CSDN学习Pyppeteer方法
- 编写爬虫：通过B站教程编写爬虫，遇到问题通过CSDN博客了解原因并解决
- 处理数据：通过B站教程学习正则表达式和Xpath提取处理数据，通过json的dump方法将数据转化为json格式，最后将数据通过CSDN博客学习的pandas模块read_json()、to_excel()方法将数据存储为excel模式
- 数据可视化：通过CSDN博客以及官方文档学习pyecharts制作疫情数据地图
- 编写博客：通过typora软件编写作业博客

**本次任务所使用的技术栈**：

- python

- requests
- selenium 
- pyppeteer 
- pyecharts (绘制图例)
- BeautifulSoup  (提取页面内容)
- xpath (提取页面内容)
- re (正则表达式)
- pandas (模块转换数据格式)



## (3.2)爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数， 他们之间的关系），并对关键的函数或算法进行说明。（20'）

**爬虫主要函数代码**：

```python
#配置浏览器驱动以及动作
def pyppteer_fetchUrl(url):
    #-headless 无头浏览器模式
    #-dumpio 解决chromium浏览器多开页面卡死问题
    #-autoClose 脚本完成时自动关闭浏览器进程
    browser = await launch({'headless': False,'dumpio':True, 'autoClose':True})
    page = await browser.newPage()
    await page.goto(url)
    await asyncio.wait([page.waitForNavigation()])
    str =await page.content()
    #关闭驱动
    await browser.close()
    return str

#获取对应url的html结构
def fetchUrl(url):
    return asyncio.get_event_loop().run_until_complete(pyppteer_fetchUrl(url))

#通过 getPageUrl 函数构造每一页的 URL 链接，head为起始页码，tail为结束页码
def getPageUrl(head,tail):
    #如果只爬取一页
    if head==tail:
        if(head!=1):
            url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_'+ str(head) +'.shtml'
        else:
            url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml'
        yield url 
    #爬取多页
    for page in range(head,tail):
        if page == 1:
            yield 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml'
        else:
            url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_'+ str(page) +'.shtml'
            yield url

#通过 getTitleUrl 函数，获取某一页的文章列表中的每一篇文章的标题，链接。
def getTitleUrl(html):
    #创建BeautifulSoup对象
    bsobj = BeautifulSoup(html,'html.parser')
    #找到存放二级目录的li
    titleList = bsobj.find('div', attrs={"class":"list"}).ul.find_all("li")
    for item in titleList:
        #补全链接
        link = "http://www.nhc.gov.cn" + item.a["href"]
        #获取标题
        title = item.a["title"]
        #返回数据
        yield title, link

#返回文章具体内容
def getContent(html):      
    bsObj = BeautifulSoup(html,'html.parser')
    Plist = bsObj.find('div', attrs={"id":"xw_box"}).find_all("p")
    str = ""
    if Plist:
        for item in Plist:
            str += item.text
        return str

    return "爬取失败！"

#存储文件
def saveFile(path, filename, content):
    #如果没有文件夹先建立
    if not os.path.exists(path):
        os.makedirs(path)  
    # 保存文件
    with open(path + filename + ".txt", 'w', encoding='utf-8') as f:
        f.write(content)
```

**爬虫关键函数说明**：

1. getTitleUrl(html)

- 首先通过利用BeautifulSoup生成html的bsObj对象用于寻找页面结构

- 分析页面结构，找到每日疫情数据文章所在位置是class为list的div盒子下的ul的每个li

![](https://img2022.cnblogs.com/blog/2971681/202209/2971681-20220920202027311-1751511535.png)

- 得到li后分析内容href，title，以及span下时间信息date的是我们所需要的提取出来
- 最后返回我们所提取的href，title，date



2. getContent(html)

- 同上先通过利用BeautifulSoup生成html的bsObj对象用于寻找页面结构
- 分析页面结构，找出文章内容所在结构是id为xw_box的div盒子下的每个p标签里的内容

![](https://img2022.cnblogs.com/blog/2971681/202209/2971681-20220920202027345-923034759.png)

- 得到所有p标签后提取文章内容并将其组合成一个字符串str返回



**爬虫运行流程**：

- 步骤一：首先通过输入的所需爬取的疫情通报页码运用函数getPageUrl获取对应的URL
- 步骤二：通过fetchUrl函数获取每个URL所对应的html结构
- 步骤三：运用getTitleUrl获取文章列表中的每一篇文章的标题，链接，和发布日期。
- 步骤四：进入所在文章界面后运用getContent函数进行文章内容获取
- 步骤五：获取文章内容后通过saveFile函数将其以txt格式保存在本地

- 循环步骤四和步骤五直到获取完所有文章内容



**数据处理主要函数代码**：

```python
#将例如：北京12例的字符串分开为{"北京"："12"}返回
def divideData(province):
    provinceRule=re.compile(r'(.*?)([0-9]+.*)')            
    provinceName=provinceRule.findall(province)[0][0] #省份名称
    provinceNum=provinceRule.findall(province)[0][1]  #对应数量
    provinceItem={provinceName:provinceNum} 
    return provinceItem

#循环获取各省份疫情数据
def getProvinceData(str):
    provinceList=init()
    index=0
    while index < provincesLength:
        provinceName=provinceNames[index]
        flag=str.find(provinceName)
        #说明该省有疫情
        if(flag!=-1):
            head=flag
            item=""
            #获取省份总数据
            while 1:
                item=item+str[head]
                head=head+1
                if str[head]=='例' :
                    break 
            provinceItem=divideData(item)   
            provinceList.update(provinceItem)
        index=index+1
    return provinceList

#用于特例比如：本土疫情1例（在广东）只有一个省份发生疫情的确诊病例数据提取
def confirmDataOnly(str,nameText):
    #获取病例数
    onlyRule=re.compile(r'新增确诊病例.*?本土.*?(\d+?)例',re.M) #正则
    provinceNum=onlyRule.findall(str)[0]

    #数据初始化
    provinceList=init()
    #获取有疫情的省份名称
    provinceName=isProvinceHave(nameText)
    provinceItem={provinceName:provinceNum} 
    provinceList.update(provinceItem)
    return provinceList

#用于特例比如：本土疫情1例（在广东）只有一个省份发生疫情的无症状感染者数据提取
def noSymptomDataOnly(str,nameText):
     #获取病例数
    onlyRule=re.compile(r'新增无症状感染者.*?本土.*?(\d+?)例',re.M) #正则
    provinceNum=onlyRule.findall(str)[0]

    provinceList=init()
    #获取省份名称
    provinceName=isProvinceHave(nameText)
    provinceItem={provinceName:provinceNum} 
    provinceList.update(provinceItem)
    return provinceList

#将传入数据转换为JSON格式
def transformJson(data): 
    data=json.dumps(data,ensure_ascii=False)
    return data

#获取传入文章数据中各省份的疫情数据函数
def getData(str,date):
    confirmData={}  #存储确诊数据
    noSymptomData={} #存储无状态感染者数据
    DataSum={}
    #通过正则匹配出对应本土新增确诊病例
    confirmRule=re.compile(r'新增确诊病例.*?本土.*?（(.*?)）',re.M)
    confirmText=confirmRule.findall(str)
    #如果没有发送疫情那么返回值confirmText为[]
    if(confirmText==[]):
        confirmData=init()
    #说明有返回值存在疫情数据
    else:
        confirmStr=confirmText[0]
        #判断是否比如：本土疫情1例（在广东）只有一个省份发生疫情的特例
        if bool(re.search(r'\d', confirmStr))==False:
            confirmData=confirmDataOnly(str,confirmStr)
        else:
            confirmData=getProvinceData(confirmStr)

    #通过正则匹配出对应本土新增无症状感染者，格式同上
    noSymptomRule=re.compile(r'新增无症状感染者.*?本土.*?（(.*?)）',re.M)
    noSymptomText=noSymptomRule.findall(str)
    if noSymptomText==[]:
        noSymptomData=init()
    else:
        noSymptomStr=noSymptomText[0]
        if bool(re.search(r'\d', noSymptomStr))==False:
            noSymptomData=noSymptomDataOnly(str,noSymptomStr)
        else:
            noSymptomData=getProvinceData(noSymptomStr)

    #汇总数据
    DataSum.update({"确诊病例":confirmData})
    DataSum.update({"无症状感染者":noSymptomData})
    #将汇总数据转换为JSON格式
    DataSum=transformJson(DataSum)
    #利用pandas模块将每日汇总数据转变为EXCEL
    df=pd.read_json(DataSum,orient='index',encoding='utf-8')
    df.to_excel(BasePathExcel+date+'.xlsx')
```

**数据处理关键函数说明：**

1.getProvinceData(str)：

- 先初始化provinceList，即每个省份数据为0
- 循环判断str中是否含有省份名称（北京、天津...）如果含有就获取对应的字符串（例如：北京12例）传入divideData函数将字符串分开为{"北京"："12"}返回，再存储进provinceList
- 返回provinceList（每个省份的疫情数据）

2.getData(str,date)：

- 通过正则匹配出存储着本土各省新增确诊病例以及新增无症状感染者的字符串
- 第一种情况如果不存在匹配的字符串说明今日没有疫情数据，这时候数据相当于各省全为0
- 第二种情况如果字符串中没有数字说明只有疫情只发生在一个省，这时候调用confirmDataOnly函数或者noSymptomDataOnly函数
- 第三种情况就是大部分情况至少有两个及两个以上省有疫情数据，这时候调用getProvinceData函数获取数据
- 获取数据完后将数据存储为JSON格式的DataSum
- 利用pandas中的read_json读取DataSum，再利用to_excel函数生成相应的excel文件

**数据处理运行流程**：

- 步骤一：循环提取所爬取的文章内容同时转化为字符串str
- 步骤二：将str传入getData函数分析数据
- 步骤三：通过正则匹配出存储着本土各省新增确诊病例以及新增无症状感染者的字符串
- 步骤四：根据不同的情况将匹配的字符串传入getProvinceData函数或者confirmDataOnly或者noSymptomDataOnly进行数据提取
- 步骤五：将获取的各省新增确诊病例以及新增无症状感染者数据以JSON格式转为EXCEL存储



## (3.3)数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如通过VS2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（6'） 

```
cProfile.run('dataProcess()','restats')

#在test.py中使用
import pstats
from pstats import SortKey

# 加载保存到restats文件中的性能数据
p = pstats.Stats('restats')

# 打印耗时排名前十的函数
p.sort_stats(SortKey.CUMULATIVE).print_stats(10)
```

![image-20220920153943285](https://img2022.cnblogs.com/blog/2971681/202209/2971681-20220920202031855-1559854053.png)

耗时最长函数：

```
def getData(str,txtName):
    confirmData={}  #存储确诊数据
    noSymptomData={} #存储无状态感染者数据
    DataSum={}
    #通过正则匹配出对应本土新增确诊病例
    confirmRule=re.compile(r'新增确诊病例.*?本土.*?（(.*?)）',re.M)
    confirmText=confirmRule.findall(str)
    #如果没有发送疫情那么返回值confirmText为[]
    if(confirmText==[]):
        confirmData=init()
    #说明有返回值存在疫情数据
    else:
        confirmStr=confirmText[0]
        #判断是否比如：本土疫情1例（在广东）只有一个省份发生疫情的特例
        if bool(re.search(r'\d', confirmStr))==False:
            confirmData=confirmDataOnly(str,confirmStr)
        else:
            confirmData=getProvinceData(confirmStr)

    #通过正则匹配出对应本土新增无症状感染者，格式同上
    noSymptomRule=re.compile(r'新增无症状感染者.*?本土.*?（(.*?)）',re.M)
    #如果没有发送疫情那么返回值noSymptomText为[]
    noSymptomText=noSymptomRule.findall(str)
    #说明有返回值存在疫情数据
    if noSymptomText==[]:
        noSymptomData=init()
    else:
        noSymptomStr=noSymptomText[0]
        #判断特例
        if bool(re.search(r'\d', noSymptomStr))==False:
            noSymptomData=noSymptomDataOnly(str,noSymptomStr)
        else:
            noSymptomData=getProvinceData(noSymptomStr)

    #汇总数据
    DataSum.update({"确诊病例":confirmData})
    DataSum.update({"无症状感染者":noSymptomData})
    #将汇总数据转换为JSON格式
    DataSum=transformJson(DataSum)
    #利用pandas模块将每日汇总数据转变为EXCEL
    df=pd.read_json(DataSum,orient='index',encoding='utf-8')
    df.to_excel(BasePathExcel+txtName+'.xlsx')
```

改进思路：

目前处理数据循环次数过多，可以选取更高效的数据处理方式，减少循环次数

## (3.4)每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（6'） 

实现思路：

- 当某个省连续一周以上没有出现本土病例突然出现本土病例时特别记录
- 当某个省确诊病例或者无症状感染者超过某一个阈值（boundary）时特别记录
- 通过数据处理后构建数据集合来运用LSTM模型预测疫情趋势，挖取热点

优缺点：

- 优点：算法简单实现容易
- 缺点：不适合处理特例数据，逻辑单一

改进方案：

- 采取更加合适的模型以及特例数据来加强智能训练

## (3.5)数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。（15'）

数据可视化大屏界面截图展示：

![](https://img2022.cnblogs.com/blog/2971681/202209/2971681-20220920202028466-901305316.png)

设计思路：

- 首先在左边显示出目前为止的各省目前确诊病例总数的地图（pyecharts的map模块）
- 在右上方展示出各省每日新增确诊病例以及无症状感染者的条形图（pyecharts的Bar模块）
- 在右下方展示出具体有确诊病例的各省数据环形图（pyecharts的Pie模块）

设计代码：

- 中国地图

```
def makeMap() -> Map:  
    c = (
        Map(init_opts=opts.InitOpts(theme=ThemeType.DARK))
        .add("该省确诊人数",province_data, "china",is_map_symbol_show=False)
        .set_global_opts(
            title_opts=opts.TitleOpts(
                title="新型冠状病毒全国疫情地图",
            ),   
            visualmap_opts=opts.VisualMapOpts(
                is_show=True,  
                min_=0,  # 刻度最小值
                max_=10000
            )
        )
    )
    return c
```

- 条形图

```
def makeBar() -> Bar:
    c = (
        
        Bar(init_opts=opts.InitOpts(theme=ThemeType.DARK))
        .add_xaxis(provinceNames)
        .add_yaxis("确诊病例", confirmData)
        .add_yaxis("无症状感染者", noSymptomData)
        .set_global_opts(
            title_opts=opts.TitleOpts(title=date+"本土各省新增确诊病例"),
            datazoom_opts=[opts.DataZoomOpts()],
        )
    )
    return c
```

- 环形图

```
def makePie(dataPair,name) -> Pie:
    c = (
        Pie(init_opts=opts.InitOpts(theme=ThemeType.DARK)) 
        .add(
            name,
            data_pair= dataPair,
            radius=["50%", "70%"],
            center=["50%", "50%"],
            label_opts=opts.LabelOpts(is_show=False),
        )
        .set_global_opts(
            title_opts=opts.TitleOpts(title=name),
            legend_opts=opts.LegendOpts(pos_left="right", orient="vertical")
        )
        .set_series_opts(
            tooltip_opts=opts.TooltipOpts(
                trigger="item", formatter="{a} <br/>{b}: {c} ({d}%)"
            ),
            label_opts=opts.LabelOpts(formatter="{b}: {c}")
        )

    )
    return c
```



# 三、心得体会

## (4.1)在这儿写下你完成本次作业的心得体会，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~（10'）

- 任务总体而言，这次的个人编程任务对我而言难度较大，之前并没有接触过python语法，也没有接触过爬虫的相关技术，耗费时间精力不少，但同时也学习了python的使用，感受到了python的各种库的方便，其次学习运用了不同的爬虫方式，以及文件之间的格式转换（json，excel）和文件中内容提取，也养成了相对合理的代码规范以及注释，知晓了先做计划和分析，当然也提升了搜索信息、查询问题的能力。

- 任务过程而言，爬虫技术再三更换，从一开始学习的用requests模块发请求，再到被反爬获取数据受阻，选择转战selenium，但是selenium依旧被反爬限制，最后选择pyppeteer才相对顺利地爬取完数据，处理数据也从一开始的普通算法不断改进，运用beautifulSoup和re正则表达式以及xpath提取所需数据，每天都是处于不断优化代码，遇到问题寻找解决方案的循环中。

- 任务不足之处，首先由于学习的知识和技术时间较短，并没有系统地进行学习，因此许多知识和技术只是似懂非懂，理解不够透彻，掌握不够熟练，运用起来仍常常出错，具体实现的时候发现各类组件总是会出现在令人意外的地方，然后一次一次地查找原因、重新学习，反而降低了效率。其次是计划和时间安排不够合理，出现后期匆忙赶工的现象，导致做出来的成果并不能令人很满意，存在很多瑕疵。

  



