[GitHub链接](https://github.com/Jeong341/RE.git)

# 一、PSP表格
| Personal SoftwareProcess Stages |预计耗时（分钟）  |实际耗时（分钟）|
|--|--|--|
| 计划 | 60 | 60 |
| 估计这个任务需要多少时间|1200 |1500 |
|开发 |1000 |1200 |
| 需求分析|300 |540 |
|生成设计文档 | 0   | 0 |
|设计复审 | 0 |  0 |
| 代码规范|20 | 20|
| 具体设计|60  |  56 |
| 具体编码  |  300   |   480  |
|  代码复审  |  60  |   40  |
|    测试     |    30   |    10   |
|    报告   |    0  |    0  |
|     测试报告    |     0  |    0   |
|    计算工作量     |     30   |   30   |
|    事后总结，并提出过程改进计划    |   60   |    90   |
|   合计  |  3120  |  4026  |

# 二、任务要求的实现

## （3.1） 项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。 
这次任务我把它划分成四个部分分别为：

 - python基础语法的学习
 - 爬虫的学习以及实现
 - 数据的收集和处理的学习以及实现
 - 数据可视化以及机器学习的学习和实现
 
 对于基础语法的学习，我通过哔哩哔哩搜索相关的学习视频进行学习，后面的三个环节则从CSDN、知乎、GitHub等网站查询相关的资料与算法，学习并加以实现。
 
 使用到的技术有：selenium库、正则表达式、xlwt库、BeautifulSoup、matplotlib库、openpyxl库 等

## (3.2)爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。（20'）
对于爬虫的实现，我也把他分成几个函数部分；首先编写一个函数，作用是通过selenium库获得对应页面的html文件，方便后续使用并且减少代码冗余。然后编写一个spider函数对1到42页的一级页面进行爬取，得到子页面链接；然后每获得一个子页面链接就对其进行爬取，对子页面内容爬取的代码段写了一个getdata函数进行调用实现。将得到的内容存在列表中，最后通过savedata函数将数据存入excel表格当中。

 - 获取HTML文件
```python
def get_html(url):
    browser = webdriver.Firefox()  #调用selenium库
    browser.get(url)  #访问页面
    html = browser.page_source   #获取HTML文件
    # print(html)
    browser.close()   #关闭浏览器
    return html
```
 - 对一级页面进行爬取
 

```python
def spider():       #爬取疫情通报链接
    url = "http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml"
    html = get_html(url)    #爬取首页含有的链接
    content = BeautifulSoup(html,"html.parser")  #对HTML文件进行解析
    list = content.select('.list > ul >li > a')
    while len(list) == 0:   #若没有得到正常网页，则重复请求
        html = get_html(url)  # 爬取首页含有的链接
        content = BeautifulSoup(html, "html.parser")
        list = content.select('.list > ul >li > a')
    for i in list:
        link = "http://www.nhc.gov.cn" + i['href']
        print(link)
        getdata(link)   #每爬取一个链接，就对该链接再次进行爬取，获得需要的数据
    for i in range(2, 42):  #爬取后续页面含有的链接
        url = "http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_{}.shtml".format(i)
        html = get_html(url)
        content = BeautifulSoup(html, "html.parser")
        list = content.select('.list > ul > li > a')
        while len(list) == 0:   #若没有得到正常网页，则重复请求
            html = get_html(url)
            content = BeautifulSoup(html, "html.parser")
            list = content.select('.list > ul > li > a')
        for j in list:
            link = "http://www.nhc.gov.cn" + j['href']
            print(link)
            getdata(link)
```

 - 获取子页面内容
 

```python
def getdata(url):    #爬取每个疫情通报连接的数据
    html = get_html(url)
    content = BeautifulSoup(html, "html.parser")
    list = content.select('.list > .con')
    while len(list) == 0:  # 若没有得到正常网页，则重复请求
        html = get_html(url)
        content = BeautifulSoup(html, "html.parser")
        list = content.select('.list > .con')
    for con in list:
        news = con.get_text()
    date = re.search('(\d+.\d+.)0—24时',news).group(1) #获取日期
    # print(date)
    riqi.append(date)
    new = re.search('新增确诊病例(.+)本土病例(\d*)例',news)   #获取本土新增
    if new == None:
        bt_new.append(0)
    else:
        bt_new.append(new)
    wzz = re.search('新增无症状感染者(.+)本土(\d*)例',news).group(2)  #获取本土无症状
    # print(wzz)
    if wzz == None:
        bt_wzz.append(0)
    else:
        bt_wzz.append(wzz)
    qz = re.search('累计收到港澳台地区通报确诊病例(\d*)例',news).group(1)  #获取港澳台
    # print(qz)
    if qz == None:
        gat_qz.append(0)
    else:
        gat_qz.append(qz)
    for i in province:
        temp_1 = re.search('新增确诊病例.*?本土病例.*?' + i +'(\d+)', news)  #获取各省份新增
        if temp_1 == None:
            province_new.append(0)
        else:
            # print(temp_1.group(1))
            province_new.append(temp_1)
    for i in province:
        temp_2 = re.search('新增无症状感染者(.+){}(\d*)'.format(i), news)  #获取各省份无症状
        if temp_2 == None:
            # print(0)
            province_wzz.append(0)
        else:
            # print(temp_2.group(2))
            province_new.append(temp_2.group(2))
    print(date + '爬取成功！')
```

 - 保存数据
 

```python
def savedata():
    workbook = xlwt.Workbook(encoding='utf-8')   #创建workbook对象
    worksheet_1 = workbook.add_sheet('新增确诊')          #创建工作表
    worksheet_2 = workbook.add_sheet('新增无症状')
    worksheet_1.write(0, 0, '日期')  #处理新增确诊数据
    j=1
    for i in riqi:
        worksheet_1.write(j, 0, i)
        j+=1
    worksheet_1.write(0, 1, '本土确诊')
    j=1
    for i in bt_new:
        worksheet_1.write(j, 1, i)
        j+=1
    worksheet_1.write(0, 2, '港澳台确诊')
    j=1
    for i in gat_qz:
        worksheet_1.write(j, 2, i)
        j += 1
    j=3
    for i in province:
        worksheet_1.write(0, j, i)
        j+=1
    length = len(province_new) // 31
    k = 0
    for i in range(1,length+1):
        for j in range(3,34):
            worksheet_1.write(i,j,province_new[k])
            k = k+1
    worksheet_2.write(0, 0, '日期')     #对无症状数据处理
    j = 1
    for i in riqi:
        worksheet_2.write(j, 0, i)
        j += 1
    worksheet_2.write(0, 1, '本土无症状')
    j = 1
    for i in bt_wzz:
        worksheet_2.write(j, 1, i)
        j += 1
    worksheet_2.write(0, 2, '港澳台确诊')
    j = 1
    for i in gat_qz:
        worksheet_2.write(j, 2, i)
        j += 1
    j = 3
    for i in province:
        worksheet_2.write(0, j, i)
        j += 1
    k = 0
    for i in range(1,length+1):
        for j in range(3,34):
            worksheet_2.write(i,j,province_wzz[k])
            k = k+1
    workbook.save('疫情数据.xls')
```
## (3.3)数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（6'）
对于数据统计部分，主要使用正则表达式来收集数据，并用xlwt库将数据写入excel表格当中。对于数据统计接口，没做多少优化，主要是对于数据统计库的使用的选择有些许的纠结。
![性能图](https://img-blog.csdnimg.cn/a9c4f9917463452988c5d0587453d41c.png#pic_center)


## (3.4)每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（6'）
对于机器学习的实现，确实苦于精力有限，有点无从下手；所以我的每日热点的实现就是对查询当天的数据与前一天数据进行对比，从而得到数据的差异。这部分功能实现的缺失算是有一点遗憾。

## (3.5)数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。（15'）
![数据可视化实现](https://img-blog.csdnimg.cn/7a38bb25a40f455baafa44077050a75c.png#pic_center)
数据可视化的实现引用了matplotlib库来进行实现，主要的思路就是将excel文件中保存的数据在程序中先用列表保存。然后输入需要查询的日期，通过openpyxl库对数据进行匹配查找，找到当天的数据并保存，然后使用matploylib库函数制作图表。

# 三、心得体会
## (4.1)在这儿写下你完成本次作业的心得体会，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~（10'）
其实总结这一次的作业让我第一个想到的不是什么收获满满巴拉巴拉的，这些都是其次，最主要的是累；对于没有python基础的我确实是一直处于赶工状态，尤其是卫健委的网页爬取部分更是让我几乎把能试的方法快用了一遍。这也使我的进度在这个时候被拉下很多。这些算是一些小抱怨，其实收获也是很大。学会了python、了解了正则表达式、学会了爬虫等等，但最重要的是我对于我所要信息的这种检索能力的提升，我觉得才是最让我受益匪浅的，也许我的代码实现比很多人都差上许多，但这些都是我一点一点如燕子衔泥一般从各个网站搜集而来搭建起来的东西，其中也有一大部分东西没法使用，但还是拼成了这次作业的小小雏形。这种从什么都不懂到能够做出一些东西的过程却也是让我开心的。




