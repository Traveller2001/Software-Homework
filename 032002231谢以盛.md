代码链接：[https://github.com/EsonQtheQuuen/2022new](https://github.com/EsonQtheQuuen/2022new)
# 一、PSP表格 #

| PSP2.1 	| Personal Software Process Stages 	| 预估耗时（分钟） 	|
| ------ | ------ | ------ | 
|Planning 	|计划 	|10|
|· Estimate 	|· 估计这个任务需要多少时间 	|10 	|
|Development 	|开发 	|100 	|100|
| Analysis 	·| 需求分析 (包括学习新技术) 	|200 	|
| Coding Standard |· 代码规范 (为目前的开发制定合适的规范) 	|10 	|
| Design |· 具体设计|  20 	| 
 |Coding| 具体编码 	|200 | 
|Code Review| 代码复审 |	100 	|
| Test 	|· 测试（自我测试，修改代码，提交修改）| 	100 |	
|Reporting 	|报告| 	60 	|
 |Test Repor |	· 测试报告| 	10 	|
|Size Measurement 	|· 计算工作量 	|10 	|
| Postmortem & Process Improvement Plan|	· 事后总结, 并提出过程改进计划 	|10 	|
 合计 	||840|

| PSP2.1 	| Personal Software Process Stages 	| 实际耗时（分钟） 	|
| ------ | ------ | ------ | 
|Planning 	|计划 	|10|
|· Estimate 	|· 估计这个任务需要多少时间 	|10 	|
|Development 	|开发 	|200 	|
| Analysis 	·| 需求分析 (包括学习新技术) 	500 	|
| Coding Standard |· 代码规范 (为目前的开发制定合适的规范) 	|10 	|
| Design |· 具体设计|  20 	| 
 |Coding| 具体编码 	|300 | 
|Code Review| 代码复审 |	100 	|
| Test 	|· 测试（自我测试，修改代码，提交修改）| 	100 |	
|Reporting 	|报告| 	60 	|
 |Test Repor |	· 测试报告| 	10 	|
|Size Measurement 	|· 计算工作量 	|10 	|
| Postmortem & Process Improvement Plan|	· 事后总结, 并提出过程改进计划 	|10 	|
 合计 	||1340|

## 二、任务要求的实现

**(3.1)项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。（5'）**


共有四个主要环节：
    *爬取通报文件
		提取通报文件中需要的数据
			数据录入excel
				对数据进行可视化处理*

本次作业主要以python语言实现各个环节，并使用了以下库：

```python
import re
import cProfile
import xlwt

from pyecharts import options as opts
from pyecharts.charts import Bar
from pyecharts.charts import Map

from bs4 import BeautifulSoup

import asyncio
from  pyppeteer import  launch
from pyppeteer import launcher
```
				
**(3.2)爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。（20'）**
首先是对网页的爬取部分

```python
import asyncio
from  pyppeteer import  launch
from pyppeteer import launcher

launcher.DEFAULT_ARGS.remove("--enable-automation")

async def pyppteer_fetchUrl(url):
    browser = await launch({'headless': False,'dumpio':True, 'autoClose':True})
    page = await browser.newPage()
    await page.goto(url)
    await asyncio.wait([page.waitForNavigation()])
    str = await page.content()
    await browser.close()
    return str

def fetchUrl(url):
    return asyncio.get_event_loop().run_until_complete(pyppteer_fetchUrl(url))
#获取html
```
利用pypeteer库绕过官网反爬虫机制

分析过网页结构后使用BeautifulSoup解析提取通报网页的url后继续爬取各个页面通报文本，对文本内容进行正则匹配提取数据：

```python
def data_detail(str):#获取各个省市数据
    dict = {
        "西藏": 0, "澳门": 0, "青海": 0, "台湾": 0, "香港": 0, "贵州": 0, "吉林": 0, "新疆": 0, "宁夏": 0, "内蒙古": 0,
        "甘肃": 0,
        "天津": 0, "山西": 0, "辽宁": 0, "黑龙江": 0, "海南": 0, "河北": 0, "陕西": 0, "云南": 0, "广西": 0, "福建": 0,
        "上海": 0,
        "北京": 0, "江苏": 0, "四川": 0, "山东": 0, "江西": 0, "重庆": 0, "安徽": 0, "湖南": 0, "河南": 0, "广东": 0,
        "浙江": 0, "湖北": 0
    }
    provi= ["(西藏)(.*?)例", "(澳门)(.*?)例", "(青海)(.*?)例", "(台湾)(.*?)例", "(香港)(.*?)例", "(贵州)(.*?)例",
                "(吉林)(.*?)例", "(新疆)(.*?)例", "(宁夏)(.*?)例", "(内蒙古)(.*?)例",
                "(甘肃)(.*?)例", "(天津)(.*?)例", "(山西)(.*?)例", "(辽宁)(.*?)例", "(黑龙江)(.*?)例", "(海南)(.*?)例",
                "(河北)(.*?)例", "(陕西)(.*?)例", "(云南)(.*?)例", "(广西)(.*?)例",
                "(福建)(.*?)例", "(上海)(.*?)例", "(北京)(.*?)例", "(江苏)(.*?)例", "(四川)(.*?)例", "(山东)(.*?)例",
                "(江西)(.*?)例", "(重庆)(.*?)例", "(安徽)(.*?)例", "(湖南)(.*?)例",
                "(河南)(.*?)例", "(广东)(.*?)例", "(浙江)(.*?)例", "(湖北)(.*?)例"
                ]
    for item in provi:
        num=re.search(item,str)
        if num:
            dict[num.group(1)]=int(num.group(2))

    return dict

def data_get(mes):#处理文本中的数据

    yq_data={}

    cfm_data=re.search("31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例.*?例.*?本土(.*?)例（(.*?)）",mes)
    asy_data=re.search("31个省（自治区、直辖市）和新疆生产建设兵团报告新增无症状感染者.*?例.*?本土(.*?)例（(.*?)）",mes)

    yq_data['cfm_dict']=data_detail(cfm_data.group(2))#返回数据字典
    yq_data['asy_dict']=data_detail(asy_data.group(2))

    yq_data['cfm']=re.search('(\d+)',cfm_data.group(1)).group(1)
    yq_data['asy']=re.search('(\d+)',asy_data.group(1)).group(1)

    return yq_data
```
接下来应用xlwt、pyechartt库进行对excel生成以及数据可视化



**(3.3)数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（6'）**


**(3.4)每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（6'）**

**(3.5)数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。（15'）**
本次数据可视化使用百度开源python库pyechart，在全国地图上显示热力图![在这里插入图片描述](https://img-blog.csdnimg.cn/27fd46990ed54e178b6bb149286b8d5e.png)




## 三、心得体会

**(4.1)在这儿写下你完成本次作业的心得体会，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~（10'）**

本次实践是第一次进行爬虫，与自己的预期难度差距很大，上来卫健委的反爬机制就给了我很大困难，但是在在这几日的学习中，我觉得收获很大，自己的知识能力有了许多提升。


