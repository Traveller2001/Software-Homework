# 一.psp表格
| PSP2.1     |Personal Software Process Stages     | 预估耗时（分钟）     |  实际耗时（分钟）     |
| -------- | -------- | -------- | -------- |
| Planning | 计划| 30 | 60 |
| · Estimate  | · 估计这个任务需要多少时间 | 5 | 5 | 
| Development |开发| 100 | 120 |
 |· Analysis  |· 需求分析 (包括学习新技术) | 300 | 500 |
 | · Design Spec |·生成设计文档| 30 | 60 |
 | · Design Review| · 设计复审| 20 | 20 |
 | · Coding Standard  | · 代码规范(为目前的开发制定合适的规范 ）| 20 | 20 |
 | · Design  | · 具体设计| 100 | 300 |
 | · Coding  | · 具体编码| 150 | 500 |
 |· Code Review  |  代码复审| 30 | 30 |
 | · Test| · 测试（自我测试，修改代码，提交修改）| 30 | 60 |
 | Reporting | 报告| 30 | 30 |
 | · Test Repor | · 测试报告| 20 | 20 |
 | · Size Measurement  |  计算工作量| 25| 30 |
 | · Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划| 30 | 30 |
 | |· 合计| 920 | 1785 |
 
# 二、任务要求的实现
### 1.项目设计与技术栈
第一步：将题目拆分为爬取数据、导出数据以及实现可视化三个部分。
第二步：查找资料，利用pyppeteer反反爬成功，将每个页面的标题作为txt文件的标题，将每个页面的内容存入txt文件中。
第三步：导入xlwt库，建立一个sheet并将数据导入sheet。
第四步：导入re库，利用正则表达式将所需数据提取出来，再利用百度开源的pyecharts实现数据的可视化。

### 2.爬虫与数据处理
1.观察分析网站的 URL 规则，发现除第一页之外，其他页码的 URL 都有显著的规律——URL 中的数字对应了当前的页码，即网页通过 URL 来实现翻页。

2.将 pyppeteer 的操作封装成 fetchUrl 函数，用于获取网页源码。

3.通过 getPageUrl 函数构造每一页的 URL 链接。

4.通过 getTitleUrl 函数，获取某一页的文章列表中的每一篇文章的标题，链接，和发布日期。

5.通过 getContent 函数，获取某一篇文章的正文内容。

6.通过 saveFile 函数，将爬取到的数据保存在本地的 txt 文档里。

### 3.数据统计接口部分的性能改进
改进思路：可以将正则表达式与导出数据相结合，可以更加有效率。（我不会55555555）

### 4.每日热点的实现思路
将热点定义为某省清零或患者增数超过50，再将处理好的数据可视化处理。

### 5.数据可视化界面的展示
先将爬好的文本数据处理好，因为我爬取的是整个网页的文本数据，所以要实现可视化还要先用正则表达式将数据处理好，再利用百度开源的pyecharts将数据导入即可得可视化的数据。
此开源的可视化图形非常的方便而且操作简单，它直接可以给我一个中国地图，可以根据各个省患者人数的不同将这个省的区域设置成不同的颜色，将鼠标放入想应省的区域也可以看到该省的疫情信息。

# 三.心得体会
这一次因为中途还有竞赛，所有做得也是十分艰难。感谢老师把作业推迟！！！不杀之恩！！！这次作业可以说是从0开始学，我之前只学了一点python的基础，在一开始真的不知道该怎么走。还是网上的资料很多，我接触到了pyppeteet、re还有很多其他神奇的库，多谢这些库的开发者救我狗命，在这里鸣谢再次鸣谢pyecharts。虽然很艰难走了很多弯路，但还好最终作品还是出来了，虽然很简陋，但我就是，也没有嫌弃它的资本，这次爬虫和可视化的实现也算是侥幸，刚刚好有很多很好的资料以及开源的资源让我能实现我能力之外的事情。这次也是感受到了用python做东西的魅力了吧，真的就是前人栽树后人乘凉，感谢在这条路上开路的前辈，以后还请多努力努力，拜托了T_T。


