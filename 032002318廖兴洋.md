# 一、PSP表格
| PSP2.1   |      Personal Software Process Stages     | 预估耗时（h） |  实际耗时（h）  |
|----------|:-------------:|:------:| ------:|
| Planing |  计划 | 0.2 | 0.2 |
| ·Estimate |  估计这个任务需要多少时间 | 33 |43|
| Development |开发 |33|43|
|  ·Analysis  |需求分析 (包括学习新技术)|10|15|
|·Design Spec| 生成设计文档|0|0|
|·Design Review|设计复审|0|0|
|·Coding Standard |代码规范 (为目前的开发制定合适的规范)|1|1|
|·Design|具体设计|0|0|
|·Coding|具体编码|10|10|
|·Code Review|代码复审|2|2|
|·Test|测试（自我测试，修改代码，提交修改）|10|15|
|Reporting|报告|0.2|0.2|
|·Test Repor|测试报告|0.1|0.1|
| ·Size Measurement |计算工作量|0.1|0.1|
| ·Postmortem & Process Improvement Plan | 事后总结, 并提出过程改进计划|0.1|0.1|
||合计|33.4|43.4|



# 二、任务要求的实现

## 1、项目设计与技术栈

​	首先对目标页面进行分析。在卫健委的“[疫情通报](http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml)”网站中，每日的疫情通报网页的链接都被罗列在这里。打开一个[子网页](http://www.nhc.gov.cn/xcs/yqtb/202209/b607d59ecc594874b90728c056176376.shtml)可以看到，本次项目的数据来源，就是该网页中的疫情新闻文本。项目要求对每日的新闻进行爬取与处理，析出所需数据并整理。考虑到（假设三个月）新闻文本较为庞大，（同时也是因为第一次爬虫的原因）这里不采取边爬边分析的处理方式，而是通过将文本下载到本地再逐条分析获取数据的方法。

​	项目环节因此被分解为以下步骤：

- 爬取”[疫情通报](http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml)“中每一日新闻所引用的超链接（sub_add_sch.py），并下载链接地址到本地（subadd.txt）
- 通过每一条超链接获取当日新闻页面（sub_page_dl.py），下载文本到本地("subpagetext/xxxxxxxx.txt")
- 通过对每日新闻文本的处理获取所需数据（datasearch.py），数据保存为表格（PrvData.xls）
- 通过对数据汇总整理（visual.py），尝试得出可视化效果（render.html)

## 2、爬虫与数据处理

### 新闻超链接：sub_add_sch

​	这部分通过爬取超链接简化后续运行步骤。由于需要爬取的内容并不很多，加上ua后直接发requests请求即可。需要注意的是，当页码为一的时候（即首页），requests网址需要进行更改：``

```python
if i == 1:
    search_url = main_url+'.shtml'
else:
    search_url = main_url+'_' + str(i) + '.shtml'
```

​	同时需要留意，当前页面可能并不能一次爬取成功，所以加一个while循环，确保爬取的进行：``

```python
while len(sub_addres) == 0:
    print('ERR,RETRY'+'\n')
    page_text = requests.get(url=search_url, headers=headers).text
    tree = etree.HTML(page_text)
    sub_addres = tree.xpath('//ul[@class="zxxx_list"]/li/a/@href')
    time.sleep(random.randint(1,5))
```

​	加入sleep语句避免短时间大量请求被误判为恶意请求。

​	最后写入本地文件储存`fp.write("http://www.nhc.gov.cn"+ads+"\n")`

### 新闻文本爬取：sub_page_dl

​	这是本次遇到的第一个难点，爬取新闻文本内容。

​	首先说下思路：爬取的对象为一个[通报页面](http://www.nhc.gov.cn/xcs/yqtb/202209/b607d59ecc594874b90728c056176376.shtml)，对其使用相应的开发工具解析页面内容，获取其html文本内容，再对其进行xpath分析获取页标题（用于判断是否为所需页面，避免下载一些无用的页，例如记者会）、页时间（用于文本文件保存的文件名）、页内容。

```python
page_title = tree.xpath('//div[@class="list"]/div[@class="tit"]/text()')
pub_date = tree.xpath('//div[@class="list"]/div[@class="source"]/span[1]/text()')
sub_page = tree.xpath('//div[@class="list"]/div[@class="con"]/p//text()')
```

​	同样的，需要加入while循环避免爬空。

```python
check=re.search('截至',page_title[0])
if check:
    title=re.findall('\d+',str(pub_date))
    print("File Write,title->"+title[0]+title[1]+title[2])
    fp = open('./subpagetext/'+title[0]+"-"+title[1]+"-"+title[2]+'.txt','w',encoding='utf-8')
    for i in sub_page:
        print(str(i),end="")
        fp.write(str(i))
    print("\n"+"Write Success"+"\n")
else:
    print("Useless Page")
    continue
```

​	page_title用于检测页标题，值得吐槽的是卫健委的页面中，文本并不成段出现，而是完全碎的，xx省，格式，数字，格式，例，格式，所以获取到的对象列表也很碎（可能设计人员的数据来源也是动态的，最终生成静态页面）。

​	另外值得一提的是，由于网站具有反扒检测，常规的request请求肯定是不够了，这里用selinium进行爬取，其实就是自动化调用浏览器。但是这样还不够，网站会检测当前的访问是否由内核调用（是否为自动化控制），因此需要对默认开着的浏览设定做前置”拦截“：

```python
bros = webdriver.Chrome(executable_path='./chromedriver.exe')
bros.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
          "source": """
            Object.defineProperty(navigator, 'webdriver', {
              get: () => undefined
            })
          """
        })
```

​	否则会返回400错误，说是头文件参数不足，实际上是检测到了自动化访问。

### 文本数据分析：datasearch

​	丑话说在前，这个地方我做的很痛苦，最终结果也并不是很满意。由于卫健委的新闻通告在大概半年前、一年前的报告格式都有所不同，导致爬取数据并不能一次性正则跑完，所以最终呈现只从今年开始分析数据。

​	用os库获得所下载的所有文件名，直接for循环每一篇报告分析

```python
filelist = os.listdir("./subpagetext")
```

```python
daily_cases = get_current_num(page)#当日全国新增确诊
daily_unknown_cases = get_current_num_uk(page)#当日新增无症状
date=re.search("(\d+月\d+日)0",page).group(1)#获取日期
table.write(hang,lie,date)#写入表格列首作为行索引
```

```python
if daily_cases:
    print("当日新增病例"+daily_cases+"例")
    table.write(hang,lie,daily_cases)
else:
    print("当日无新增病例")
    table.write(hang,lie,0)
```

获取全国新增病例数，无症状同理。

```python
for prv in pr_list:
    local_cases = get_prv_cases(prv,page)
    local_unkonwn_cases = get_prv_unkonwn_cases(prv,page)
    lie+=1
    if local_cases:
        print(prv+"新增确诊"+str(local_cases)+"例",end=",")
        table.write(hang,lie,local_cases)
    else:
        table.write(hang,lie,0)
    if local_unkonwn_cases:
        print(prv+"新增无症状"+str(local_unkonwn_cases)+"例",end="")
        table.write(hang,lie+33,local_unkonwn_cases)
    else:
        table.write(hang,lie+33,0)
```

​	pr_list为手打的省份列表。遍历每个省份，有则写表，无则写0，填充表格。”+33“是为了将省份的新增和无症状分开，这样一次循环即可。

​	至于表格的操作，这里选择xlwt以及后面要用到的xlrd。这两个库不是最好的，但是在我的实际尝试中，这俩是唯一成功的（悲）。

## 3、数据可视化界面的展示

​	数据可视化这里实在是没多少时间做了（实际上只够尝试一个下午），这里采用了pyecharts的模板，只呈现了开年以来全国疫情数据的走势图（render.html）。即使是粗略尝试也能体会到pyecharts 的强大，可惜没能调用map来呈现每日每省的数据变化以及热力图的实现。

# 三、心得体会

 	这是我第一次设计一个爬虫程序（意味着两周之内速通python和爬虫），可以说一开始就是赶着懒狗上架（楽）。实际上爬取花了很长时间，大头就是爬虫的请求修改、文本正则分析。对于零基础小白来说可以说是即痛苦又快乐。痛苦如ddl前改代码到2点发现只是优化了可读性，爬虫挂机一下午发现啥也没爬到，快乐也是有的，例如第一次看到爬虫输出了正确的文本内容，窗口在一堆Err之后终于返回了200，等等等等。总的来说还是很充实的。项目开始之前我还暗自吐槽：啥也不会这还怎么做……

​	”你要是都会了，还让你做啥啊！“（笑）