https://github.com/Dxcyc9/012004114

# 一、PSP表格

| PSP2.1                                  | Personal Software Process Stages       | 预估耗时 （小时） | 实际耗时 （小时） |
| --------------------------------------- | -------------------------------------- | ----------------- | ----------------- |
| Planning                                | 计划                                   | 0.5               | 0.25              |
| · Estimate                              | 估计这个任务需要多少时间               | 0.5               | 0.25              |
| Development                             | 开发                                   | 53.5              | 61.5              |
| · Analysis                              | 需求分析 (包括学习新技术)              | 1.5               | 1                 |
| · Design Spec                           | 生成设计文档                           | 1                 | 0.5               |
| · Design Review                         | 设计复审                               | 1                 | 0.5               |
| · Coding Standard                       | 代码规范 (为目前的开发制 定合适的规范) | 2                 | 2                 |
| · Design                                | 具体设计                               | 2                 | 0.5               |
| · Coding                                | 具体编码                               | 20                | 25                |
| · Code Review                           | 代码复审                               | 1                 | 2                 |
| · Test                                  | 测试（自我测试，修改代 码，提交修改）  | 25                | 30                |
| Reporting                               | 报告                                   | 3                 | 3.75              |
| · Test Repor                            | 测试报告                               | 2                 | 3                 |
| · Size Measurement                      | 计算工作量                             | 0.5               | 0.25              |
| · Postmortem & Process Improvement Plan | 事后总结, 并提出过程改进 计划          | 0.5               | 0.5               |
|                                         | 合计                                   | 57                | 65.5              |

# 二、任务要求的实现

## 2.1 项目设计与技术栈

- 从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？

  首先阅读要求，根据要求分为四大类：数据获取、热点分析、可视化处理、附加题。

  数据获取分为：爬虫、存入数据库、写入excel

  热点分析：模型查找、数据集处理、模型实现

  可视化处理：寻找现成低代码化图表、后端前端联动实现动态化图表（bushi），由于前端学不会，在耗费两天时间后放弃原方案，选择直接调用pyecharts，生成数据可视化html。

- 你分别通过什么渠道、使用什么方式方法完成了各个环节？

  询问已掌握该技术栈的同学、网上寻找已使用该技术栈的代码例子进行学习、搜索指定库或方法的使用方法、在debug中进步

- 列出你完成本次任务所使用的技术栈。

  - python

  - mysql

  - ISTM

  - pyecharts

  - smtplib

  - pyppeteer

## 2.2 爬虫与数据处理

说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数， 他们之间的关系），并对关键的函数或算法进行说明。

- 业务逻辑

首先对卫健委的首页进行信息爬取，爬取获得每一天的疫情情况的url。再将爬取获得的urls存入list，将list中所有的url进行爬取，将爬取获得的数据存入mysql，再将mysql中的数据读取写入excel。

- 爬虫代码设计

首先简单的爬虫无法进行卫健委数据的爬取，利用pyppeteer库包进行爬取。需要在爬虫时填入反反爬虫信息，在session中加入User-Agent，伪装为浏览器访问，将session内部的cookies修改成pypeteer浏览器页面对象中的cookies（在makeSession函数中），在opposeAntiCrawler中添加反反爬虫手段，最后通过get对页面的整体内容进行爬取。

- 数据处理

爬取网页发现，卫健委每一天的疫情数据的html每个一段时间就存在不同，给数据处理带来了不小的麻烦。故我们想将页面的主要信息进行爬取，而不需要那些标签（如<font></font>>）。将文字信息提取出来后再对文字信息进行筛选，即可得到我们所需要的数据’

- 函数说明

  - ```python
    # 获取网页通过Session
    def getHtmlBySession(session, url)
    # 为page添加预防反爬虫手段
    async def opposeAntiCrawler(page)
    # 返回一个session,将其内部cookies修改成pypeteer浏览器页面对象中的cookies
    async def handleSession(page)
    # 爬取网页主题信息
    async def getMainInfo(url)
    #获取页面信息，并进行初步筛选
    def getMainInfo(url)
    #爬取主页链接
    def getUrls(url)
    #爬取各个日期页面信息并存入数据库
    def getDayData(url)
    #创建完整信息表
    def writeToExcel(dateList)
    #创建全国每日信息表
    def writeCountryInfo(dateList)
    ```



## 2.3 数据统计接口部分的性能改进

![img](http://a1.qpic.cn/psc?/V123YVdJ1qKfoR/05RlWl8gsTOH*Z17MtCBzHb2kMMUZoq*iHhEJ1d4KTRgySK2ry2cPNRT0zyF3ApzNzhgrCF7uSPMWDhTqPqqEA!!/b&ek=1&kp=1&pt=0&bo=mwVzA5sFcwMWADA!&tl=1&vuin=2287511565&tm=1663660800&dis_t=1663661241&dis_k=6dfb3ac7c18647b9415ba45446bc99ea&sce=50-1-1&rf=viewer_311)

- 根据本图可知，CPU内核所占时间最大，因为本题主要是需要给爬虫sleep时间，故本程序运行时间较长
- 爬虫是基于pyppeteer实现的，pyppeteer是开启一个虚拟浏览器，故不会占用程序时间，效果较佳。
- 本程序是数据存储读取基于mysql实现的，故爬取数据可持久化，存储读取能更为便捷。

## 2.4 每日热点的实现思路

​	将新增无症状感染者数据作为训练集，将新增确诊人数作为测试集。训练集用于训练LSTM模型，训练收敛后预测新增确诊人数。然后再将偏离值较大的选出，作为本次热点分析结果，这是一个朴素的想法，无症状感染者与确诊者有较强的关联性，通过挖掘无症状感染者的数量变化规律可以较好的预测出确诊人数，如若不然，就说明当天有其他因素影响了确诊人数的变化，需要作为热点重点分析。

## 结果

- 蓝线为标签
- 橙线为预测

### 训练结果

![img](http://a1.qpic.cn/psc?/V123YVdJ1qKfoR/05RlWl8gsTOH*Z17MtCBzNf6GPSawUx0jEeThk7xvSptH.EEYrGCfdWqcqtWRAoM7DE1OnqA*tGuAntRWQKtMw!!/b&ek=1&kp=1&pt=0&bo=yAL6AcgC.gEWADA!&tl=1&vuin=2287511565&tm=1663660800&dis_t=1663661241&dis_k=671365b4814bebc53c049bafcc3a3c37&sce=50-1-1&rf=viewer_311)

### 预测结果

![img](http://r.photo.store.qq.com/psc?/V123YVdJ1qKfoR/05RlWl8gsTOH*Z17MtCBzH3MzOerCm7A84.xG2aCc3AcOSLXXuEZGb6gEqqlLCtSVjpIiKIWGLa0GMXNTZeU4w!!/o&ek=1&kp=1&pt=0&bo=QwMOAkMDDgIWADA!&tl=1&tm=1663660800&dis_t=1663661440&dis_k=96a4ddfa7e40d7edb15a52f26c293142&sce=0-12-12&rf=viewer_311)

## 热点挖掘

### 结果

```bash
日期下标 真实值 预测值  偏差百分比
	25 566 	[509] [10.07067138]
	28 623 	[538] [13.64365971]
	29 646 	[574] [11.14551084]
	30 648 	[570] [12.03703704]
	31 614 	[541] [11.88925081]
	37 162 	[189] [16.66666667]
	38 53 	[81] [52.83018868]
	39 38 	[42] [10.52631579]
	42 74 	[65] [12.16216216]
	43 49 	[57] [16.32653061]
	51 106 	[117] [10.37735849]
	53 108 	[123] [13.88888889]
	55 117 	[140] [19.65811966]
	57 75 	[86] [14.66666667]
	60 56 	[67] [19.64285714]
	62 46 	[53] [15.2173913]
	65 47 	[53] [12.76595745]
	68 69 	[83] [20.28985507]
	69 41 	[49] [19.51219512]
	71 38 	[48] [26.31578947]
	72 12 	[18] [50.]
	73 8 	[7] [12.5]
	74 3 	[2] [33.33333333]
	75 1 	[-1] [200.]
	76 5 	[1] [80.]
	77 2 	[0] [100.]
	78 6 	[3] [50.]
	79 18 	[13] [27.77777778]
	81 10 	[8] [20.]
	82 9 	[7] [22.22222222]
	83 13 	[10] [23.07692308]
	85 11 	[7] [36.36363636]
	86 23 	[18] [21.73913043]
	87 42 	[37] [11.9047619]
	92 65 	[82] [26.15384615]
	93 30 	[39] [30.]
	97 25 	[28] [12.]
	102 22 	[19] [13.63636364]
	105 54 	[45] [16.66666667]
```





## 2.5 数据可视化界面的展示

![img](http://a1.qpic.cn/psc?/V123YVdJ1qKfoR/05RlWl8gsTOH*Z17MtCBzDWFkw7h1kVivjArqz9D0JlTgEICqL5wY2fI5c7GNhTI43iE*u9orfhqTZ7Pre3J6g!!/b&ek=1&kp=1&pt=0&bo=igN.AooDfgIWADA!&tl=1&vuin=2287511565&tm=1663660800&dis_t=1663661241&dis_k=8e60ef423258b69b6af1cd92721b1c8e&sce=50-1-1&rf=viewer_311)



如上图所示，该可视化页面可以展示一天中的全国各地的新增确证和新增无症状感染者，根据上方“新增确诊”和“新增无症状”的按钮，当二者选其一时，只会出现二者之一的地图；当二者都选择时，则地图出现的是二者累加的结果。

![img](http://a1.qpic.cn/psc?/V123YVdJ1qKfoR/05RlWl8gsTOH*Z17MtCBzFbFZwlUakV1v5jKX0R7z*g75p3HR4kLk0Bn6uebF3ctGD*6UJCYlCdQ*K0afsREkw!!/b&ek=1&kp=1&pt=0&bo=DgRRAg4EUQIWADA!&tl=1&vuin=2287511565&tm=1663660800&dis_t=1663661241&dis_k=49a592e90eb7ba4aef9354ba667f10f6&sce=50-1-1&rf=viewer_311)

如上图所示，该图展示出全国本土近七天的新增确诊和新增无症状感染者的每日新增数量。因港台每日新增数量较多，如将其添加进图，可视化图表将有较大的变化，不具备直观的观察效果。

- 实现方法

两者图表都是运用python的pyecharts库进行生成html，当每天进行更新数据时（即每日进行爬取每日新增数据时），当数据爬取完毕时将调运该生成图表的函数，即在每天爬取完数据后图表的信息也会进行更新。地图疫情图，每日将新增一张，当生成的数量达到一定限度值时，将删除日期较前的图表。而近七天的疫情表，当爬取的数据存入数据库时，调用生成该图表的函数，函数将重新根据日期降序查询七条数据，将所得数据读出生成上述的柱状图。

pyecharts直接引入柱状图（bar）和地图（Map）的库包即可直接使用，上手极为方便。

## 2.6附加功能的实现

- 自动发送所在地区当日疫情的邮件的功能

用户输入自己所在地区与用户邮箱，将用户数据存入数据库。

当每日进行爬取完数据后，对数据库中的user表进行查询，遍历表中的信息，将每个用户对应的地理位置信息读出，再用该位置去匹配地区疫情表中的信息，匹配用户所在地的当日疫情信息，将其返回，发送给用户。

- python邮件实现

将发送方的邮箱的权限打开，获取授权码。在代码中引入smtplib模块和email模块，smtplib模块主要负责发送邮件，email模块主要负责构造邮件。随后即可生成发送邮件的正文进行发送。

# 三、心得体会

- 爬虫阶段的心得体会

  爬虫雀食好用，但研究如何防止被反爬雀食挺麻烦的。卫健委的每日疫情的信息都是手动输入的雀食让我没想到（因为所有的信息都写在html上，没有调用后端的接口），卫健委的html的格式不断的变化让人很头疼。正则表达式yyds，在学之前一脸疑惑，不知道这是什么奇奇怪怪的东西，学完之后就真香警告，以后做字符串处理类的算法题直接一个正则表达式就能解决（bushi。因为我有将爬到的数据存入mysql，因为之前对python并不是很熟悉，所有在该次作业中学会了关于python的一些后端操作，不错。用python处理excel雀食好用，但该次作业对excel的操作比较少，以后可以多尝试一些。

- 热点分析阶段的心得体会

  机器学习的日程要提上日程了，现在不会点分析聚类预测优化算法是真的顶不住了。机器学习雀食挺有趣的，只是我太菜了（

- 可视化阶段新的体会

  本想找些已有的图表的前端代码，实际上也找好了，它们生成的图表相当好看。但因为我不会前后端接口对接，在尝试两天后，决定放弃了（，可惜了那么好看的图表。于是花了一个小时用pyecharts生成了图表，pyecharts的生成图表功能确实很方便，但就是图表的精美程度还不太让人满意。

- 附加题阶段心得

  本来想用kafka进行监听卫健委的数据，当有新数据上传时就立刻将数据写入数据库，可惜这段时间太忙了，没有时间去实现这个附加题。

  于是就自己新增了一个新功能，因为每天去看疫情的信息实属麻烦，于是我就想用户直接告诉客户端自己所在的地区，客户端将该地区每天的疫情信息直接发送给用户，就无需用户每日查看信息了，个人认为还是一个比较方便的功能。

